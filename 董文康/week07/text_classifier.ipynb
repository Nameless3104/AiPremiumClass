{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71e6b80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence  # 长度不同张量填充为相同长度\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b90a2c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_from_doc(doc):\n",
    "    vocab = set()\n",
    "    for line in doc:\n",
    "        vocab.update(line[0])\n",
    "\n",
    "    vocab =  ['__PAD__','__UNK__'] + list(vocab)  # PAD: padding, UNK: unknown\n",
    "    w2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    return w2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03f168f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词汇表大小: 62878\n"
     ]
    }
   ],
   "source": [
    "# 加载训练语料\n",
    "with open('data/comments.pkl','rb') as f:\n",
    "    comments_data = pickle.load(f)\n",
    "\n",
    "# 构建词汇表\n",
    "vocab = build_from_doc(comments_data)\n",
    "print('词汇表大小:', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "197b913e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义数据转换方法(callback function)回调函数\n",
    "# 该函数会在每个batch数据加载时被调用\n",
    "def convert_data(batch_data):\n",
    "    comments, votes = [],[]\n",
    "    # 分别提取评论和标签\n",
    "    for comment, vote in batch_data:\n",
    "        comments.append(torch.tensor([vocab.get(word, vocab['__UNK__']) for word in comment]))\n",
    "        votes.append(vote)\n",
    "    \n",
    "    # 将评论和标签转换为tensor\n",
    "    commt = pad_sequence(comments, batch_first=True, padding_value=vocab['__PAD__'])  # 填充为相同长度\n",
    "    labels = torch.tensor(votes)\n",
    "    # 返回评论和标签\n",
    "    return commt, labels\n",
    "\n",
    "# 通过Dataset构建DataLoader\n",
    "dataloader = DataLoader(comments_data, batch_size=4, shuffle=True, collate_fn=convert_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b086da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "# vocab_size: 词汇表大小\n",
    "# embedding_dim: 词嵌入维度\n",
    "# hidden_size: LSTM隐藏层大小\n",
    "# num_classes: 分类数量\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 100\n",
    "hidden_size = 128\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33b1d9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Comments_Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)  # padding_idx=0\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # input_ids: (batch_size, seq_len)\n",
    "        # embedded: (batch_size, seq_len, embedding_dim)\n",
    "        embedded = self.embedding(input_ids)\n",
    "        # output: (batch_size, seq_len, hidden_size)\n",
    "        output, (hidden, _) = self.rnn(embedded)\n",
    "        output = self.fc(output[:, -1, :])  # 取最后一个时间步的输出\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d9e86cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 所有向量集合 Embedding（词嵌入）\n",
    "emb = nn.Embedding(len(vocab), embedding_dim) # 词汇表大小，向量维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "648d5aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [10/25000], Loss: 0.6589\n",
      "Epoch [1/1], Step [20/25000], Loss: 0.6225\n",
      "Epoch [1/1], Step [30/25000], Loss: 1.6179\n",
      "Epoch [1/1], Step [40/25000], Loss: 0.3345\n",
      "Epoch [1/1], Step [50/25000], Loss: 0.5181\n",
      "Epoch [1/1], Step [60/25000], Loss: 0.5538\n",
      "Epoch [1/1], Step [70/25000], Loss: 1.2838\n",
      "Epoch [1/1], Step [80/25000], Loss: 0.2880\n",
      "Epoch [1/1], Step [90/25000], Loss: 0.8785\n",
      "Epoch [1/1], Step [100/25000], Loss: 0.2423\n",
      "Epoch [1/1], Step [110/25000], Loss: 1.0732\n",
      "Epoch [1/1], Step [120/25000], Loss: 0.7739\n",
      "Epoch [1/1], Step [130/25000], Loss: 0.5920\n",
      "Epoch [1/1], Step [140/25000], Loss: 0.4664\n",
      "Epoch [1/1], Step [150/25000], Loss: 0.8679\n",
      "Epoch [1/1], Step [160/25000], Loss: 0.6486\n",
      "Epoch [1/1], Step [170/25000], Loss: 0.2618\n",
      "Epoch [1/1], Step [180/25000], Loss: 1.2081\n",
      "Epoch [1/1], Step [190/25000], Loss: 0.5834\n",
      "Epoch [1/1], Step [200/25000], Loss: 0.5591\n",
      "Epoch [1/1], Step [210/25000], Loss: 0.5112\n",
      "Epoch [1/1], Step [220/25000], Loss: 0.3644\n",
      "Epoch [1/1], Step [230/25000], Loss: 1.1742\n",
      "Epoch [1/1], Step [240/25000], Loss: 0.5439\n",
      "Epoch [1/1], Step [250/25000], Loss: 0.2665\n",
      "Epoch [1/1], Step [260/25000], Loss: 0.5729\n",
      "Epoch [1/1], Step [270/25000], Loss: 0.8727\n",
      "Epoch [1/1], Step [280/25000], Loss: 0.2896\n",
      "Epoch [1/1], Step [290/25000], Loss: 0.4856\n",
      "Epoch [1/1], Step [300/25000], Loss: 0.5322\n",
      "Epoch [1/1], Step [310/25000], Loss: 0.2971\n",
      "Epoch [1/1], Step [320/25000], Loss: 0.6117\n",
      "Epoch [1/1], Step [330/25000], Loss: 0.5042\n",
      "Epoch [1/1], Step [340/25000], Loss: 0.6115\n",
      "Epoch [1/1], Step [350/25000], Loss: 0.9329\n",
      "Epoch [1/1], Step [360/25000], Loss: 0.7541\n",
      "Epoch [1/1], Step [370/25000], Loss: 0.3821\n",
      "Epoch [1/1], Step [380/25000], Loss: 0.3252\n",
      "Epoch [1/1], Step [390/25000], Loss: 0.5307\n",
      "Epoch [1/1], Step [400/25000], Loss: 0.5077\n",
      "Epoch [1/1], Step [410/25000], Loss: 0.2268\n",
      "Epoch [1/1], Step [420/25000], Loss: 0.6560\n",
      "Epoch [1/1], Step [430/25000], Loss: 0.9135\n",
      "Epoch [1/1], Step [440/25000], Loss: 0.3194\n",
      "Epoch [1/1], Step [450/25000], Loss: 0.3390\n",
      "Epoch [1/1], Step [460/25000], Loss: 0.7840\n",
      "Epoch [1/1], Step [470/25000], Loss: 0.8353\n",
      "Epoch [1/1], Step [480/25000], Loss: 0.3240\n",
      "Epoch [1/1], Step [490/25000], Loss: 0.3306\n",
      "Epoch [1/1], Step [500/25000], Loss: 0.5370\n",
      "Epoch [1/1], Step [510/25000], Loss: 0.8736\n",
      "Epoch [1/1], Step [520/25000], Loss: 0.7080\n",
      "Epoch [1/1], Step [530/25000], Loss: 0.5837\n",
      "Epoch [1/1], Step [540/25000], Loss: 0.3348\n",
      "Epoch [1/1], Step [550/25000], Loss: 0.3375\n",
      "Epoch [1/1], Step [560/25000], Loss: 0.2582\n",
      "Epoch [1/1], Step [570/25000], Loss: 0.3082\n",
      "Epoch [1/1], Step [580/25000], Loss: 1.0865\n",
      "Epoch [1/1], Step [590/25000], Loss: 0.6109\n",
      "Epoch [1/1], Step [600/25000], Loss: 0.8775\n",
      "Epoch [1/1], Step [610/25000], Loss: 0.2679\n",
      "Epoch [1/1], Step [620/25000], Loss: 1.1005\n",
      "Epoch [1/1], Step [630/25000], Loss: 0.5604\n",
      "Epoch [1/1], Step [640/25000], Loss: 1.1057\n",
      "Epoch [1/1], Step [650/25000], Loss: 0.2198\n",
      "Epoch [1/1], Step [660/25000], Loss: 0.2663\n",
      "Epoch [1/1], Step [670/25000], Loss: 0.8555\n",
      "Epoch [1/1], Step [680/25000], Loss: 0.5572\n",
      "Epoch [1/1], Step [690/25000], Loss: 0.2531\n",
      "Epoch [1/1], Step [700/25000], Loss: 0.5552\n",
      "Epoch [1/1], Step [710/25000], Loss: 0.9413\n",
      "Epoch [1/1], Step [720/25000], Loss: 0.9010\n",
      "Epoch [1/1], Step [730/25000], Loss: 1.0495\n",
      "Epoch [1/1], Step [740/25000], Loss: 0.6213\n",
      "Epoch [1/1], Step [750/25000], Loss: 0.7421\n",
      "Epoch [1/1], Step [760/25000], Loss: 0.5790\n",
      "Epoch [1/1], Step [770/25000], Loss: 0.5911\n",
      "Epoch [1/1], Step [780/25000], Loss: 0.7588\n",
      "Epoch [1/1], Step [790/25000], Loss: 0.5619\n",
      "Epoch [1/1], Step [800/25000], Loss: 0.3395\n",
      "Epoch [1/1], Step [810/25000], Loss: 0.2401\n",
      "Epoch [1/1], Step [820/25000], Loss: 0.4108\n",
      "Epoch [1/1], Step [830/25000], Loss: 0.5948\n",
      "Epoch [1/1], Step [840/25000], Loss: 0.2812\n",
      "Epoch [1/1], Step [850/25000], Loss: 0.1680\n",
      "Epoch [1/1], Step [860/25000], Loss: 0.1794\n",
      "Epoch [1/1], Step [870/25000], Loss: 0.8219\n",
      "Epoch [1/1], Step [880/25000], Loss: 0.7968\n",
      "Epoch [1/1], Step [890/25000], Loss: 0.2565\n",
      "Epoch [1/1], Step [900/25000], Loss: 0.6478\n",
      "Epoch [1/1], Step [910/25000], Loss: 1.2726\n",
      "Epoch [1/1], Step [920/25000], Loss: 0.5927\n",
      "Epoch [1/1], Step [930/25000], Loss: 0.5721\n",
      "Epoch [1/1], Step [940/25000], Loss: 0.8739\n",
      "Epoch [1/1], Step [950/25000], Loss: 0.3109\n",
      "Epoch [1/1], Step [960/25000], Loss: 0.3657\n",
      "Epoch [1/1], Step [970/25000], Loss: 0.8066\n",
      "Epoch [1/1], Step [980/25000], Loss: 0.7903\n",
      "Epoch [1/1], Step [990/25000], Loss: 0.5079\n",
      "Epoch [1/1], Step [1000/25000], Loss: 0.5729\n",
      "Epoch [1/1], Step [1010/25000], Loss: 0.5851\n",
      "Epoch [1/1], Step [1020/25000], Loss: 0.5353\n",
      "Epoch [1/1], Step [1030/25000], Loss: 0.6255\n",
      "Epoch [1/1], Step [1040/25000], Loss: 0.6032\n",
      "Epoch [1/1], Step [1050/25000], Loss: 0.5513\n",
      "Epoch [1/1], Step [1060/25000], Loss: 0.5522\n",
      "Epoch [1/1], Step [1070/25000], Loss: 1.0852\n",
      "Epoch [1/1], Step [1080/25000], Loss: 0.3426\n",
      "Epoch [1/1], Step [1090/25000], Loss: 0.3208\n",
      "Epoch [1/1], Step [1100/25000], Loss: 0.3376\n",
      "Epoch [1/1], Step [1110/25000], Loss: 0.5309\n",
      "Epoch [1/1], Step [1120/25000], Loss: 0.6498\n",
      "Epoch [1/1], Step [1130/25000], Loss: 0.5743\n",
      "Epoch [1/1], Step [1140/25000], Loss: 0.5491\n",
      "Epoch [1/1], Step [1150/25000], Loss: 0.4207\n",
      "Epoch [1/1], Step [1160/25000], Loss: 0.3324\n",
      "Epoch [1/1], Step [1170/25000], Loss: 0.5236\n",
      "Epoch [1/1], Step [1180/25000], Loss: 0.7812\n",
      "Epoch [1/1], Step [1190/25000], Loss: 0.5229\n",
      "Epoch [1/1], Step [1200/25000], Loss: 0.6098\n",
      "Epoch [1/1], Step [1210/25000], Loss: 0.5715\n",
      "Epoch [1/1], Step [1220/25000], Loss: 0.3849\n",
      "Epoch [1/1], Step [1230/25000], Loss: 0.9158\n",
      "Epoch [1/1], Step [1240/25000], Loss: 0.5585\n",
      "Epoch [1/1], Step [1250/25000], Loss: 0.6411\n",
      "Epoch [1/1], Step [1260/25000], Loss: 0.2686\n",
      "Epoch [1/1], Step [1270/25000], Loss: 0.5424\n",
      "Epoch [1/1], Step [1280/25000], Loss: 0.2359\n",
      "Epoch [1/1], Step [1290/25000], Loss: 0.7163\n",
      "Epoch [1/1], Step [1300/25000], Loss: 0.2528\n",
      "Epoch [1/1], Step [1310/25000], Loss: 0.6610\n",
      "Epoch [1/1], Step [1320/25000], Loss: 0.1723\n",
      "Epoch [1/1], Step [1330/25000], Loss: 0.5346\n",
      "Epoch [1/1], Step [1340/25000], Loss: 0.2888\n",
      "Epoch [1/1], Step [1350/25000], Loss: 0.2383\n",
      "Epoch [1/1], Step [1360/25000], Loss: 0.2011\n",
      "Epoch [1/1], Step [1370/25000], Loss: 0.5399\n",
      "Epoch [1/1], Step [1380/25000], Loss: 0.5440\n",
      "Epoch [1/1], Step [1390/25000], Loss: 0.5763\n",
      "Epoch [1/1], Step [1400/25000], Loss: 0.7341\n",
      "Epoch [1/1], Step [1410/25000], Loss: 0.3478\n",
      "Epoch [1/1], Step [1420/25000], Loss: 0.2727\n",
      "Epoch [1/1], Step [1430/25000], Loss: 0.2184\n",
      "Epoch [1/1], Step [1440/25000], Loss: 0.8513\n",
      "Epoch [1/1], Step [1450/25000], Loss: 0.9056\n",
      "Epoch [1/1], Step [1460/25000], Loss: 0.2242\n",
      "Epoch [1/1], Step [1470/25000], Loss: 0.3614\n",
      "Epoch [1/1], Step [1480/25000], Loss: 0.2120\n",
      "Epoch [1/1], Step [1490/25000], Loss: 0.8655\n",
      "Epoch [1/1], Step [1500/25000], Loss: 0.7863\n",
      "Epoch [1/1], Step [1510/25000], Loss: 0.3174\n",
      "Epoch [1/1], Step [1520/25000], Loss: 0.8113\n",
      "Epoch [1/1], Step [1530/25000], Loss: 0.5475\n",
      "Epoch [1/1], Step [1540/25000], Loss: 0.8460\n",
      "Epoch [1/1], Step [1550/25000], Loss: 0.3011\n",
      "Epoch [1/1], Step [1560/25000], Loss: 0.9092\n",
      "Epoch [1/1], Step [1570/25000], Loss: 0.1775\n",
      "Epoch [1/1], Step [1580/25000], Loss: 0.8970\n",
      "Epoch [1/1], Step [1590/25000], Loss: 0.8373\n",
      "Epoch [1/1], Step [1600/25000], Loss: 0.6250\n",
      "Epoch [1/1], Step [1610/25000], Loss: 0.5192\n",
      "Epoch [1/1], Step [1620/25000], Loss: 0.3834\n",
      "Epoch [1/1], Step [1630/25000], Loss: 1.0520\n",
      "Epoch [1/1], Step [1640/25000], Loss: 0.2446\n",
      "Epoch [1/1], Step [1650/25000], Loss: 0.7259\n",
      "Epoch [1/1], Step [1660/25000], Loss: 0.5195\n",
      "Epoch [1/1], Step [1670/25000], Loss: 0.3183\n",
      "Epoch [1/1], Step [1680/25000], Loss: 0.6197\n",
      "Epoch [1/1], Step [1690/25000], Loss: 0.4744\n",
      "Epoch [1/1], Step [1700/25000], Loss: 1.3712\n",
      "Epoch [1/1], Step [1710/25000], Loss: 0.4374\n",
      "Epoch [1/1], Step [1720/25000], Loss: 0.4319\n",
      "Epoch [1/1], Step [1730/25000], Loss: 1.0470\n",
      "Epoch [1/1], Step [1740/25000], Loss: 0.2346\n",
      "Epoch [1/1], Step [1750/25000], Loss: 0.5217\n",
      "Epoch [1/1], Step [1760/25000], Loss: 0.8675\n",
      "Epoch [1/1], Step [1770/25000], Loss: 0.1974\n",
      "Epoch [1/1], Step [1780/25000], Loss: 0.6598\n",
      "Epoch [1/1], Step [1790/25000], Loss: 1.0986\n",
      "Epoch [1/1], Step [1800/25000], Loss: 0.4263\n",
      "Epoch [1/1], Step [1810/25000], Loss: 0.4299\n",
      "Epoch [1/1], Step [1820/25000], Loss: 0.1480\n",
      "Epoch [1/1], Step [1830/25000], Loss: 0.5826\n",
      "Epoch [1/1], Step [1840/25000], Loss: 0.1638\n",
      "Epoch [1/1], Step [1850/25000], Loss: 1.4678\n",
      "Epoch [1/1], Step [1860/25000], Loss: 0.9136\n",
      "Epoch [1/1], Step [1870/25000], Loss: 0.3487\n",
      "Epoch [1/1], Step [1880/25000], Loss: 0.7014\n",
      "Epoch [1/1], Step [1890/25000], Loss: 0.2493\n",
      "Epoch [1/1], Step [1900/25000], Loss: 0.2900\n",
      "Epoch [1/1], Step [1910/25000], Loss: 1.0145\n",
      "Epoch [1/1], Step [1920/25000], Loss: 0.6429\n",
      "Epoch [1/1], Step [1930/25000], Loss: 0.4456\n",
      "Epoch [1/1], Step [1940/25000], Loss: 0.5678\n",
      "Epoch [1/1], Step [1950/25000], Loss: 0.1931\n",
      "Epoch [1/1], Step [1960/25000], Loss: 0.6265\n",
      "Epoch [1/1], Step [1970/25000], Loss: 0.3472\n",
      "Epoch [1/1], Step [1980/25000], Loss: 0.2854\n",
      "Epoch [1/1], Step [1990/25000], Loss: 0.1741\n",
      "Epoch [1/1], Step [2000/25000], Loss: 0.7193\n",
      "Epoch [1/1], Step [2010/25000], Loss: 0.6609\n",
      "Epoch [1/1], Step [2020/25000], Loss: 0.7158\n",
      "Epoch [1/1], Step [2030/25000], Loss: 0.5605\n",
      "Epoch [1/1], Step [2040/25000], Loss: 0.6312\n",
      "Epoch [1/1], Step [2050/25000], Loss: 0.2460\n",
      "Epoch [1/1], Step [2060/25000], Loss: 0.2089\n",
      "Epoch [1/1], Step [2070/25000], Loss: 0.5323\n",
      "Epoch [1/1], Step [2080/25000], Loss: 0.7372\n",
      "Epoch [1/1], Step [2090/25000], Loss: 0.5657\n",
      "Epoch [1/1], Step [2100/25000], Loss: 0.5529\n",
      "Epoch [1/1], Step [2110/25000], Loss: 0.8526\n",
      "Epoch [1/1], Step [2120/25000], Loss: 0.6653\n",
      "Epoch [1/1], Step [2130/25000], Loss: 0.5404\n",
      "Epoch [1/1], Step [2140/25000], Loss: 0.5457\n",
      "Epoch [1/1], Step [2150/25000], Loss: 0.3459\n",
      "Epoch [1/1], Step [2160/25000], Loss: 0.6588\n",
      "Epoch [1/1], Step [2170/25000], Loss: 0.2317\n",
      "Epoch [1/1], Step [2180/25000], Loss: 0.5723\n",
      "Epoch [1/1], Step [2190/25000], Loss: 0.9048\n",
      "Epoch [1/1], Step [2200/25000], Loss: 1.1200\n",
      "Epoch [1/1], Step [2210/25000], Loss: 0.5617\n",
      "Epoch [1/1], Step [2220/25000], Loss: 0.5248\n",
      "Epoch [1/1], Step [2230/25000], Loss: 0.2836\n",
      "Epoch [1/1], Step [2240/25000], Loss: 0.6348\n",
      "Epoch [1/1], Step [2250/25000], Loss: 0.5638\n",
      "Epoch [1/1], Step [2260/25000], Loss: 0.5324\n",
      "Epoch [1/1], Step [2270/25000], Loss: 0.7847\n",
      "Epoch [1/1], Step [2280/25000], Loss: 0.5719\n",
      "Epoch [1/1], Step [2290/25000], Loss: 0.2755\n",
      "Epoch [1/1], Step [2300/25000], Loss: 0.5823\n",
      "Epoch [1/1], Step [2310/25000], Loss: 0.5620\n",
      "Epoch [1/1], Step [2320/25000], Loss: 0.2114\n",
      "Epoch [1/1], Step [2330/25000], Loss: 0.5479\n",
      "Epoch [1/1], Step [2340/25000], Loss: 0.5812\n",
      "Epoch [1/1], Step [2350/25000], Loss: 0.2963\n",
      "Epoch [1/1], Step [2360/25000], Loss: 0.2800\n",
      "Epoch [1/1], Step [2370/25000], Loss: 0.4504\n",
      "Epoch [1/1], Step [2380/25000], Loss: 0.5630\n",
      "Epoch [1/1], Step [2390/25000], Loss: 1.0314\n",
      "Epoch [1/1], Step [2400/25000], Loss: 0.5638\n",
      "Epoch [1/1], Step [2410/25000], Loss: 0.4497\n",
      "Epoch [1/1], Step [2420/25000], Loss: 0.2664\n",
      "Epoch [1/1], Step [2430/25000], Loss: 0.6186\n",
      "Epoch [1/1], Step [2440/25000], Loss: 0.5723\n",
      "Epoch [1/1], Step [2450/25000], Loss: 0.8820\n",
      "Epoch [1/1], Step [2460/25000], Loss: 0.2202\n",
      "Epoch [1/1], Step [2470/25000], Loss: 0.6869\n",
      "Epoch [1/1], Step [2480/25000], Loss: 0.2593\n",
      "Epoch [1/1], Step [2490/25000], Loss: 0.5939\n",
      "Epoch [1/1], Step [2500/25000], Loss: 0.5539\n",
      "Epoch [1/1], Step [2510/25000], Loss: 0.8662\n",
      "Epoch [1/1], Step [2520/25000], Loss: 0.2875\n",
      "Epoch [1/1], Step [2530/25000], Loss: 0.3471\n",
      "Epoch [1/1], Step [2540/25000], Loss: 0.7113\n",
      "Epoch [1/1], Step [2550/25000], Loss: 0.3511\n",
      "Epoch [1/1], Step [2560/25000], Loss: 0.2613\n",
      "Epoch [1/1], Step [2570/25000], Loss: 0.1396\n",
      "Epoch [1/1], Step [2580/25000], Loss: 0.6642\n",
      "Epoch [1/1], Step [2590/25000], Loss: 0.8670\n",
      "Epoch [1/1], Step [2600/25000], Loss: 0.4592\n",
      "Epoch [1/1], Step [2610/25000], Loss: 0.8390\n",
      "Epoch [1/1], Step [2620/25000], Loss: 0.5628\n",
      "Epoch [1/1], Step [2630/25000], Loss: 0.7611\n",
      "Epoch [1/1], Step [2640/25000], Loss: 0.5171\n",
      "Epoch [1/1], Step [2650/25000], Loss: 0.1876\n",
      "Epoch [1/1], Step [2660/25000], Loss: 0.6849\n",
      "Epoch [1/1], Step [2670/25000], Loss: 0.1931\n",
      "Epoch [1/1], Step [2680/25000], Loss: 0.1793\n",
      "Epoch [1/1], Step [2690/25000], Loss: 1.2186\n",
      "Epoch [1/1], Step [2700/25000], Loss: 0.2629\n",
      "Epoch [1/1], Step [2710/25000], Loss: 0.8054\n",
      "Epoch [1/1], Step [2720/25000], Loss: 0.6323\n",
      "Epoch [1/1], Step [2730/25000], Loss: 0.5389\n",
      "Epoch [1/1], Step [2740/25000], Loss: 0.9972\n",
      "Epoch [1/1], Step [2750/25000], Loss: 0.7059\n",
      "Epoch [1/1], Step [2760/25000], Loss: 0.5499\n",
      "Epoch [1/1], Step [2770/25000], Loss: 0.3874\n",
      "Epoch [1/1], Step [2780/25000], Loss: 1.1358\n",
      "Epoch [1/1], Step [2790/25000], Loss: 0.6177\n",
      "Epoch [1/1], Step [2800/25000], Loss: 0.6783\n",
      "Epoch [1/1], Step [2810/25000], Loss: 0.4389\n",
      "Epoch [1/1], Step [2820/25000], Loss: 0.1240\n",
      "Epoch [1/1], Step [2830/25000], Loss: 0.5659\n",
      "Epoch [1/1], Step [2840/25000], Loss: 0.3670\n",
      "Epoch [1/1], Step [2850/25000], Loss: 0.7946\n",
      "Epoch [1/1], Step [2860/25000], Loss: 0.8607\n",
      "Epoch [1/1], Step [2870/25000], Loss: 0.1458\n",
      "Epoch [1/1], Step [2880/25000], Loss: 0.5907\n",
      "Epoch [1/1], Step [2890/25000], Loss: 0.5669\n",
      "Epoch [1/1], Step [2900/25000], Loss: 0.3592\n",
      "Epoch [1/1], Step [2910/25000], Loss: 0.2252\n",
      "Epoch [1/1], Step [2920/25000], Loss: 0.1609\n",
      "Epoch [1/1], Step [2930/25000], Loss: 0.5761\n",
      "Epoch [1/1], Step [2940/25000], Loss: 1.0194\n",
      "Epoch [1/1], Step [2950/25000], Loss: 0.3927\n",
      "Epoch [1/1], Step [2960/25000], Loss: 0.4846\n",
      "Epoch [1/1], Step [2970/25000], Loss: 0.8603\n",
      "Epoch [1/1], Step [2980/25000], Loss: 0.6707\n",
      "Epoch [1/1], Step [2990/25000], Loss: 0.7486\n",
      "Epoch [1/1], Step [3000/25000], Loss: 0.3305\n",
      "Epoch [1/1], Step [3010/25000], Loss: 0.7880\n",
      "Epoch [1/1], Step [3020/25000], Loss: 0.6332\n",
      "Epoch [1/1], Step [3030/25000], Loss: 0.2202\n",
      "Epoch [1/1], Step [3040/25000], Loss: 0.8928\n",
      "Epoch [1/1], Step [3050/25000], Loss: 0.6917\n",
      "Epoch [1/1], Step [3060/25000], Loss: 0.9477\n",
      "Epoch [1/1], Step [3070/25000], Loss: 0.6058\n",
      "Epoch [1/1], Step [3080/25000], Loss: 0.6934\n",
      "Epoch [1/1], Step [3090/25000], Loss: 0.5358\n",
      "Epoch [1/1], Step [3100/25000], Loss: 0.3379\n",
      "Epoch [1/1], Step [3110/25000], Loss: 0.1720\n",
      "Epoch [1/1], Step [3120/25000], Loss: 0.2900\n",
      "Epoch [1/1], Step [3130/25000], Loss: 0.5514\n",
      "Epoch [1/1], Step [3140/25000], Loss: 0.3439\n",
      "Epoch [1/1], Step [3150/25000], Loss: 0.6156\n",
      "Epoch [1/1], Step [3160/25000], Loss: 0.6548\n",
      "Epoch [1/1], Step [3170/25000], Loss: 0.2690\n",
      "Epoch [1/1], Step [3180/25000], Loss: 0.7722\n",
      "Epoch [1/1], Step [3190/25000], Loss: 0.8086\n",
      "Epoch [1/1], Step [3200/25000], Loss: 0.2933\n",
      "Epoch [1/1], Step [3210/25000], Loss: 0.8246\n",
      "Epoch [1/1], Step [3220/25000], Loss: 0.5353\n",
      "Epoch [1/1], Step [3230/25000], Loss: 0.3259\n",
      "Epoch [1/1], Step [3240/25000], Loss: 0.3054\n",
      "Epoch [1/1], Step [3250/25000], Loss: 0.7010\n",
      "Epoch [1/1], Step [3260/25000], Loss: 0.9530\n",
      "Epoch [1/1], Step [3270/25000], Loss: 0.7032\n",
      "Epoch [1/1], Step [3280/25000], Loss: 0.9055\n",
      "Epoch [1/1], Step [3290/25000], Loss: 0.2135\n",
      "Epoch [1/1], Step [3300/25000], Loss: 1.2059\n",
      "Epoch [1/1], Step [3310/25000], Loss: 0.7863\n",
      "Epoch [1/1], Step [3320/25000], Loss: 0.3572\n",
      "Epoch [1/1], Step [3330/25000], Loss: 0.3405\n",
      "Epoch [1/1], Step [3340/25000], Loss: 0.9428\n",
      "Epoch [1/1], Step [3350/25000], Loss: 0.8237\n",
      "Epoch [1/1], Step [3360/25000], Loss: 0.6425\n",
      "Epoch [1/1], Step [3370/25000], Loss: 0.2336\n",
      "Epoch [1/1], Step [3380/25000], Loss: 0.2388\n",
      "Epoch [1/1], Step [3390/25000], Loss: 0.9043\n",
      "Epoch [1/1], Step [3400/25000], Loss: 0.5491\n",
      "Epoch [1/1], Step [3410/25000], Loss: 0.2523\n",
      "Epoch [1/1], Step [3420/25000], Loss: 0.5591\n",
      "Epoch [1/1], Step [3430/25000], Loss: 0.5614\n",
      "Epoch [1/1], Step [3440/25000], Loss: 0.4049\n",
      "Epoch [1/1], Step [3450/25000], Loss: 0.5304\n",
      "Epoch [1/1], Step [3460/25000], Loss: 0.5474\n",
      "Epoch [1/1], Step [3470/25000], Loss: 0.4206\n",
      "Epoch [1/1], Step [3480/25000], Loss: 0.4448\n",
      "Epoch [1/1], Step [3490/25000], Loss: 0.2034\n",
      "Epoch [1/1], Step [3500/25000], Loss: 0.4985\n",
      "Epoch [1/1], Step [3510/25000], Loss: 0.6292\n",
      "Epoch [1/1], Step [3520/25000], Loss: 0.5625\n",
      "Epoch [1/1], Step [3530/25000], Loss: 0.5735\n",
      "Epoch [1/1], Step [3540/25000], Loss: 0.3409\n",
      "Epoch [1/1], Step [3550/25000], Loss: 0.2079\n",
      "Epoch [1/1], Step [3560/25000], Loss: 0.1970\n",
      "Epoch [1/1], Step [3570/25000], Loss: 0.2404\n",
      "Epoch [1/1], Step [3580/25000], Loss: 0.2936\n",
      "Epoch [1/1], Step [3590/25000], Loss: 0.2400\n",
      "Epoch [1/1], Step [3600/25000], Loss: 0.4865\n",
      "Epoch [1/1], Step [3610/25000], Loss: 0.5648\n",
      "Epoch [1/1], Step [3620/25000], Loss: 0.2620\n",
      "Epoch [1/1], Step [3630/25000], Loss: 0.4121\n",
      "Epoch [1/1], Step [3640/25000], Loss: 0.6069\n",
      "Epoch [1/1], Step [3650/25000], Loss: 0.5881\n",
      "Epoch [1/1], Step [3660/25000], Loss: 0.7289\n",
      "Epoch [1/1], Step [3670/25000], Loss: 0.3210\n",
      "Epoch [1/1], Step [3680/25000], Loss: 0.5319\n",
      "Epoch [1/1], Step [3690/25000], Loss: 1.1255\n",
      "Epoch [1/1], Step [3700/25000], Loss: 0.5615\n",
      "Epoch [1/1], Step [3710/25000], Loss: 0.3972\n",
      "Epoch [1/1], Step [3720/25000], Loss: 0.4395\n",
      "Epoch [1/1], Step [3730/25000], Loss: 0.2319\n",
      "Epoch [1/1], Step [3740/25000], Loss: 0.6723\n",
      "Epoch [1/1], Step [3750/25000], Loss: 0.2075\n",
      "Epoch [1/1], Step [3760/25000], Loss: 0.2803\n",
      "Epoch [1/1], Step [3770/25000], Loss: 0.1632\n",
      "Epoch [1/1], Step [3780/25000], Loss: 1.1659\n",
      "Epoch [1/1], Step [3790/25000], Loss: 0.5152\n",
      "Epoch [1/1], Step [3800/25000], Loss: 0.5660\n",
      "Epoch [1/1], Step [3810/25000], Loss: 0.3459\n",
      "Epoch [1/1], Step [3820/25000], Loss: 0.6689\n",
      "Epoch [1/1], Step [3830/25000], Loss: 0.1706\n",
      "Epoch [1/1], Step [3840/25000], Loss: 0.1751\n",
      "Epoch [1/1], Step [3850/25000], Loss: 0.9380\n",
      "Epoch [1/1], Step [3860/25000], Loss: 1.3090\n",
      "Epoch [1/1], Step [3870/25000], Loss: 0.4727\n",
      "Epoch [1/1], Step [3880/25000], Loss: 0.5970\n",
      "Epoch [1/1], Step [3890/25000], Loss: 0.3363\n",
      "Epoch [1/1], Step [3900/25000], Loss: 1.2191\n",
      "Epoch [1/1], Step [3910/25000], Loss: 0.5185\n",
      "Epoch [1/1], Step [3920/25000], Loss: 0.4504\n",
      "Epoch [1/1], Step [3930/25000], Loss: 0.1713\n",
      "Epoch [1/1], Step [3940/25000], Loss: 0.4150\n",
      "Epoch [1/1], Step [3950/25000], Loss: 0.1736\n",
      "Epoch [1/1], Step [3960/25000], Loss: 1.0566\n",
      "Epoch [1/1], Step [3970/25000], Loss: 0.7550\n",
      "Epoch [1/1], Step [3980/25000], Loss: 0.2718\n",
      "Epoch [1/1], Step [3990/25000], Loss: 0.2349\n",
      "Epoch [1/1], Step [4000/25000], Loss: 0.6739\n",
      "Epoch [1/1], Step [4010/25000], Loss: 0.8328\n",
      "Epoch [1/1], Step [4020/25000], Loss: 0.5510\n",
      "Epoch [1/1], Step [4030/25000], Loss: 0.2864\n",
      "Epoch [1/1], Step [4040/25000], Loss: 0.6900\n",
      "Epoch [1/1], Step [4050/25000], Loss: 0.7048\n",
      "Epoch [1/1], Step [4060/25000], Loss: 0.8738\n",
      "Epoch [1/1], Step [4070/25000], Loss: 0.5184\n",
      "Epoch [1/1], Step [4080/25000], Loss: 0.6536\n",
      "Epoch [1/1], Step [4090/25000], Loss: 0.1519\n",
      "Epoch [1/1], Step [4100/25000], Loss: 0.1731\n",
      "Epoch [1/1], Step [4110/25000], Loss: 0.8307\n",
      "Epoch [1/1], Step [4120/25000], Loss: 0.1796\n",
      "Epoch [1/1], Step [4130/25000], Loss: 0.9403\n",
      "Epoch [1/1], Step [4140/25000], Loss: 0.5252\n",
      "Epoch [1/1], Step [4150/25000], Loss: 0.8218\n",
      "Epoch [1/1], Step [4160/25000], Loss: 0.5449\n",
      "Epoch [1/1], Step [4170/25000], Loss: 0.6408\n",
      "Epoch [1/1], Step [4180/25000], Loss: 0.4666\n",
      "Epoch [1/1], Step [4190/25000], Loss: 0.4883\n",
      "Epoch [1/1], Step [4200/25000], Loss: 0.4070\n",
      "Epoch [1/1], Step [4210/25000], Loss: 0.1918\n",
      "Epoch [1/1], Step [4220/25000], Loss: 0.1250\n",
      "Epoch [1/1], Step [4230/25000], Loss: 0.6020\n",
      "Epoch [1/1], Step [4240/25000], Loss: 0.1959\n",
      "Epoch [1/1], Step [4250/25000], Loss: 0.6850\n",
      "Epoch [1/1], Step [4260/25000], Loss: 0.5866\n",
      "Epoch [1/1], Step [4270/25000], Loss: 0.2289\n",
      "Epoch [1/1], Step [4280/25000], Loss: 0.2637\n",
      "Epoch [1/1], Step [4290/25000], Loss: 0.3333\n",
      "Epoch [1/1], Step [4300/25000], Loss: 0.4522\n",
      "Epoch [1/1], Step [4310/25000], Loss: 0.5990\n",
      "Epoch [1/1], Step [4320/25000], Loss: 0.5799\n",
      "Epoch [1/1], Step [4330/25000], Loss: 0.6455\n",
      "Epoch [1/1], Step [4340/25000], Loss: 0.4830\n",
      "Epoch [1/1], Step [4350/25000], Loss: 0.5793\n",
      "Epoch [1/1], Step [4360/25000], Loss: 0.6218\n",
      "Epoch [1/1], Step [4370/25000], Loss: 0.2564\n",
      "Epoch [1/1], Step [4380/25000], Loss: 0.3148\n",
      "Epoch [1/1], Step [4390/25000], Loss: 0.5133\n",
      "Epoch [1/1], Step [4400/25000], Loss: 0.3089\n",
      "Epoch [1/1], Step [4410/25000], Loss: 0.0723\n",
      "Epoch [1/1], Step [4420/25000], Loss: 0.1343\n",
      "Epoch [1/1], Step [4430/25000], Loss: 1.1149\n",
      "Epoch [1/1], Step [4440/25000], Loss: 0.1642\n",
      "Epoch [1/1], Step [4450/25000], Loss: 0.3643\n",
      "Epoch [1/1], Step [4460/25000], Loss: 0.5243\n",
      "Epoch [1/1], Step [4470/25000], Loss: 0.4848\n",
      "Epoch [1/1], Step [4480/25000], Loss: 0.5030\n",
      "Epoch [1/1], Step [4490/25000], Loss: 0.3100\n",
      "Epoch [1/1], Step [4500/25000], Loss: 1.0920\n",
      "Epoch [1/1], Step [4510/25000], Loss: 0.7439\n",
      "Epoch [1/1], Step [4520/25000], Loss: 0.3329\n",
      "Epoch [1/1], Step [4530/25000], Loss: 0.2286\n",
      "Epoch [1/1], Step [4540/25000], Loss: 0.9681\n",
      "Epoch [1/1], Step [4550/25000], Loss: 0.5161\n",
      "Epoch [1/1], Step [4560/25000], Loss: 0.4272\n",
      "Epoch [1/1], Step [4570/25000], Loss: 0.5528\n",
      "Epoch [1/1], Step [4580/25000], Loss: 0.3183\n",
      "Epoch [1/1], Step [4590/25000], Loss: 0.0805\n",
      "Epoch [1/1], Step [4600/25000], Loss: 0.5686\n",
      "Epoch [1/1], Step [4610/25000], Loss: 0.6644\n",
      "Epoch [1/1], Step [4620/25000], Loss: 0.1798\n",
      "Epoch [1/1], Step [4630/25000], Loss: 0.0768\n",
      "Epoch [1/1], Step [4640/25000], Loss: 0.5481\n",
      "Epoch [1/1], Step [4650/25000], Loss: 0.1987\n",
      "Epoch [1/1], Step [4660/25000], Loss: 0.1705\n",
      "Epoch [1/1], Step [4670/25000], Loss: 0.1140\n",
      "Epoch [1/1], Step [4680/25000], Loss: 0.3612\n",
      "Epoch [1/1], Step [4690/25000], Loss: 0.5169\n",
      "Epoch [1/1], Step [4700/25000], Loss: 0.4013\n",
      "Epoch [1/1], Step [4710/25000], Loss: 0.2422\n",
      "Epoch [1/1], Step [4720/25000], Loss: 0.7778\n",
      "Epoch [1/1], Step [4730/25000], Loss: 0.4291\n",
      "Epoch [1/1], Step [4740/25000], Loss: 0.4485\n",
      "Epoch [1/1], Step [4750/25000], Loss: 0.8149\n",
      "Epoch [1/1], Step [4760/25000], Loss: 0.4574\n",
      "Epoch [1/1], Step [4770/25000], Loss: 0.3879\n",
      "Epoch [1/1], Step [4780/25000], Loss: 0.2380\n",
      "Epoch [1/1], Step [4790/25000], Loss: 0.5967\n",
      "Epoch [1/1], Step [4800/25000], Loss: 0.5003\n",
      "Epoch [1/1], Step [4810/25000], Loss: 0.8240\n",
      "Epoch [1/1], Step [4820/25000], Loss: 0.3458\n",
      "Epoch [1/1], Step [4830/25000], Loss: 0.3361\n",
      "Epoch [1/1], Step [4840/25000], Loss: 0.0912\n",
      "Epoch [1/1], Step [4850/25000], Loss: 0.6856\n",
      "Epoch [1/1], Step [4860/25000], Loss: 0.2373\n",
      "Epoch [1/1], Step [4870/25000], Loss: 0.1133\n",
      "Epoch [1/1], Step [4880/25000], Loss: 0.1156\n",
      "Epoch [1/1], Step [4890/25000], Loss: 0.3159\n",
      "Epoch [1/1], Step [4900/25000], Loss: 0.3624\n",
      "Epoch [1/1], Step [4910/25000], Loss: 0.1120\n",
      "Epoch [1/1], Step [4920/25000], Loss: 0.0978\n",
      "Epoch [1/1], Step [4930/25000], Loss: 0.4224\n",
      "Epoch [1/1], Step [4940/25000], Loss: 0.4253\n",
      "Epoch [1/1], Step [4950/25000], Loss: 0.2415\n",
      "Epoch [1/1], Step [4960/25000], Loss: 0.2631\n",
      "Epoch [1/1], Step [4970/25000], Loss: 0.8962\n",
      "Epoch [1/1], Step [4980/25000], Loss: 0.4311\n",
      "Epoch [1/1], Step [4990/25000], Loss: 0.3539\n",
      "Epoch [1/1], Step [5000/25000], Loss: 0.3772\n",
      "Epoch [1/1], Step [5010/25000], Loss: 0.3864\n",
      "Epoch [1/1], Step [5020/25000], Loss: 0.1230\n",
      "Epoch [1/1], Step [5030/25000], Loss: 0.2110\n",
      "Epoch [1/1], Step [5040/25000], Loss: 0.2975\n",
      "Epoch [1/1], Step [5050/25000], Loss: 1.1760\n",
      "Epoch [1/1], Step [5060/25000], Loss: 0.1494\n",
      "Epoch [1/1], Step [5070/25000], Loss: 0.9370\n",
      "Epoch [1/1], Step [5080/25000], Loss: 0.2563\n",
      "Epoch [1/1], Step [5090/25000], Loss: 0.2374\n",
      "Epoch [1/1], Step [5100/25000], Loss: 0.7197\n",
      "Epoch [1/1], Step [5110/25000], Loss: 0.4814\n",
      "Epoch [1/1], Step [5120/25000], Loss: 0.3451\n",
      "Epoch [1/1], Step [5130/25000], Loss: 0.7762\n",
      "Epoch [1/1], Step [5140/25000], Loss: 0.9832\n",
      "Epoch [1/1], Step [5150/25000], Loss: 0.2862\n",
      "Epoch [1/1], Step [5160/25000], Loss: 0.3377\n",
      "Epoch [1/1], Step [5170/25000], Loss: 0.1229\n",
      "Epoch [1/1], Step [5180/25000], Loss: 0.2538\n",
      "Epoch [1/1], Step [5190/25000], Loss: 1.1168\n",
      "Epoch [1/1], Step [5200/25000], Loss: 0.2556\n",
      "Epoch [1/1], Step [5210/25000], Loss: 0.5864\n",
      "Epoch [1/1], Step [5220/25000], Loss: 0.1610\n",
      "Epoch [1/1], Step [5230/25000], Loss: 0.3489\n",
      "Epoch [1/1], Step [5240/25000], Loss: 0.1950\n",
      "Epoch [1/1], Step [5250/25000], Loss: 0.8498\n",
      "Epoch [1/1], Step [5260/25000], Loss: 0.6163\n",
      "Epoch [1/1], Step [5270/25000], Loss: 0.6214\n",
      "Epoch [1/1], Step [5280/25000], Loss: 0.3207\n",
      "Epoch [1/1], Step [5290/25000], Loss: 0.1633\n",
      "Epoch [1/1], Step [5300/25000], Loss: 0.3867\n",
      "Epoch [1/1], Step [5310/25000], Loss: 0.8414\n",
      "Epoch [1/1], Step [5320/25000], Loss: 0.6409\n",
      "Epoch [1/1], Step [5330/25000], Loss: 0.4082\n",
      "Epoch [1/1], Step [5340/25000], Loss: 0.3083\n",
      "Epoch [1/1], Step [5350/25000], Loss: 0.6744\n",
      "Epoch [1/1], Step [5360/25000], Loss: 0.2807\n",
      "Epoch [1/1], Step [5370/25000], Loss: 0.7884\n",
      "Epoch [1/1], Step [5380/25000], Loss: 0.2319\n",
      "Epoch [1/1], Step [5390/25000], Loss: 0.3871\n",
      "Epoch [1/1], Step [5400/25000], Loss: 0.5539\n",
      "Epoch [1/1], Step [5410/25000], Loss: 0.3023\n",
      "Epoch [1/1], Step [5420/25000], Loss: 0.2509\n",
      "Epoch [1/1], Step [5430/25000], Loss: 1.0174\n",
      "Epoch [1/1], Step [5440/25000], Loss: 0.6604\n",
      "Epoch [1/1], Step [5450/25000], Loss: 0.0867\n",
      "Epoch [1/1], Step [5460/25000], Loss: 0.7247\n",
      "Epoch [1/1], Step [5470/25000], Loss: 0.4410\n",
      "Epoch [1/1], Step [5480/25000], Loss: 0.6198\n",
      "Epoch [1/1], Step [5490/25000], Loss: 0.3463\n",
      "Epoch [1/1], Step [5500/25000], Loss: 0.9804\n",
      "Epoch [1/1], Step [5510/25000], Loss: 0.2026\n",
      "Epoch [1/1], Step [5520/25000], Loss: 0.3896\n",
      "Epoch [1/1], Step [5530/25000], Loss: 0.6147\n",
      "Epoch [1/1], Step [5540/25000], Loss: 0.4619\n",
      "Epoch [1/1], Step [5550/25000], Loss: 0.2335\n",
      "Epoch [1/1], Step [5560/25000], Loss: 0.9174\n",
      "Epoch [1/1], Step [5570/25000], Loss: 0.1042\n",
      "Epoch [1/1], Step [5580/25000], Loss: 1.0943\n",
      "Epoch [1/1], Step [5590/25000], Loss: 0.3830\n",
      "Epoch [1/1], Step [5600/25000], Loss: 0.5821\n",
      "Epoch [1/1], Step [5610/25000], Loss: 0.5341\n",
      "Epoch [1/1], Step [5620/25000], Loss: 0.3215\n",
      "Epoch [1/1], Step [5630/25000], Loss: 0.3739\n",
      "Epoch [1/1], Step [5640/25000], Loss: 0.5896\n",
      "Epoch [1/1], Step [5650/25000], Loss: 0.5713\n",
      "Epoch [1/1], Step [5660/25000], Loss: 0.2774\n",
      "Epoch [1/1], Step [5670/25000], Loss: 0.5996\n",
      "Epoch [1/1], Step [5680/25000], Loss: 0.2619\n",
      "Epoch [1/1], Step [5690/25000], Loss: 0.9460\n",
      "Epoch [1/1], Step [5700/25000], Loss: 0.1719\n",
      "Epoch [1/1], Step [5710/25000], Loss: 0.2037\n",
      "Epoch [1/1], Step [5720/25000], Loss: 0.9711\n",
      "Epoch [1/1], Step [5730/25000], Loss: 0.2079\n",
      "Epoch [1/1], Step [5740/25000], Loss: 0.6301\n",
      "Epoch [1/1], Step [5750/25000], Loss: 0.5614\n",
      "Epoch [1/1], Step [5760/25000], Loss: 0.7710\n",
      "Epoch [1/1], Step [5770/25000], Loss: 0.5606\n",
      "Epoch [1/1], Step [5780/25000], Loss: 0.2343\n",
      "Epoch [1/1], Step [5790/25000], Loss: 0.2289\n",
      "Epoch [1/1], Step [5800/25000], Loss: 0.2228\n",
      "Epoch [1/1], Step [5810/25000], Loss: 0.5634\n",
      "Epoch [1/1], Step [5820/25000], Loss: 0.5884\n",
      "Epoch [1/1], Step [5830/25000], Loss: 0.5916\n",
      "Epoch [1/1], Step [5840/25000], Loss: 0.5947\n",
      "Epoch [1/1], Step [5850/25000], Loss: 0.9771\n",
      "Epoch [1/1], Step [5860/25000], Loss: 0.3439\n",
      "Epoch [1/1], Step [5870/25000], Loss: 0.6498\n",
      "Epoch [1/1], Step [5880/25000], Loss: 0.5921\n",
      "Epoch [1/1], Step [5890/25000], Loss: 0.5566\n",
      "Epoch [1/1], Step [5900/25000], Loss: 0.3020\n",
      "Epoch [1/1], Step [5910/25000], Loss: 0.5476\n",
      "Epoch [1/1], Step [5920/25000], Loss: 0.6077\n",
      "Epoch [1/1], Step [5930/25000], Loss: 0.2227\n",
      "Epoch [1/1], Step [5940/25000], Loss: 0.1742\n",
      "Epoch [1/1], Step [5950/25000], Loss: 0.7168\n",
      "Epoch [1/1], Step [5960/25000], Loss: 0.5675\n",
      "Epoch [1/1], Step [5970/25000], Loss: 0.1607\n",
      "Epoch [1/1], Step [5980/25000], Loss: 0.5752\n",
      "Epoch [1/1], Step [5990/25000], Loss: 0.1848\n",
      "Epoch [1/1], Step [6000/25000], Loss: 0.3503\n",
      "Epoch [1/1], Step [6010/25000], Loss: 0.1621\n",
      "Epoch [1/1], Step [6020/25000], Loss: 0.4891\n",
      "Epoch [1/1], Step [6030/25000], Loss: 0.6195\n",
      "Epoch [1/1], Step [6040/25000], Loss: 0.6129\n",
      "Epoch [1/1], Step [6050/25000], Loss: 0.1452\n",
      "Epoch [1/1], Step [6060/25000], Loss: 1.0147\n",
      "Epoch [1/1], Step [6070/25000], Loss: 0.2073\n",
      "Epoch [1/1], Step [6080/25000], Loss: 0.1434\n",
      "Epoch [1/1], Step [6090/25000], Loss: 0.8300\n",
      "Epoch [1/1], Step [6100/25000], Loss: 0.1805\n",
      "Epoch [1/1], Step [6110/25000], Loss: 0.1692\n",
      "Epoch [1/1], Step [6120/25000], Loss: 0.9412\n",
      "Epoch [1/1], Step [6130/25000], Loss: 0.1227\n",
      "Epoch [1/1], Step [6140/25000], Loss: 0.1252\n",
      "Epoch [1/1], Step [6150/25000], Loss: 0.6022\n",
      "Epoch [1/1], Step [6160/25000], Loss: 0.1984\n",
      "Epoch [1/1], Step [6170/25000], Loss: 0.2022\n",
      "Epoch [1/1], Step [6180/25000], Loss: 0.8402\n",
      "Epoch [1/1], Step [6190/25000], Loss: 0.1945\n",
      "Epoch [1/1], Step [6200/25000], Loss: 0.2925\n",
      "Epoch [1/1], Step [6210/25000], Loss: 0.5485\n",
      "Epoch [1/1], Step [6220/25000], Loss: 0.2125\n",
      "Epoch [1/1], Step [6230/25000], Loss: 0.5786\n",
      "Epoch [1/1], Step [6240/25000], Loss: 0.5805\n",
      "Epoch [1/1], Step [6250/25000], Loss: 0.5304\n",
      "Epoch [1/1], Step [6260/25000], Loss: 0.7597\n",
      "Epoch [1/1], Step [6270/25000], Loss: 0.4965\n",
      "Epoch [1/1], Step [6280/25000], Loss: 0.8643\n",
      "Epoch [1/1], Step [6290/25000], Loss: 0.2856\n",
      "Epoch [1/1], Step [6300/25000], Loss: 0.2624\n",
      "Epoch [1/1], Step [6310/25000], Loss: 0.8433\n",
      "Epoch [1/1], Step [6320/25000], Loss: 0.2586\n",
      "Epoch [1/1], Step [6330/25000], Loss: 0.3937\n",
      "Epoch [1/1], Step [6340/25000], Loss: 0.5249\n",
      "Epoch [1/1], Step [6350/25000], Loss: 0.2295\n",
      "Epoch [1/1], Step [6360/25000], Loss: 0.2266\n",
      "Epoch [1/1], Step [6370/25000], Loss: 0.5114\n",
      "Epoch [1/1], Step [6380/25000], Loss: 0.5628\n",
      "Epoch [1/1], Step [6390/25000], Loss: 0.2218\n",
      "Epoch [1/1], Step [6400/25000], Loss: 0.5426\n",
      "Epoch [1/1], Step [6410/25000], Loss: 0.5250\n",
      "Epoch [1/1], Step [6420/25000], Loss: 0.5150\n",
      "Epoch [1/1], Step [6430/25000], Loss: 0.5002\n",
      "Epoch [1/1], Step [6440/25000], Loss: 0.5337\n",
      "Epoch [1/1], Step [6450/25000], Loss: 0.6058\n",
      "Epoch [1/1], Step [6460/25000], Loss: 0.7529\n",
      "Epoch [1/1], Step [6470/25000], Loss: 0.6846\n",
      "Epoch [1/1], Step [6480/25000], Loss: 0.3772\n",
      "Epoch [1/1], Step [6490/25000], Loss: 0.3149\n",
      "Epoch [1/1], Step [6500/25000], Loss: 0.5598\n",
      "Epoch [1/1], Step [6510/25000], Loss: 0.7788\n",
      "Epoch [1/1], Step [6520/25000], Loss: 0.3594\n",
      "Epoch [1/1], Step [6530/25000], Loss: 0.3123\n",
      "Epoch [1/1], Step [6540/25000], Loss: 0.8251\n",
      "Epoch [1/1], Step [6550/25000], Loss: 0.3709\n",
      "Epoch [1/1], Step [6560/25000], Loss: 0.5071\n",
      "Epoch [1/1], Step [6570/25000], Loss: 0.6128\n",
      "Epoch [1/1], Step [6580/25000], Loss: 0.4858\n",
      "Epoch [1/1], Step [6590/25000], Loss: 0.9088\n",
      "Epoch [1/1], Step [6600/25000], Loss: 0.7926\n",
      "Epoch [1/1], Step [6610/25000], Loss: 0.3984\n",
      "Epoch [1/1], Step [6620/25000], Loss: 0.7683\n",
      "Epoch [1/1], Step [6630/25000], Loss: 0.5882\n",
      "Epoch [1/1], Step [6640/25000], Loss: 0.5609\n",
      "Epoch [1/1], Step [6650/25000], Loss: 0.1012\n",
      "Epoch [1/1], Step [6660/25000], Loss: 0.2622\n",
      "Epoch [1/1], Step [6670/25000], Loss: 0.3786\n",
      "Epoch [1/1], Step [6680/25000], Loss: 0.0988\n",
      "Epoch [1/1], Step [6690/25000], Loss: 0.0768\n",
      "Epoch [1/1], Step [6700/25000], Loss: 0.1226\n",
      "Epoch [1/1], Step [6710/25000], Loss: 0.6924\n",
      "Epoch [1/1], Step [6720/25000], Loss: 1.1390\n",
      "Epoch [1/1], Step [6730/25000], Loss: 0.7143\n",
      "Epoch [1/1], Step [6740/25000], Loss: 0.8538\n",
      "Epoch [1/1], Step [6750/25000], Loss: 0.0803\n",
      "Epoch [1/1], Step [6760/25000], Loss: 0.2026\n",
      "Epoch [1/1], Step [6770/25000], Loss: 0.5014\n",
      "Epoch [1/1], Step [6780/25000], Loss: 0.7193\n",
      "Epoch [1/1], Step [6790/25000], Loss: 0.2652\n",
      "Epoch [1/1], Step [6800/25000], Loss: 0.4085\n",
      "Epoch [1/1], Step [6810/25000], Loss: 0.1968\n",
      "Epoch [1/1], Step [6820/25000], Loss: 0.6292\n",
      "Epoch [1/1], Step [6830/25000], Loss: 0.3272\n",
      "Epoch [1/1], Step [6840/25000], Loss: 0.4369\n",
      "Epoch [1/1], Step [6850/25000], Loss: 0.5970\n",
      "Epoch [1/1], Step [6860/25000], Loss: 0.2546\n",
      "Epoch [1/1], Step [6870/25000], Loss: 0.2474\n",
      "Epoch [1/1], Step [6880/25000], Loss: 0.5612\n",
      "Epoch [1/1], Step [6890/25000], Loss: 0.3906\n",
      "Epoch [1/1], Step [6900/25000], Loss: 0.3316\n",
      "Epoch [1/1], Step [6910/25000], Loss: 0.5183\n",
      "Epoch [1/1], Step [6920/25000], Loss: 0.2466\n",
      "Epoch [1/1], Step [6930/25000], Loss: 0.5042\n",
      "Epoch [1/1], Step [6940/25000], Loss: 0.6591\n",
      "Epoch [1/1], Step [6950/25000], Loss: 0.2520\n",
      "Epoch [1/1], Step [6960/25000], Loss: 0.3986\n",
      "Epoch [1/1], Step [6970/25000], Loss: 1.3094\n",
      "Epoch [1/1], Step [6980/25000], Loss: 0.9863\n",
      "Epoch [1/1], Step [6990/25000], Loss: 0.5804\n",
      "Epoch [1/1], Step [7000/25000], Loss: 0.2611\n",
      "Epoch [1/1], Step [7010/25000], Loss: 0.6632\n",
      "Epoch [1/1], Step [7020/25000], Loss: 0.4532\n",
      "Epoch [1/1], Step [7030/25000], Loss: 0.4232\n",
      "Epoch [1/1], Step [7040/25000], Loss: 0.4715\n",
      "Epoch [1/1], Step [7050/25000], Loss: 0.3119\n",
      "Epoch [1/1], Step [7060/25000], Loss: 0.5799\n",
      "Epoch [1/1], Step [7070/25000], Loss: 0.5486\n",
      "Epoch [1/1], Step [7080/25000], Loss: 0.2886\n",
      "Epoch [1/1], Step [7090/25000], Loss: 0.8909\n",
      "Epoch [1/1], Step [7100/25000], Loss: 0.3776\n",
      "Epoch [1/1], Step [7110/25000], Loss: 0.4351\n",
      "Epoch [1/1], Step [7120/25000], Loss: 0.4525\n",
      "Epoch [1/1], Step [7130/25000], Loss: 0.5461\n",
      "Epoch [1/1], Step [7140/25000], Loss: 1.0948\n",
      "Epoch [1/1], Step [7150/25000], Loss: 0.4301\n",
      "Epoch [1/1], Step [7160/25000], Loss: 0.3942\n",
      "Epoch [1/1], Step [7170/25000], Loss: 0.3110\n",
      "Epoch [1/1], Step [7180/25000], Loss: 0.4055\n",
      "Epoch [1/1], Step [7190/25000], Loss: 0.0914\n",
      "Epoch [1/1], Step [7200/25000], Loss: 0.5043\n",
      "Epoch [1/1], Step [7210/25000], Loss: 0.1681\n",
      "Epoch [1/1], Step [7220/25000], Loss: 0.1348\n",
      "Epoch [1/1], Step [7230/25000], Loss: 0.0948\n",
      "Epoch [1/1], Step [7240/25000], Loss: 0.8830\n",
      "Epoch [1/1], Step [7250/25000], Loss: 0.5610\n",
      "Epoch [1/1], Step [7260/25000], Loss: 0.4634\n",
      "Epoch [1/1], Step [7270/25000], Loss: 0.7631\n",
      "Epoch [1/1], Step [7280/25000], Loss: 0.4985\n",
      "Epoch [1/1], Step [7290/25000], Loss: 0.2235\n",
      "Epoch [1/1], Step [7300/25000], Loss: 0.2857\n",
      "Epoch [1/1], Step [7310/25000], Loss: 0.2958\n",
      "Epoch [1/1], Step [7320/25000], Loss: 0.4215\n",
      "Epoch [1/1], Step [7330/25000], Loss: 0.6023\n",
      "Epoch [1/1], Step [7340/25000], Loss: 0.3115\n",
      "Epoch [1/1], Step [7350/25000], Loss: 0.3671\n",
      "Epoch [1/1], Step [7360/25000], Loss: 0.5984\n",
      "Epoch [1/1], Step [7370/25000], Loss: 0.9827\n",
      "Epoch [1/1], Step [7380/25000], Loss: 0.3642\n",
      "Epoch [1/1], Step [7390/25000], Loss: 0.2404\n",
      "Epoch [1/1], Step [7400/25000], Loss: 0.2967\n",
      "Epoch [1/1], Step [7410/25000], Loss: 0.8538\n",
      "Epoch [1/1], Step [7420/25000], Loss: 0.5321\n",
      "Epoch [1/1], Step [7430/25000], Loss: 0.6501\n",
      "Epoch [1/1], Step [7440/25000], Loss: 0.2467\n",
      "Epoch [1/1], Step [7450/25000], Loss: 0.5223\n",
      "Epoch [1/1], Step [7460/25000], Loss: 0.4026\n",
      "Epoch [1/1], Step [7470/25000], Loss: 0.2645\n",
      "Epoch [1/1], Step [7480/25000], Loss: 0.6402\n",
      "Epoch [1/1], Step [7490/25000], Loss: 0.7280\n",
      "Epoch [1/1], Step [7500/25000], Loss: 0.2840\n",
      "Epoch [1/1], Step [7510/25000], Loss: 0.5236\n",
      "Epoch [1/1], Step [7520/25000], Loss: 0.7415\n",
      "Epoch [1/1], Step [7530/25000], Loss: 0.3994\n",
      "Epoch [1/1], Step [7540/25000], Loss: 0.5347\n",
      "Epoch [1/1], Step [7550/25000], Loss: 0.3972\n",
      "Epoch [1/1], Step [7560/25000], Loss: 0.4724\n",
      "Epoch [1/1], Step [7570/25000], Loss: 0.1049\n",
      "Epoch [1/1], Step [7580/25000], Loss: 0.1031\n",
      "Epoch [1/1], Step [7590/25000], Loss: 0.6289\n",
      "Epoch [1/1], Step [7600/25000], Loss: 0.1076\n",
      "Epoch [1/1], Step [7610/25000], Loss: 0.0859\n",
      "Epoch [1/1], Step [7620/25000], Loss: 0.0999\n",
      "Epoch [1/1], Step [7630/25000], Loss: 0.3843\n",
      "Epoch [1/1], Step [7640/25000], Loss: 0.1091\n",
      "Epoch [1/1], Step [7650/25000], Loss: 0.3783\n",
      "Epoch [1/1], Step [7660/25000], Loss: 0.8960\n",
      "Epoch [1/1], Step [7670/25000], Loss: 0.2386\n",
      "Epoch [1/1], Step [7680/25000], Loss: 0.5189\n",
      "Epoch [1/1], Step [7690/25000], Loss: 0.3997\n",
      "Epoch [1/1], Step [7700/25000], Loss: 0.1032\n",
      "Epoch [1/1], Step [7710/25000], Loss: 0.6808\n",
      "Epoch [1/1], Step [7720/25000], Loss: 0.2630\n",
      "Epoch [1/1], Step [7730/25000], Loss: 0.8294\n",
      "Epoch [1/1], Step [7740/25000], Loss: 0.3076\n",
      "Epoch [1/1], Step [7750/25000], Loss: 0.5830\n",
      "Epoch [1/1], Step [7760/25000], Loss: 0.1809\n",
      "Epoch [1/1], Step [7770/25000], Loss: 0.2646\n",
      "Epoch [1/1], Step [7780/25000], Loss: 0.5278\n",
      "Epoch [1/1], Step [7790/25000], Loss: 0.8829\n",
      "Epoch [1/1], Step [7800/25000], Loss: 0.5230\n",
      "Epoch [1/1], Step [7810/25000], Loss: 0.8153\n",
      "Epoch [1/1], Step [7820/25000], Loss: 0.3580\n",
      "Epoch [1/1], Step [7830/25000], Loss: 0.2215\n",
      "Epoch [1/1], Step [7840/25000], Loss: 0.4500\n",
      "Epoch [1/1], Step [7850/25000], Loss: 0.5470\n",
      "Epoch [1/1], Step [7860/25000], Loss: 0.2904\n",
      "Epoch [1/1], Step [7870/25000], Loss: 0.2665\n",
      "Epoch [1/1], Step [7880/25000], Loss: 0.8018\n",
      "Epoch [1/1], Step [7890/25000], Loss: 0.9245\n",
      "Epoch [1/1], Step [7900/25000], Loss: 0.2788\n",
      "Epoch [1/1], Step [7910/25000], Loss: 1.1307\n",
      "Epoch [1/1], Step [7920/25000], Loss: 0.3090\n",
      "Epoch [1/1], Step [7930/25000], Loss: 0.1178\n",
      "Epoch [1/1], Step [7940/25000], Loss: 0.3145\n",
      "Epoch [1/1], Step [7950/25000], Loss: 0.8182\n",
      "Epoch [1/1], Step [7960/25000], Loss: 0.4921\n",
      "Epoch [1/1], Step [7970/25000], Loss: 0.4928\n",
      "Epoch [1/1], Step [7980/25000], Loss: 0.8419\n",
      "Epoch [1/1], Step [7990/25000], Loss: 0.7206\n",
      "Epoch [1/1], Step [8000/25000], Loss: 0.6837\n",
      "Epoch [1/1], Step [8010/25000], Loss: 0.4362\n",
      "Epoch [1/1], Step [8020/25000], Loss: 0.7090\n",
      "Epoch [1/1], Step [8030/25000], Loss: 0.6643\n",
      "Epoch [1/1], Step [8040/25000], Loss: 0.5375\n",
      "Epoch [1/1], Step [8050/25000], Loss: 0.4754\n",
      "Epoch [1/1], Step [8060/25000], Loss: 0.3665\n",
      "Epoch [1/1], Step [8070/25000], Loss: 0.2921\n",
      "Epoch [1/1], Step [8080/25000], Loss: 0.1864\n",
      "Epoch [1/1], Step [8090/25000], Loss: 0.1501\n",
      "Epoch [1/1], Step [8100/25000], Loss: 0.2716\n",
      "Epoch [1/1], Step [8110/25000], Loss: 0.6324\n",
      "Epoch [1/1], Step [8120/25000], Loss: 0.9453\n",
      "Epoch [1/1], Step [8130/25000], Loss: 0.6699\n",
      "Epoch [1/1], Step [8140/25000], Loss: 0.8318\n",
      "Epoch [1/1], Step [8150/25000], Loss: 1.1100\n",
      "Epoch [1/1], Step [8160/25000], Loss: 0.4385\n",
      "Epoch [1/1], Step [8170/25000], Loss: 0.8447\n",
      "Epoch [1/1], Step [8180/25000], Loss: 0.2366\n",
      "Epoch [1/1], Step [8190/25000], Loss: 0.5018\n",
      "Epoch [1/1], Step [8200/25000], Loss: 0.2862\n",
      "Epoch [1/1], Step [8210/25000], Loss: 0.3257\n",
      "Epoch [1/1], Step [8220/25000], Loss: 0.4390\n",
      "Epoch [1/1], Step [8230/25000], Loss: 0.1921\n",
      "Epoch [1/1], Step [8240/25000], Loss: 0.1980\n",
      "Epoch [1/1], Step [8250/25000], Loss: 0.1739\n",
      "Epoch [1/1], Step [8260/25000], Loss: 0.4269\n",
      "Epoch [1/1], Step [8270/25000], Loss: 0.3635\n",
      "Epoch [1/1], Step [8280/25000], Loss: 0.4504\n",
      "Epoch [1/1], Step [8290/25000], Loss: 0.2530\n",
      "Epoch [1/1], Step [8300/25000], Loss: 0.2915\n",
      "Epoch [1/1], Step [8310/25000], Loss: 0.5965\n",
      "Epoch [1/1], Step [8320/25000], Loss: 0.1257\n",
      "Epoch [1/1], Step [8330/25000], Loss: 0.3499\n",
      "Epoch [1/1], Step [8340/25000], Loss: 0.4312\n",
      "Epoch [1/1], Step [8350/25000], Loss: 0.2448\n",
      "Epoch [1/1], Step [8360/25000], Loss: 0.2349\n",
      "Epoch [1/1], Step [8370/25000], Loss: 0.4708\n",
      "Epoch [1/1], Step [8380/25000], Loss: 0.4092\n",
      "Epoch [1/1], Step [8390/25000], Loss: 0.2762\n",
      "Epoch [1/1], Step [8400/25000], Loss: 0.2645\n",
      "Epoch [1/1], Step [8410/25000], Loss: 0.2157\n",
      "Epoch [1/1], Step [8420/25000], Loss: 1.0187\n",
      "Epoch [1/1], Step [8430/25000], Loss: 0.0702\n",
      "Epoch [1/1], Step [8440/25000], Loss: 0.1557\n",
      "Epoch [1/1], Step [8450/25000], Loss: 0.2903\n",
      "Epoch [1/1], Step [8460/25000], Loss: 1.6214\n",
      "Epoch [1/1], Step [8470/25000], Loss: 0.9923\n",
      "Epoch [1/1], Step [8480/25000], Loss: 0.4970\n",
      "Epoch [1/1], Step [8490/25000], Loss: 0.2000\n",
      "Epoch [1/1], Step [8500/25000], Loss: 0.6179\n",
      "Epoch [1/1], Step [8510/25000], Loss: 0.3532\n",
      "Epoch [1/1], Step [8520/25000], Loss: 0.3882\n",
      "Epoch [1/1], Step [8530/25000], Loss: 0.5711\n",
      "Epoch [1/1], Step [8540/25000], Loss: 0.9701\n",
      "Epoch [1/1], Step [8550/25000], Loss: 0.3256\n",
      "Epoch [1/1], Step [8560/25000], Loss: 0.3947\n",
      "Epoch [1/1], Step [8570/25000], Loss: 0.0851\n",
      "Epoch [1/1], Step [8580/25000], Loss: 0.2109\n",
      "Epoch [1/1], Step [8590/25000], Loss: 0.7849\n",
      "Epoch [1/1], Step [8600/25000], Loss: 0.6087\n",
      "Epoch [1/1], Step [8610/25000], Loss: 0.4736\n",
      "Epoch [1/1], Step [8620/25000], Loss: 0.7841\n",
      "Epoch [1/1], Step [8630/25000], Loss: 0.2325\n",
      "Epoch [1/1], Step [8640/25000], Loss: 0.1278\n",
      "Epoch [1/1], Step [8650/25000], Loss: 0.9241\n",
      "Epoch [1/1], Step [8660/25000], Loss: 0.5381\n",
      "Epoch [1/1], Step [8670/25000], Loss: 0.2124\n",
      "Epoch [1/1], Step [8680/25000], Loss: 0.2662\n",
      "Epoch [1/1], Step [8690/25000], Loss: 0.0851\n",
      "Epoch [1/1], Step [8700/25000], Loss: 0.4102\n",
      "Epoch [1/1], Step [8710/25000], Loss: 0.6813\n",
      "Epoch [1/1], Step [8720/25000], Loss: 0.1293\n",
      "Epoch [1/1], Step [8730/25000], Loss: 0.9419\n",
      "Epoch [1/1], Step [8740/25000], Loss: 0.5936\n",
      "Epoch [1/1], Step [8750/25000], Loss: 0.1651\n",
      "Epoch [1/1], Step [8760/25000], Loss: 0.1715\n",
      "Epoch [1/1], Step [8770/25000], Loss: 0.7291\n",
      "Epoch [1/1], Step [8780/25000], Loss: 0.7903\n",
      "Epoch [1/1], Step [8790/25000], Loss: 1.2084\n",
      "Epoch [1/1], Step [8800/25000], Loss: 0.2726\n",
      "Epoch [1/1], Step [8810/25000], Loss: 0.1669\n",
      "Epoch [1/1], Step [8820/25000], Loss: 0.8384\n",
      "Epoch [1/1], Step [8830/25000], Loss: 0.3039\n",
      "Epoch [1/1], Step [8840/25000], Loss: 0.2281\n",
      "Epoch [1/1], Step [8850/25000], Loss: 0.6763\n",
      "Epoch [1/1], Step [8860/25000], Loss: 0.7308\n",
      "Epoch [1/1], Step [8870/25000], Loss: 0.2932\n",
      "Epoch [1/1], Step [8880/25000], Loss: 0.3425\n",
      "Epoch [1/1], Step [8890/25000], Loss: 0.6099\n",
      "Epoch [1/1], Step [8900/25000], Loss: 0.6301\n",
      "Epoch [1/1], Step [8910/25000], Loss: 0.2047\n",
      "Epoch [1/1], Step [8920/25000], Loss: 0.8054\n",
      "Epoch [1/1], Step [8930/25000], Loss: 0.1321\n",
      "Epoch [1/1], Step [8940/25000], Loss: 0.5481\n",
      "Epoch [1/1], Step [8950/25000], Loss: 0.2363\n",
      "Epoch [1/1], Step [8960/25000], Loss: 0.8011\n",
      "Epoch [1/1], Step [8970/25000], Loss: 0.2918\n",
      "Epoch [1/1], Step [8980/25000], Loss: 0.1621\n",
      "Epoch [1/1], Step [8990/25000], Loss: 0.3589\n",
      "Epoch [1/1], Step [9000/25000], Loss: 0.4351\n",
      "Epoch [1/1], Step [9010/25000], Loss: 0.3218\n",
      "Epoch [1/1], Step [9020/25000], Loss: 0.5521\n",
      "Epoch [1/1], Step [9030/25000], Loss: 0.6781\n",
      "Epoch [1/1], Step [9040/25000], Loss: 0.7579\n",
      "Epoch [1/1], Step [9050/25000], Loss: 0.8352\n",
      "Epoch [1/1], Step [9060/25000], Loss: 0.8927\n",
      "Epoch [1/1], Step [9070/25000], Loss: 0.4833\n",
      "Epoch [1/1], Step [9080/25000], Loss: 0.1625\n",
      "Epoch [1/1], Step [9090/25000], Loss: 0.6433\n",
      "Epoch [1/1], Step [9100/25000], Loss: 0.3511\n",
      "Epoch [1/1], Step [9110/25000], Loss: 0.1789\n",
      "Epoch [1/1], Step [9120/25000], Loss: 0.5220\n",
      "Epoch [1/1], Step [9130/25000], Loss: 0.3767\n",
      "Epoch [1/1], Step [9140/25000], Loss: 0.3180\n",
      "Epoch [1/1], Step [9150/25000], Loss: 0.4431\n",
      "Epoch [1/1], Step [9160/25000], Loss: 0.3824\n",
      "Epoch [1/1], Step [9170/25000], Loss: 0.1116\n",
      "Epoch [1/1], Step [9180/25000], Loss: 0.4302\n",
      "Epoch [1/1], Step [9190/25000], Loss: 0.6941\n",
      "Epoch [1/1], Step [9200/25000], Loss: 0.4570\n",
      "Epoch [1/1], Step [9210/25000], Loss: 1.0250\n",
      "Epoch [1/1], Step [9220/25000], Loss: 0.3257\n",
      "Epoch [1/1], Step [9230/25000], Loss: 0.4502\n",
      "Epoch [1/1], Step [9240/25000], Loss: 0.1123\n",
      "Epoch [1/1], Step [9250/25000], Loss: 0.4495\n",
      "Epoch [1/1], Step [9260/25000], Loss: 0.1835\n",
      "Epoch [1/1], Step [9270/25000], Loss: 0.7182\n",
      "Epoch [1/1], Step [9280/25000], Loss: 0.9199\n",
      "Epoch [1/1], Step [9290/25000], Loss: 0.1625\n",
      "Epoch [1/1], Step [9300/25000], Loss: 0.1473\n",
      "Epoch [1/1], Step [9310/25000], Loss: 0.7129\n",
      "Epoch [1/1], Step [9320/25000], Loss: 0.2306\n",
      "Epoch [1/1], Step [9330/25000], Loss: 0.8409\n",
      "Epoch [1/1], Step [9340/25000], Loss: 0.2156\n",
      "Epoch [1/1], Step [9350/25000], Loss: 0.5215\n",
      "Epoch [1/1], Step [9360/25000], Loss: 0.3282\n",
      "Epoch [1/1], Step [9370/25000], Loss: 0.7066\n",
      "Epoch [1/1], Step [9380/25000], Loss: 0.1888\n",
      "Epoch [1/1], Step [9390/25000], Loss: 0.5645\n",
      "Epoch [1/1], Step [9400/25000], Loss: 0.4048\n",
      "Epoch [1/1], Step [9410/25000], Loss: 0.8943\n",
      "Epoch [1/1], Step [9420/25000], Loss: 0.2779\n",
      "Epoch [1/1], Step [9430/25000], Loss: 0.7294\n",
      "Epoch [1/1], Step [9440/25000], Loss: 0.4305\n",
      "Epoch [1/1], Step [9450/25000], Loss: 0.1312\n",
      "Epoch [1/1], Step [9460/25000], Loss: 0.3774\n",
      "Epoch [1/1], Step [9470/25000], Loss: 0.0744\n",
      "Epoch [1/1], Step [9480/25000], Loss: 0.8511\n",
      "Epoch [1/1], Step [9490/25000], Loss: 0.5376\n",
      "Epoch [1/1], Step [9500/25000], Loss: 0.1957\n",
      "Epoch [1/1], Step [9510/25000], Loss: 0.7227\n",
      "Epoch [1/1], Step [9520/25000], Loss: 0.2653\n",
      "Epoch [1/1], Step [9530/25000], Loss: 0.8958\n",
      "Epoch [1/1], Step [9540/25000], Loss: 0.0808\n",
      "Epoch [1/1], Step [9550/25000], Loss: 0.2403\n",
      "Epoch [1/1], Step [9560/25000], Loss: 0.9100\n",
      "Epoch [1/1], Step [9570/25000], Loss: 0.4333\n",
      "Epoch [1/1], Step [9580/25000], Loss: 0.3330\n",
      "Epoch [1/1], Step [9590/25000], Loss: 0.4333\n",
      "Epoch [1/1], Step [9600/25000], Loss: 0.3261\n",
      "Epoch [1/1], Step [9610/25000], Loss: 0.1448\n",
      "Epoch [1/1], Step [9620/25000], Loss: 0.5096\n",
      "Epoch [1/1], Step [9630/25000], Loss: 0.1598\n",
      "Epoch [1/1], Step [9640/25000], Loss: 0.8470\n",
      "Epoch [1/1], Step [9650/25000], Loss: 0.7245\n",
      "Epoch [1/1], Step [9660/25000], Loss: 0.2845\n",
      "Epoch [1/1], Step [9670/25000], Loss: 1.0536\n",
      "Epoch [1/1], Step [9680/25000], Loss: 0.3513\n",
      "Epoch [1/1], Step [9690/25000], Loss: 0.3234\n",
      "Epoch [1/1], Step [9700/25000], Loss: 0.2310\n",
      "Epoch [1/1], Step [9710/25000], Loss: 0.9630\n",
      "Epoch [1/1], Step [9720/25000], Loss: 0.5404\n",
      "Epoch [1/1], Step [9730/25000], Loss: 0.5235\n",
      "Epoch [1/1], Step [9740/25000], Loss: 0.6420\n",
      "Epoch [1/1], Step [9750/25000], Loss: 0.1872\n",
      "Epoch [1/1], Step [9760/25000], Loss: 0.1877\n",
      "Epoch [1/1], Step [9770/25000], Loss: 0.2729\n",
      "Epoch [1/1], Step [9780/25000], Loss: 0.1845\n",
      "Epoch [1/1], Step [9790/25000], Loss: 0.4764\n",
      "Epoch [1/1], Step [9800/25000], Loss: 0.0744\n",
      "Epoch [1/1], Step [9810/25000], Loss: 0.3559\n",
      "Epoch [1/1], Step [9820/25000], Loss: 0.3710\n",
      "Epoch [1/1], Step [9830/25000], Loss: 0.2489\n",
      "Epoch [1/1], Step [9840/25000], Loss: 0.1486\n",
      "Epoch [1/1], Step [9850/25000], Loss: 0.3572\n",
      "Epoch [1/1], Step [9860/25000], Loss: 0.3770\n",
      "Epoch [1/1], Step [9870/25000], Loss: 0.1307\n",
      "Epoch [1/1], Step [9880/25000], Loss: 0.4025\n",
      "Epoch [1/1], Step [9890/25000], Loss: 0.1408\n",
      "Epoch [1/1], Step [9900/25000], Loss: 0.3975\n",
      "Epoch [1/1], Step [9910/25000], Loss: 0.3625\n",
      "Epoch [1/1], Step [9920/25000], Loss: 0.1147\n",
      "Epoch [1/1], Step [9930/25000], Loss: 0.0691\n",
      "Epoch [1/1], Step [9940/25000], Loss: 0.3403\n",
      "Epoch [1/1], Step [9950/25000], Loss: 0.3570\n",
      "Epoch [1/1], Step [9960/25000], Loss: 0.2657\n",
      "Epoch [1/1], Step [9970/25000], Loss: 0.7817\n",
      "Epoch [1/1], Step [9980/25000], Loss: 0.6146\n",
      "Epoch [1/1], Step [9990/25000], Loss: 0.6798\n",
      "Epoch [1/1], Step [10000/25000], Loss: 1.0675\n",
      "Epoch [1/1], Step [10010/25000], Loss: 0.3274\n",
      "Epoch [1/1], Step [10020/25000], Loss: 0.1668\n",
      "Epoch [1/1], Step [10030/25000], Loss: 0.1047\n",
      "Epoch [1/1], Step [10040/25000], Loss: 0.2722\n",
      "Epoch [1/1], Step [10050/25000], Loss: 0.1185\n",
      "Epoch [1/1], Step [10060/25000], Loss: 0.7425\n",
      "Epoch [1/1], Step [10070/25000], Loss: 0.3793\n",
      "Epoch [1/1], Step [10080/25000], Loss: 0.4980\n",
      "Epoch [1/1], Step [10090/25000], Loss: 0.3549\n",
      "Epoch [1/1], Step [10100/25000], Loss: 0.1460\n",
      "Epoch [1/1], Step [10110/25000], Loss: 1.2302\n",
      "Epoch [1/1], Step [10120/25000], Loss: 0.1657\n",
      "Epoch [1/1], Step [10130/25000], Loss: 0.3635\n",
      "Epoch [1/1], Step [10140/25000], Loss: 1.0892\n",
      "Epoch [1/1], Step [10150/25000], Loss: 0.6305\n",
      "Epoch [1/1], Step [10160/25000], Loss: 1.0075\n",
      "Epoch [1/1], Step [10170/25000], Loss: 0.1298\n",
      "Epoch [1/1], Step [10180/25000], Loss: 0.1337\n",
      "Epoch [1/1], Step [10190/25000], Loss: 0.7138\n",
      "Epoch [1/1], Step [10200/25000], Loss: 0.3869\n",
      "Epoch [1/1], Step [10210/25000], Loss: 0.1459\n",
      "Epoch [1/1], Step [10220/25000], Loss: 0.3090\n",
      "Epoch [1/1], Step [10230/25000], Loss: 0.1181\n",
      "Epoch [1/1], Step [10240/25000], Loss: 0.1320\n",
      "Epoch [1/1], Step [10250/25000], Loss: 0.2534\n",
      "Epoch [1/1], Step [10260/25000], Loss: 0.3617\n",
      "Epoch [1/1], Step [10270/25000], Loss: 1.0341\n",
      "Epoch [1/1], Step [10280/25000], Loss: 0.1457\n",
      "Epoch [1/1], Step [10290/25000], Loss: 0.2911\n",
      "Epoch [1/1], Step [10300/25000], Loss: 0.1282\n",
      "Epoch [1/1], Step [10310/25000], Loss: 0.6596\n",
      "Epoch [1/1], Step [10320/25000], Loss: 0.4026\n",
      "Epoch [1/1], Step [10330/25000], Loss: 0.2831\n",
      "Epoch [1/1], Step [10340/25000], Loss: 0.4531\n",
      "Epoch [1/1], Step [10350/25000], Loss: 0.5114\n",
      "Epoch [1/1], Step [10360/25000], Loss: 0.1095\n",
      "Epoch [1/1], Step [10370/25000], Loss: 0.3812\n",
      "Epoch [1/1], Step [10380/25000], Loss: 1.0015\n",
      "Epoch [1/1], Step [10390/25000], Loss: 0.6377\n",
      "Epoch [1/1], Step [10400/25000], Loss: 0.5988\n",
      "Epoch [1/1], Step [10410/25000], Loss: 0.9702\n",
      "Epoch [1/1], Step [10420/25000], Loss: 0.3114\n",
      "Epoch [1/1], Step [10430/25000], Loss: 0.4392\n",
      "Epoch [1/1], Step [10440/25000], Loss: 1.4524\n",
      "Epoch [1/1], Step [10450/25000], Loss: 0.4590\n",
      "Epoch [1/1], Step [10460/25000], Loss: 0.3389\n",
      "Epoch [1/1], Step [10470/25000], Loss: 0.2278\n",
      "Epoch [1/1], Step [10480/25000], Loss: 0.7471\n",
      "Epoch [1/1], Step [10490/25000], Loss: 0.1203\n",
      "Epoch [1/1], Step [10500/25000], Loss: 0.2668\n",
      "Epoch [1/1], Step [10510/25000], Loss: 0.1054\n",
      "Epoch [1/1], Step [10520/25000], Loss: 0.2483\n",
      "Epoch [1/1], Step [10530/25000], Loss: 0.4382\n",
      "Epoch [1/1], Step [10540/25000], Loss: 0.0800\n",
      "Epoch [1/1], Step [10550/25000], Loss: 0.4143\n",
      "Epoch [1/1], Step [10560/25000], Loss: 0.6860\n",
      "Epoch [1/1], Step [10570/25000], Loss: 0.2323\n",
      "Epoch [1/1], Step [10580/25000], Loss: 0.7759\n",
      "Epoch [1/1], Step [10590/25000], Loss: 0.4116\n",
      "Epoch [1/1], Step [10600/25000], Loss: 0.8900\n",
      "Epoch [1/1], Step [10610/25000], Loss: 0.3894\n",
      "Epoch [1/1], Step [10620/25000], Loss: 0.2781\n",
      "Epoch [1/1], Step [10630/25000], Loss: 0.4313\n",
      "Epoch [1/1], Step [10640/25000], Loss: 0.4420\n",
      "Epoch [1/1], Step [10650/25000], Loss: 0.2113\n",
      "Epoch [1/1], Step [10660/25000], Loss: 0.1325\n",
      "Epoch [1/1], Step [10670/25000], Loss: 0.6779\n",
      "Epoch [1/1], Step [10680/25000], Loss: 0.0991\n",
      "Epoch [1/1], Step [10690/25000], Loss: 0.2766\n",
      "Epoch [1/1], Step [10700/25000], Loss: 0.3229\n",
      "Epoch [1/1], Step [10710/25000], Loss: 0.6825\n",
      "Epoch [1/1], Step [10720/25000], Loss: 0.4859\n",
      "Epoch [1/1], Step [10730/25000], Loss: 0.0694\n",
      "Epoch [1/1], Step [10740/25000], Loss: 0.2168\n",
      "Epoch [1/1], Step [10750/25000], Loss: 0.5346\n",
      "Epoch [1/1], Step [10760/25000], Loss: 0.5894\n",
      "Epoch [1/1], Step [10770/25000], Loss: 0.4666\n",
      "Epoch [1/1], Step [10780/25000], Loss: 0.0858\n",
      "Epoch [1/1], Step [10790/25000], Loss: 0.1794\n",
      "Epoch [1/1], Step [10800/25000], Loss: 0.5533\n",
      "Epoch [1/1], Step [10810/25000], Loss: 0.5352\n",
      "Epoch [1/1], Step [10820/25000], Loss: 1.0570\n",
      "Epoch [1/1], Step [10830/25000], Loss: 0.2476\n",
      "Epoch [1/1], Step [10840/25000], Loss: 0.8234\n",
      "Epoch [1/1], Step [10850/25000], Loss: 0.0941\n",
      "Epoch [1/1], Step [10860/25000], Loss: 0.4328\n",
      "Epoch [1/1], Step [10870/25000], Loss: 0.4257\n",
      "Epoch [1/1], Step [10880/25000], Loss: 0.0667\n",
      "Epoch [1/1], Step [10890/25000], Loss: 0.3681\n",
      "Epoch [1/1], Step [10900/25000], Loss: 0.2184\n",
      "Epoch [1/1], Step [10910/25000], Loss: 0.1818\n",
      "Epoch [1/1], Step [10920/25000], Loss: 0.0745\n",
      "Epoch [1/1], Step [10930/25000], Loss: 0.1342\n",
      "Epoch [1/1], Step [10940/25000], Loss: 0.1134\n",
      "Epoch [1/1], Step [10950/25000], Loss: 0.3657\n",
      "Epoch [1/1], Step [10960/25000], Loss: 0.3015\n",
      "Epoch [1/1], Step [10970/25000], Loss: 0.4704\n",
      "Epoch [1/1], Step [10980/25000], Loss: 0.1540\n",
      "Epoch [1/1], Step [10990/25000], Loss: 0.4442\n",
      "Epoch [1/1], Step [11000/25000], Loss: 1.4718\n",
      "Epoch [1/1], Step [11010/25000], Loss: 0.0799\n",
      "Epoch [1/1], Step [11020/25000], Loss: 0.0636\n",
      "Epoch [1/1], Step [11030/25000], Loss: 0.4813\n",
      "Epoch [1/1], Step [11040/25000], Loss: 0.1601\n",
      "Epoch [1/1], Step [11050/25000], Loss: 0.1708\n",
      "Epoch [1/1], Step [11060/25000], Loss: 0.2331\n",
      "Epoch [1/1], Step [11070/25000], Loss: 1.0480\n",
      "Epoch [1/1], Step [11080/25000], Loss: 0.6890\n",
      "Epoch [1/1], Step [11090/25000], Loss: 0.4971\n",
      "Epoch [1/1], Step [11100/25000], Loss: 0.3555\n",
      "Epoch [1/1], Step [11110/25000], Loss: 0.9747\n",
      "Epoch [1/1], Step [11120/25000], Loss: 0.2591\n",
      "Epoch [1/1], Step [11130/25000], Loss: 0.8426\n",
      "Epoch [1/1], Step [11140/25000], Loss: 0.9690\n",
      "Epoch [1/1], Step [11150/25000], Loss: 0.5303\n",
      "Epoch [1/1], Step [11160/25000], Loss: 0.5088\n",
      "Epoch [1/1], Step [11170/25000], Loss: 0.0822\n",
      "Epoch [1/1], Step [11180/25000], Loss: 0.1920\n",
      "Epoch [1/1], Step [11190/25000], Loss: 0.9772\n",
      "Epoch [1/1], Step [11200/25000], Loss: 0.5864\n",
      "Epoch [1/1], Step [11210/25000], Loss: 0.4434\n",
      "Epoch [1/1], Step [11220/25000], Loss: 0.3297\n",
      "Epoch [1/1], Step [11230/25000], Loss: 0.1562\n",
      "Epoch [1/1], Step [11240/25000], Loss: 0.1574\n",
      "Epoch [1/1], Step [11250/25000], Loss: 0.3029\n",
      "Epoch [1/1], Step [11260/25000], Loss: 0.0690\n",
      "Epoch [1/1], Step [11270/25000], Loss: 0.3674\n",
      "Epoch [1/1], Step [11280/25000], Loss: 0.5287\n",
      "Epoch [1/1], Step [11290/25000], Loss: 0.4059\n",
      "Epoch [1/1], Step [11300/25000], Loss: 0.1475\n",
      "Epoch [1/1], Step [11310/25000], Loss: 0.2068\n",
      "Epoch [1/1], Step [11320/25000], Loss: 0.1703\n",
      "Epoch [1/1], Step [11330/25000], Loss: 0.4392\n",
      "Epoch [1/1], Step [11340/25000], Loss: 0.1977\n",
      "Epoch [1/1], Step [11350/25000], Loss: 0.1423\n",
      "Epoch [1/1], Step [11360/25000], Loss: 0.1711\n",
      "Epoch [1/1], Step [11370/25000], Loss: 0.1043\n",
      "Epoch [1/1], Step [11380/25000], Loss: 0.1717\n",
      "Epoch [1/1], Step [11390/25000], Loss: 0.3709\n",
      "Epoch [1/1], Step [11400/25000], Loss: 0.3838\n",
      "Epoch [1/1], Step [11410/25000], Loss: 0.3564\n",
      "Epoch [1/1], Step [11420/25000], Loss: 0.7510\n",
      "Epoch [1/1], Step [11430/25000], Loss: 0.1716\n",
      "Epoch [1/1], Step [11440/25000], Loss: 0.6256\n",
      "Epoch [1/1], Step [11450/25000], Loss: 0.6596\n",
      "Epoch [1/1], Step [11460/25000], Loss: 0.4290\n",
      "Epoch [1/1], Step [11470/25000], Loss: 0.8160\n",
      "Epoch [1/1], Step [11480/25000], Loss: 0.5278\n",
      "Epoch [1/1], Step [11490/25000], Loss: 0.4250\n",
      "Epoch [1/1], Step [11500/25000], Loss: 0.6671\n",
      "Epoch [1/1], Step [11510/25000], Loss: 0.5074\n",
      "Epoch [1/1], Step [11520/25000], Loss: 1.0738\n",
      "Epoch [1/1], Step [11530/25000], Loss: 0.1870\n",
      "Epoch [1/1], Step [11540/25000], Loss: 0.1658\n",
      "Epoch [1/1], Step [11550/25000], Loss: 0.1716\n",
      "Epoch [1/1], Step [11560/25000], Loss: 0.4542\n",
      "Epoch [1/1], Step [11570/25000], Loss: 0.6567\n",
      "Epoch [1/1], Step [11580/25000], Loss: 0.1383\n",
      "Epoch [1/1], Step [11590/25000], Loss: 0.6411\n",
      "Epoch [1/1], Step [11600/25000], Loss: 0.2012\n",
      "Epoch [1/1], Step [11610/25000], Loss: 0.8820\n",
      "Epoch [1/1], Step [11620/25000], Loss: 0.1242\n",
      "Epoch [1/1], Step [11630/25000], Loss: 0.1075\n",
      "Epoch [1/1], Step [11640/25000], Loss: 0.2055\n",
      "Epoch [1/1], Step [11650/25000], Loss: 0.3432\n",
      "Epoch [1/1], Step [11660/25000], Loss: 0.5345\n",
      "Epoch [1/1], Step [11670/25000], Loss: 0.0696\n",
      "Epoch [1/1], Step [11680/25000], Loss: 0.3274\n",
      "Epoch [1/1], Step [11690/25000], Loss: 0.8733\n",
      "Epoch [1/1], Step [11700/25000], Loss: 0.9507\n",
      "Epoch [1/1], Step [11710/25000], Loss: 0.0801\n",
      "Epoch [1/1], Step [11720/25000], Loss: 0.2402\n",
      "Epoch [1/1], Step [11730/25000], Loss: 0.1923\n",
      "Epoch [1/1], Step [11740/25000], Loss: 0.4334\n",
      "Epoch [1/1], Step [11750/25000], Loss: 0.2660\n",
      "Epoch [1/1], Step [11760/25000], Loss: 0.9622\n",
      "Epoch [1/1], Step [11770/25000], Loss: 0.0594\n",
      "Epoch [1/1], Step [11780/25000], Loss: 0.2835\n",
      "Epoch [1/1], Step [11790/25000], Loss: 0.3711\n",
      "Epoch [1/1], Step [11800/25000], Loss: 0.9117\n",
      "Epoch [1/1], Step [11810/25000], Loss: 0.1196\n",
      "Epoch [1/1], Step [11820/25000], Loss: 0.2442\n",
      "Epoch [1/1], Step [11830/25000], Loss: 0.0856\n",
      "Epoch [1/1], Step [11840/25000], Loss: 0.6880\n",
      "Epoch [1/1], Step [11850/25000], Loss: 0.9217\n",
      "Epoch [1/1], Step [11860/25000], Loss: 0.0903\n",
      "Epoch [1/1], Step [11870/25000], Loss: 0.4078\n",
      "Epoch [1/1], Step [11880/25000], Loss: 1.5866\n",
      "Epoch [1/1], Step [11890/25000], Loss: 0.0934\n",
      "Epoch [1/1], Step [11900/25000], Loss: 0.1033\n",
      "Epoch [1/1], Step [11910/25000], Loss: 0.0975\n",
      "Epoch [1/1], Step [11920/25000], Loss: 0.0914\n",
      "Epoch [1/1], Step [11930/25000], Loss: 0.9550\n",
      "Epoch [1/1], Step [11940/25000], Loss: 0.1503\n",
      "Epoch [1/1], Step [11950/25000], Loss: 0.1232\n",
      "Epoch [1/1], Step [11960/25000], Loss: 0.0957\n",
      "Epoch [1/1], Step [11970/25000], Loss: 0.1652\n",
      "Epoch [1/1], Step [11980/25000], Loss: 0.4026\n",
      "Epoch [1/1], Step [11990/25000], Loss: 0.1929\n",
      "Epoch [1/1], Step [12000/25000], Loss: 0.1408\n",
      "Epoch [1/1], Step [12010/25000], Loss: 0.4620\n",
      "Epoch [1/1], Step [12020/25000], Loss: 0.6024\n",
      "Epoch [1/1], Step [12030/25000], Loss: 0.6504\n",
      "Epoch [1/1], Step [12040/25000], Loss: 0.3998\n",
      "Epoch [1/1], Step [12050/25000], Loss: 0.0708\n",
      "Epoch [1/1], Step [12060/25000], Loss: 0.1199\n",
      "Epoch [1/1], Step [12070/25000], Loss: 0.7802\n",
      "Epoch [1/1], Step [12080/25000], Loss: 0.2738\n",
      "Epoch [1/1], Step [12090/25000], Loss: 0.2629\n",
      "Epoch [1/1], Step [12100/25000], Loss: 0.6144\n",
      "Epoch [1/1], Step [12110/25000], Loss: 0.1151\n",
      "Epoch [1/1], Step [12120/25000], Loss: 0.4312\n",
      "Epoch [1/1], Step [12130/25000], Loss: 0.1373\n",
      "Epoch [1/1], Step [12140/25000], Loss: 0.3628\n",
      "Epoch [1/1], Step [12150/25000], Loss: 0.2963\n",
      "Epoch [1/1], Step [12160/25000], Loss: 0.1314\n",
      "Epoch [1/1], Step [12170/25000], Loss: 0.7589\n",
      "Epoch [1/1], Step [12180/25000], Loss: 0.0622\n",
      "Epoch [1/1], Step [12190/25000], Loss: 0.1695\n",
      "Epoch [1/1], Step [12200/25000], Loss: 0.3025\n",
      "Epoch [1/1], Step [12210/25000], Loss: 0.1887\n",
      "Epoch [1/1], Step [12220/25000], Loss: 0.0841\n",
      "Epoch [1/1], Step [12230/25000], Loss: 0.7428\n",
      "Epoch [1/1], Step [12240/25000], Loss: 0.3726\n",
      "Epoch [1/1], Step [12250/25000], Loss: 0.0987\n",
      "Epoch [1/1], Step [12260/25000], Loss: 0.1239\n",
      "Epoch [1/1], Step [12270/25000], Loss: 0.9504\n",
      "Epoch [1/1], Step [12280/25000], Loss: 0.1380\n",
      "Epoch [1/1], Step [12290/25000], Loss: 0.1161\n",
      "Epoch [1/1], Step [12300/25000], Loss: 0.0767\n",
      "Epoch [1/1], Step [12310/25000], Loss: 0.6461\n",
      "Epoch [1/1], Step [12320/25000], Loss: 0.1227\n",
      "Epoch [1/1], Step [12330/25000], Loss: 1.0248\n",
      "Epoch [1/1], Step [12340/25000], Loss: 0.0661\n",
      "Epoch [1/1], Step [12350/25000], Loss: 0.3324\n",
      "Epoch [1/1], Step [12360/25000], Loss: 0.2130\n",
      "Epoch [1/1], Step [12370/25000], Loss: 0.2750\n",
      "Epoch [1/1], Step [12380/25000], Loss: 0.1587\n",
      "Epoch [1/1], Step [12390/25000], Loss: 0.1592\n",
      "Epoch [1/1], Step [12400/25000], Loss: 0.1697\n",
      "Epoch [1/1], Step [12410/25000], Loss: 0.4531\n",
      "Epoch [1/1], Step [12420/25000], Loss: 0.1941\n",
      "Epoch [1/1], Step [12430/25000], Loss: 0.7769\n",
      "Epoch [1/1], Step [12440/25000], Loss: 0.8524\n",
      "Epoch [1/1], Step [12450/25000], Loss: 0.0403\n",
      "Epoch [1/1], Step [12460/25000], Loss: 0.3297\n",
      "Epoch [1/1], Step [12470/25000], Loss: 0.2391\n",
      "Epoch [1/1], Step [12480/25000], Loss: 0.3018\n",
      "Epoch [1/1], Step [12490/25000], Loss: 0.2674\n",
      "Epoch [1/1], Step [12500/25000], Loss: 0.1263\n",
      "Epoch [1/1], Step [12510/25000], Loss: 0.0543\n",
      "Epoch [1/1], Step [12520/25000], Loss: 0.1846\n",
      "Epoch [1/1], Step [12530/25000], Loss: 0.0861\n",
      "Epoch [1/1], Step [12540/25000], Loss: 0.0297\n",
      "Epoch [1/1], Step [12550/25000], Loss: 0.3048\n",
      "Epoch [1/1], Step [12560/25000], Loss: 0.6320\n",
      "Epoch [1/1], Step [12570/25000], Loss: 0.5127\n",
      "Epoch [1/1], Step [12580/25000], Loss: 0.6356\n",
      "Epoch [1/1], Step [12590/25000], Loss: 0.3267\n",
      "Epoch [1/1], Step [12600/25000], Loss: 0.7767\n",
      "Epoch [1/1], Step [12610/25000], Loss: 0.2816\n",
      "Epoch [1/1], Step [12620/25000], Loss: 0.0942\n",
      "Epoch [1/1], Step [12630/25000], Loss: 0.2793\n",
      "Epoch [1/1], Step [12640/25000], Loss: 0.1029\n",
      "Epoch [1/1], Step [12650/25000], Loss: 0.9762\n",
      "Epoch [1/1], Step [12660/25000], Loss: 0.9279\n",
      "Epoch [1/1], Step [12670/25000], Loss: 0.4585\n",
      "Epoch [1/1], Step [12680/25000], Loss: 0.4608\n",
      "Epoch [1/1], Step [12690/25000], Loss: 0.5890\n",
      "Epoch [1/1], Step [12700/25000], Loss: 0.7163\n",
      "Epoch [1/1], Step [12710/25000], Loss: 0.1626\n",
      "Epoch [1/1], Step [12720/25000], Loss: 0.6277\n",
      "Epoch [1/1], Step [12730/25000], Loss: 0.3009\n",
      "Epoch [1/1], Step [12740/25000], Loss: 0.3076\n",
      "Epoch [1/1], Step [12750/25000], Loss: 0.3369\n",
      "Epoch [1/1], Step [12760/25000], Loss: 0.6794\n",
      "Epoch [1/1], Step [12770/25000], Loss: 0.4327\n",
      "Epoch [1/1], Step [12780/25000], Loss: 0.9531\n",
      "Epoch [1/1], Step [12790/25000], Loss: 0.7092\n",
      "Epoch [1/1], Step [12800/25000], Loss: 0.3227\n",
      "Epoch [1/1], Step [12810/25000], Loss: 0.2438\n",
      "Epoch [1/1], Step [12820/25000], Loss: 0.0222\n",
      "Epoch [1/1], Step [12830/25000], Loss: 0.1606\n",
      "Epoch [1/1], Step [12840/25000], Loss: 0.5467\n",
      "Epoch [1/1], Step [12850/25000], Loss: 0.5826\n",
      "Epoch [1/1], Step [12860/25000], Loss: 0.2145\n",
      "Epoch [1/1], Step [12870/25000], Loss: 0.3547\n",
      "Epoch [1/1], Step [12880/25000], Loss: 0.1700\n",
      "Epoch [1/1], Step [12890/25000], Loss: 0.3966\n",
      "Epoch [1/1], Step [12900/25000], Loss: 0.0997\n",
      "Epoch [1/1], Step [12910/25000], Loss: 0.2332\n",
      "Epoch [1/1], Step [12920/25000], Loss: 0.5261\n",
      "Epoch [1/1], Step [12930/25000], Loss: 0.3860\n",
      "Epoch [1/1], Step [12940/25000], Loss: 0.4816\n",
      "Epoch [1/1], Step [12950/25000], Loss: 0.2457\n",
      "Epoch [1/1], Step [12960/25000], Loss: 0.8443\n",
      "Epoch [1/1], Step [12970/25000], Loss: 0.1088\n",
      "Epoch [1/1], Step [12980/25000], Loss: 0.0917\n",
      "Epoch [1/1], Step [12990/25000], Loss: 0.1154\n",
      "Epoch [1/1], Step [13000/25000], Loss: 0.7015\n",
      "Epoch [1/1], Step [13010/25000], Loss: 0.2543\n",
      "Epoch [1/1], Step [13020/25000], Loss: 0.4374\n",
      "Epoch [1/1], Step [13030/25000], Loss: 0.1892\n",
      "Epoch [1/1], Step [13040/25000], Loss: 0.3368\n",
      "Epoch [1/1], Step [13050/25000], Loss: 0.2137\n",
      "Epoch [1/1], Step [13060/25000], Loss: 0.2433\n",
      "Epoch [1/1], Step [13070/25000], Loss: 0.1767\n",
      "Epoch [1/1], Step [13080/25000], Loss: 0.1299\n",
      "Epoch [1/1], Step [13090/25000], Loss: 1.3072\n",
      "Epoch [1/1], Step [13100/25000], Loss: 0.2532\n",
      "Epoch [1/1], Step [13110/25000], Loss: 0.6345\n",
      "Epoch [1/1], Step [13120/25000], Loss: 0.4029\n",
      "Epoch [1/1], Step [13130/25000], Loss: 0.5842\n",
      "Epoch [1/1], Step [13140/25000], Loss: 0.1291\n",
      "Epoch [1/1], Step [13150/25000], Loss: 0.1099\n",
      "Epoch [1/1], Step [13160/25000], Loss: 0.2509\n",
      "Epoch [1/1], Step [13170/25000], Loss: 1.0598\n",
      "Epoch [1/1], Step [13180/25000], Loss: 0.2423\n",
      "Epoch [1/1], Step [13190/25000], Loss: 0.1111\n",
      "Epoch [1/1], Step [13200/25000], Loss: 0.0494\n",
      "Epoch [1/1], Step [13210/25000], Loss: 0.2727\n",
      "Epoch [1/1], Step [13220/25000], Loss: 0.0380\n",
      "Epoch [1/1], Step [13230/25000], Loss: 0.5987\n",
      "Epoch [1/1], Step [13240/25000], Loss: 0.7151\n",
      "Epoch [1/1], Step [13250/25000], Loss: 0.6015\n",
      "Epoch [1/1], Step [13260/25000], Loss: 0.4580\n",
      "Epoch [1/1], Step [13270/25000], Loss: 0.4701\n",
      "Epoch [1/1], Step [13280/25000], Loss: 0.1690\n",
      "Epoch [1/1], Step [13290/25000], Loss: 0.3260\n",
      "Epoch [1/1], Step [13300/25000], Loss: 0.2280\n",
      "Epoch [1/1], Step [13310/25000], Loss: 0.3726\n",
      "Epoch [1/1], Step [13320/25000], Loss: 0.1751\n",
      "Epoch [1/1], Step [13330/25000], Loss: 0.6696\n",
      "Epoch [1/1], Step [13340/25000], Loss: 0.5093\n",
      "Epoch [1/1], Step [13350/25000], Loss: 1.5453\n",
      "Epoch [1/1], Step [13360/25000], Loss: 0.0831\n",
      "Epoch [1/1], Step [13370/25000], Loss: 0.3363\n",
      "Epoch [1/1], Step [13380/25000], Loss: 0.1852\n",
      "Epoch [1/1], Step [13390/25000], Loss: 0.4162\n",
      "Epoch [1/1], Step [13400/25000], Loss: 0.2042\n",
      "Epoch [1/1], Step [13410/25000], Loss: 0.1303\n",
      "Epoch [1/1], Step [13420/25000], Loss: 0.2040\n",
      "Epoch [1/1], Step [13430/25000], Loss: 0.1529\n",
      "Epoch [1/1], Step [13440/25000], Loss: 0.3706\n",
      "Epoch [1/1], Step [13450/25000], Loss: 0.0445\n",
      "Epoch [1/1], Step [13460/25000], Loss: 0.4237\n",
      "Epoch [1/1], Step [13470/25000], Loss: 0.1254\n",
      "Epoch [1/1], Step [13480/25000], Loss: 0.0930\n",
      "Epoch [1/1], Step [13490/25000], Loss: 0.3078\n",
      "Epoch [1/1], Step [13500/25000], Loss: 0.2863\n",
      "Epoch [1/1], Step [13510/25000], Loss: 0.3244\n",
      "Epoch [1/1], Step [13520/25000], Loss: 0.4581\n",
      "Epoch [1/1], Step [13530/25000], Loss: 0.1871\n",
      "Epoch [1/1], Step [13540/25000], Loss: 0.2439\n",
      "Epoch [1/1], Step [13550/25000], Loss: 0.3431\n",
      "Epoch [1/1], Step [13560/25000], Loss: 0.2997\n",
      "Epoch [1/1], Step [13570/25000], Loss: 0.1326\n",
      "Epoch [1/1], Step [13580/25000], Loss: 0.3300\n",
      "Epoch [1/1], Step [13590/25000], Loss: 0.0907\n",
      "Epoch [1/1], Step [13600/25000], Loss: 0.1162\n",
      "Epoch [1/1], Step [13610/25000], Loss: 0.2308\n",
      "Epoch [1/1], Step [13620/25000], Loss: 0.0985\n",
      "Epoch [1/1], Step [13630/25000], Loss: 0.2379\n",
      "Epoch [1/1], Step [13640/25000], Loss: 0.2742\n",
      "Epoch [1/1], Step [13650/25000], Loss: 0.1207\n",
      "Epoch [1/1], Step [13660/25000], Loss: 0.0577\n",
      "Epoch [1/1], Step [13670/25000], Loss: 0.1314\n",
      "Epoch [1/1], Step [13680/25000], Loss: 0.4002\n",
      "Epoch [1/1], Step [13690/25000], Loss: 1.1928\n",
      "Epoch [1/1], Step [13700/25000], Loss: 0.4565\n",
      "Epoch [1/1], Step [13710/25000], Loss: 0.1515\n",
      "Epoch [1/1], Step [13720/25000], Loss: 0.0895\n",
      "Epoch [1/1], Step [13730/25000], Loss: 0.5563\n",
      "Epoch [1/1], Step [13740/25000], Loss: 0.1057\n",
      "Epoch [1/1], Step [13750/25000], Loss: 0.7794\n",
      "Epoch [1/1], Step [13760/25000], Loss: 0.2524\n",
      "Epoch [1/1], Step [13770/25000], Loss: 0.1185\n",
      "Epoch [1/1], Step [13780/25000], Loss: 0.1492\n",
      "Epoch [1/1], Step [13790/25000], Loss: 0.4233\n",
      "Epoch [1/1], Step [13800/25000], Loss: 0.1159\n",
      "Epoch [1/1], Step [13810/25000], Loss: 0.2639\n",
      "Epoch [1/1], Step [13820/25000], Loss: 0.3065\n",
      "Epoch [1/1], Step [13830/25000], Loss: 0.9606\n",
      "Epoch [1/1], Step [13840/25000], Loss: 0.1318\n",
      "Epoch [1/1], Step [13850/25000], Loss: 0.0623\n",
      "Epoch [1/1], Step [13860/25000], Loss: 0.5669\n",
      "Epoch [1/1], Step [13870/25000], Loss: 0.1353\n",
      "Epoch [1/1], Step [13880/25000], Loss: 0.0805\n",
      "Epoch [1/1], Step [13890/25000], Loss: 0.0926\n",
      "Epoch [1/1], Step [13900/25000], Loss: 0.4585\n",
      "Epoch [1/1], Step [13910/25000], Loss: 0.2213\n",
      "Epoch [1/1], Step [13920/25000], Loss: 0.5650\n",
      "Epoch [1/1], Step [13930/25000], Loss: 0.2684\n",
      "Epoch [1/1], Step [13940/25000], Loss: 0.1236\n",
      "Epoch [1/1], Step [13950/25000], Loss: 0.1685\n",
      "Epoch [1/1], Step [13960/25000], Loss: 0.1044\n",
      "Epoch [1/1], Step [13970/25000], Loss: 0.2810\n",
      "Epoch [1/1], Step [13980/25000], Loss: 0.3612\n",
      "Epoch [1/1], Step [13990/25000], Loss: 0.6002\n",
      "Epoch [1/1], Step [14000/25000], Loss: 1.0298\n",
      "Epoch [1/1], Step [14010/25000], Loss: 0.4023\n",
      "Epoch [1/1], Step [14020/25000], Loss: 0.2011\n",
      "Epoch [1/1], Step [14030/25000], Loss: 0.6556\n",
      "Epoch [1/1], Step [14040/25000], Loss: 0.1150\n",
      "Epoch [1/1], Step [14050/25000], Loss: 0.1930\n",
      "Epoch [1/1], Step [14060/25000], Loss: 1.2573\n",
      "Epoch [1/1], Step [14070/25000], Loss: 0.4494\n",
      "Epoch [1/1], Step [14080/25000], Loss: 0.0974\n",
      "Epoch [1/1], Step [14090/25000], Loss: 0.5905\n",
      "Epoch [1/1], Step [14100/25000], Loss: 0.2064\n",
      "Epoch [1/1], Step [14110/25000], Loss: 0.0971\n",
      "Epoch [1/1], Step [14120/25000], Loss: 0.2055\n",
      "Epoch [1/1], Step [14130/25000], Loss: 0.0589\n",
      "Epoch [1/1], Step [14140/25000], Loss: 0.4141\n",
      "Epoch [1/1], Step [14150/25000], Loss: 0.1386\n",
      "Epoch [1/1], Step [14160/25000], Loss: 0.5995\n",
      "Epoch [1/1], Step [14170/25000], Loss: 0.0276\n",
      "Epoch [1/1], Step [14180/25000], Loss: 0.7942\n",
      "Epoch [1/1], Step [14190/25000], Loss: 0.7735\n",
      "Epoch [1/1], Step [14200/25000], Loss: 0.0998\n",
      "Epoch [1/1], Step [14210/25000], Loss: 0.0253\n",
      "Epoch [1/1], Step [14220/25000], Loss: 0.7001\n",
      "Epoch [1/1], Step [14230/25000], Loss: 0.0491\n",
      "Epoch [1/1], Step [14240/25000], Loss: 0.1261\n",
      "Epoch [1/1], Step [14250/25000], Loss: 0.5279\n",
      "Epoch [1/1], Step [14260/25000], Loss: 1.6226\n",
      "Epoch [1/1], Step [14270/25000], Loss: 0.1754\n",
      "Epoch [1/1], Step [14280/25000], Loss: 0.3727\n",
      "Epoch [1/1], Step [14290/25000], Loss: 0.4886\n",
      "Epoch [1/1], Step [14300/25000], Loss: 0.6768\n",
      "Epoch [1/1], Step [14310/25000], Loss: 0.0743\n",
      "Epoch [1/1], Step [14320/25000], Loss: 0.3266\n",
      "Epoch [1/1], Step [14330/25000], Loss: 0.5182\n",
      "Epoch [1/1], Step [14340/25000], Loss: 0.5278\n",
      "Epoch [1/1], Step [14350/25000], Loss: 0.7427\n",
      "Epoch [1/1], Step [14360/25000], Loss: 0.0699\n",
      "Epoch [1/1], Step [14370/25000], Loss: 0.1734\n",
      "Epoch [1/1], Step [14380/25000], Loss: 0.2890\n",
      "Epoch [1/1], Step [14390/25000], Loss: 0.1109\n",
      "Epoch [1/1], Step [14400/25000], Loss: 0.1063\n",
      "Epoch [1/1], Step [14410/25000], Loss: 0.0839\n",
      "Epoch [1/1], Step [14420/25000], Loss: 0.1565\n",
      "Epoch [1/1], Step [14430/25000], Loss: 0.0523\n",
      "Epoch [1/1], Step [14440/25000], Loss: 0.8705\n",
      "Epoch [1/1], Step [14450/25000], Loss: 0.7001\n",
      "Epoch [1/1], Step [14460/25000], Loss: 0.0733\n",
      "Epoch [1/1], Step [14470/25000], Loss: 0.4969\n",
      "Epoch [1/1], Step [14480/25000], Loss: 0.0657\n",
      "Epoch [1/1], Step [14490/25000], Loss: 0.0927\n",
      "Epoch [1/1], Step [14500/25000], Loss: 0.3103\n",
      "Epoch [1/1], Step [14510/25000], Loss: 0.1651\n",
      "Epoch [1/1], Step [14520/25000], Loss: 0.2058\n",
      "Epoch [1/1], Step [14530/25000], Loss: 0.0794\n",
      "Epoch [1/1], Step [14540/25000], Loss: 0.1606\n",
      "Epoch [1/1], Step [14550/25000], Loss: 0.5568\n",
      "Epoch [1/1], Step [14560/25000], Loss: 0.2734\n",
      "Epoch [1/1], Step [14570/25000], Loss: 0.3863\n",
      "Epoch [1/1], Step [14580/25000], Loss: 0.2817\n",
      "Epoch [1/1], Step [14590/25000], Loss: 1.0643\n",
      "Epoch [1/1], Step [14600/25000], Loss: 0.3651\n",
      "Epoch [1/1], Step [14610/25000], Loss: 0.1311\n",
      "Epoch [1/1], Step [14620/25000], Loss: 0.2051\n",
      "Epoch [1/1], Step [14630/25000], Loss: 0.3639\n",
      "Epoch [1/1], Step [14640/25000], Loss: 0.1365\n",
      "Epoch [1/1], Step [14650/25000], Loss: 0.3519\n",
      "Epoch [1/1], Step [14660/25000], Loss: 0.2133\n",
      "Epoch [1/1], Step [14670/25000], Loss: 0.1489\n",
      "Epoch [1/1], Step [14680/25000], Loss: 0.1301\n",
      "Epoch [1/1], Step [14690/25000], Loss: 0.3236\n",
      "Epoch [1/1], Step [14700/25000], Loss: 0.5566\n",
      "Epoch [1/1], Step [14710/25000], Loss: 0.2607\n",
      "Epoch [1/1], Step [14720/25000], Loss: 0.7785\n",
      "Epoch [1/1], Step [14730/25000], Loss: 0.0834\n",
      "Epoch [1/1], Step [14740/25000], Loss: 0.1968\n",
      "Epoch [1/1], Step [14750/25000], Loss: 0.3035\n",
      "Epoch [1/1], Step [14760/25000], Loss: 0.7017\n",
      "Epoch [1/1], Step [14770/25000], Loss: 0.1512\n",
      "Epoch [1/1], Step [14780/25000], Loss: 0.0741\n",
      "Epoch [1/1], Step [14790/25000], Loss: 0.0838\n",
      "Epoch [1/1], Step [14800/25000], Loss: 0.4511\n",
      "Epoch [1/1], Step [14810/25000], Loss: 0.2267\n",
      "Epoch [1/1], Step [14820/25000], Loss: 0.4987\n",
      "Epoch [1/1], Step [14830/25000], Loss: 0.6556\n",
      "Epoch [1/1], Step [14840/25000], Loss: 0.1265\n",
      "Epoch [1/1], Step [14850/25000], Loss: 0.1264\n",
      "Epoch [1/1], Step [14860/25000], Loss: 0.6026\n",
      "Epoch [1/1], Step [14870/25000], Loss: 0.0671\n",
      "Epoch [1/1], Step [14880/25000], Loss: 0.1137\n",
      "Epoch [1/1], Step [14890/25000], Loss: 0.5215\n",
      "Epoch [1/1], Step [14900/25000], Loss: 0.0778\n",
      "Epoch [1/1], Step [14910/25000], Loss: 0.6139\n",
      "Epoch [1/1], Step [14920/25000], Loss: 0.2018\n",
      "Epoch [1/1], Step [14930/25000], Loss: 0.2768\n",
      "Epoch [1/1], Step [14940/25000], Loss: 0.0745\n",
      "Epoch [1/1], Step [14950/25000], Loss: 0.5010\n",
      "Epoch [1/1], Step [14960/25000], Loss: 0.2213\n",
      "Epoch [1/1], Step [14970/25000], Loss: 0.1661\n",
      "Epoch [1/1], Step [14980/25000], Loss: 0.1626\n",
      "Epoch [1/1], Step [14990/25000], Loss: 0.0793\n",
      "Epoch [1/1], Step [15000/25000], Loss: 0.1332\n",
      "Epoch [1/1], Step [15010/25000], Loss: 0.1060\n",
      "Epoch [1/1], Step [15020/25000], Loss: 0.6322\n",
      "Epoch [1/1], Step [15030/25000], Loss: 0.4085\n",
      "Epoch [1/1], Step [15040/25000], Loss: 0.1679\n",
      "Epoch [1/1], Step [15050/25000], Loss: 0.0352\n",
      "Epoch [1/1], Step [15060/25000], Loss: 0.1098\n",
      "Epoch [1/1], Step [15070/25000], Loss: 0.0831\n",
      "Epoch [1/1], Step [15080/25000], Loss: 0.7203\n",
      "Epoch [1/1], Step [15090/25000], Loss: 0.0294\n",
      "Epoch [1/1], Step [15100/25000], Loss: 0.0488\n",
      "Epoch [1/1], Step [15110/25000], Loss: 0.5821\n",
      "Epoch [1/1], Step [15120/25000], Loss: 0.3872\n",
      "Epoch [1/1], Step [15130/25000], Loss: 0.7093\n",
      "Epoch [1/1], Step [15140/25000], Loss: 0.4920\n",
      "Epoch [1/1], Step [15150/25000], Loss: 0.0610\n",
      "Epoch [1/1], Step [15160/25000], Loss: 0.3852\n",
      "Epoch [1/1], Step [15170/25000], Loss: 0.4443\n",
      "Epoch [1/1], Step [15180/25000], Loss: 0.7664\n",
      "Epoch [1/1], Step [15190/25000], Loss: 1.7224\n",
      "Epoch [1/1], Step [15200/25000], Loss: 0.2707\n",
      "Epoch [1/1], Step [15210/25000], Loss: 0.0573\n",
      "Epoch [1/1], Step [15220/25000], Loss: 0.1561\n",
      "Epoch [1/1], Step [15230/25000], Loss: 0.2375\n",
      "Epoch [1/1], Step [15240/25000], Loss: 0.8821\n",
      "Epoch [1/1], Step [15250/25000], Loss: 0.1502\n",
      "Epoch [1/1], Step [15260/25000], Loss: 0.0783\n",
      "Epoch [1/1], Step [15270/25000], Loss: 1.1793\n",
      "Epoch [1/1], Step [15280/25000], Loss: 0.5642\n",
      "Epoch [1/1], Step [15290/25000], Loss: 0.5217\n",
      "Epoch [1/1], Step [15300/25000], Loss: 0.1432\n",
      "Epoch [1/1], Step [15310/25000], Loss: 0.3444\n",
      "Epoch [1/1], Step [15320/25000], Loss: 1.0036\n",
      "Epoch [1/1], Step [15330/25000], Loss: 0.1983\n",
      "Epoch [1/1], Step [15340/25000], Loss: 0.3540\n",
      "Epoch [1/1], Step [15350/25000], Loss: 0.5963\n",
      "Epoch [1/1], Step [15360/25000], Loss: 0.2771\n",
      "Epoch [1/1], Step [15370/25000], Loss: 1.1133\n",
      "Epoch [1/1], Step [15380/25000], Loss: 0.2250\n",
      "Epoch [1/1], Step [15390/25000], Loss: 0.2115\n",
      "Epoch [1/1], Step [15400/25000], Loss: 0.2914\n",
      "Epoch [1/1], Step [15410/25000], Loss: 0.7606\n",
      "Epoch [1/1], Step [15420/25000], Loss: 0.2228\n",
      "Epoch [1/1], Step [15430/25000], Loss: 0.7697\n",
      "Epoch [1/1], Step [15440/25000], Loss: 0.1190\n",
      "Epoch [1/1], Step [15450/25000], Loss: 0.8312\n",
      "Epoch [1/1], Step [15460/25000], Loss: 0.5005\n",
      "Epoch [1/1], Step [15470/25000], Loss: 0.6319\n",
      "Epoch [1/1], Step [15480/25000], Loss: 0.5155\n",
      "Epoch [1/1], Step [15490/25000], Loss: 0.1266\n",
      "Epoch [1/1], Step [15500/25000], Loss: 0.7473\n",
      "Epoch [1/1], Step [15510/25000], Loss: 0.3894\n",
      "Epoch [1/1], Step [15520/25000], Loss: 0.4352\n",
      "Epoch [1/1], Step [15530/25000], Loss: 0.2042\n",
      "Epoch [1/1], Step [15540/25000], Loss: 0.1116\n",
      "Epoch [1/1], Step [15550/25000], Loss: 0.3107\n",
      "Epoch [1/1], Step [15560/25000], Loss: 0.2690\n",
      "Epoch [1/1], Step [15570/25000], Loss: 1.4437\n",
      "Epoch [1/1], Step [15580/25000], Loss: 0.3106\n",
      "Epoch [1/1], Step [15590/25000], Loss: 0.7152\n",
      "Epoch [1/1], Step [15600/25000], Loss: 0.0475\n",
      "Epoch [1/1], Step [15610/25000], Loss: 0.2668\n",
      "Epoch [1/1], Step [15620/25000], Loss: 0.0673\n",
      "Epoch [1/1], Step [15630/25000], Loss: 0.6015\n",
      "Epoch [1/1], Step [15640/25000], Loss: 0.0840\n",
      "Epoch [1/1], Step [15650/25000], Loss: 0.7999\n",
      "Epoch [1/1], Step [15660/25000], Loss: 0.3873\n",
      "Epoch [1/1], Step [15670/25000], Loss: 0.5677\n",
      "Epoch [1/1], Step [15680/25000], Loss: 0.4636\n",
      "Epoch [1/1], Step [15690/25000], Loss: 0.4278\n",
      "Epoch [1/1], Step [15700/25000], Loss: 0.0904\n",
      "Epoch [1/1], Step [15710/25000], Loss: 0.5487\n",
      "Epoch [1/1], Step [15720/25000], Loss: 0.6264\n",
      "Epoch [1/1], Step [15730/25000], Loss: 0.6749\n",
      "Epoch [1/1], Step [15740/25000], Loss: 0.8992\n",
      "Epoch [1/1], Step [15750/25000], Loss: 0.3427\n",
      "Epoch [1/1], Step [15760/25000], Loss: 0.0586\n",
      "Epoch [1/1], Step [15770/25000], Loss: 0.0995\n",
      "Epoch [1/1], Step [15780/25000], Loss: 0.2246\n",
      "Epoch [1/1], Step [15790/25000], Loss: 0.0291\n",
      "Epoch [1/1], Step [15800/25000], Loss: 0.7498\n",
      "Epoch [1/1], Step [15810/25000], Loss: 0.3340\n",
      "Epoch [1/1], Step [15820/25000], Loss: 0.0897\n",
      "Epoch [1/1], Step [15830/25000], Loss: 0.0958\n",
      "Epoch [1/1], Step [15840/25000], Loss: 0.2993\n",
      "Epoch [1/1], Step [15850/25000], Loss: 0.5493\n",
      "Epoch [1/1], Step [15860/25000], Loss: 0.1436\n",
      "Epoch [1/1], Step [15870/25000], Loss: 0.3188\n",
      "Epoch [1/1], Step [15880/25000], Loss: 0.1417\n",
      "Epoch [1/1], Step [15890/25000], Loss: 0.3779\n",
      "Epoch [1/1], Step [15900/25000], Loss: 0.5557\n",
      "Epoch [1/1], Step [15910/25000], Loss: 1.3765\n",
      "Epoch [1/1], Step [15920/25000], Loss: 0.0542\n",
      "Epoch [1/1], Step [15930/25000], Loss: 0.0442\n",
      "Epoch [1/1], Step [15940/25000], Loss: 0.0685\n",
      "Epoch [1/1], Step [15950/25000], Loss: 0.1884\n",
      "Epoch [1/1], Step [15960/25000], Loss: 0.9619\n",
      "Epoch [1/1], Step [15970/25000], Loss: 0.9334\n",
      "Epoch [1/1], Step [15980/25000], Loss: 0.5159\n",
      "Epoch [1/1], Step [15990/25000], Loss: 0.7575\n",
      "Epoch [1/1], Step [16000/25000], Loss: 0.1917\n",
      "Epoch [1/1], Step [16010/25000], Loss: 0.5748\n",
      "Epoch [1/1], Step [16020/25000], Loss: 0.1261\n",
      "Epoch [1/1], Step [16030/25000], Loss: 0.0572\n",
      "Epoch [1/1], Step [16040/25000], Loss: 0.2125\n",
      "Epoch [1/1], Step [16050/25000], Loss: 0.7056\n",
      "Epoch [1/1], Step [16060/25000], Loss: 0.7934\n",
      "Epoch [1/1], Step [16070/25000], Loss: 0.8860\n",
      "Epoch [1/1], Step [16080/25000], Loss: 0.1082\n",
      "Epoch [1/1], Step [16090/25000], Loss: 0.2761\n",
      "Epoch [1/1], Step [16100/25000], Loss: 0.1487\n",
      "Epoch [1/1], Step [16110/25000], Loss: 0.1856\n",
      "Epoch [1/1], Step [16120/25000], Loss: 0.4419\n",
      "Epoch [1/1], Step [16130/25000], Loss: 0.4927\n",
      "Epoch [1/1], Step [16140/25000], Loss: 1.0114\n",
      "Epoch [1/1], Step [16150/25000], Loss: 0.6573\n",
      "Epoch [1/1], Step [16160/25000], Loss: 0.3332\n",
      "Epoch [1/1], Step [16170/25000], Loss: 0.1905\n",
      "Epoch [1/1], Step [16180/25000], Loss: 0.1911\n",
      "Epoch [1/1], Step [16190/25000], Loss: 0.2925\n",
      "Epoch [1/1], Step [16200/25000], Loss: 0.6402\n",
      "Epoch [1/1], Step [16210/25000], Loss: 0.5703\n",
      "Epoch [1/1], Step [16220/25000], Loss: 0.1789\n",
      "Epoch [1/1], Step [16230/25000], Loss: 0.7255\n",
      "Epoch [1/1], Step [16240/25000], Loss: 0.6755\n",
      "Epoch [1/1], Step [16250/25000], Loss: 0.5383\n",
      "Epoch [1/1], Step [16260/25000], Loss: 0.0813\n",
      "Epoch [1/1], Step [16270/25000], Loss: 1.1882\n",
      "Epoch [1/1], Step [16280/25000], Loss: 0.6317\n",
      "Epoch [1/1], Step [16290/25000], Loss: 0.4130\n",
      "Epoch [1/1], Step [16300/25000], Loss: 0.3249\n",
      "Epoch [1/1], Step [16310/25000], Loss: 0.4641\n",
      "Epoch [1/1], Step [16320/25000], Loss: 0.2475\n",
      "Epoch [1/1], Step [16330/25000], Loss: 0.0465\n",
      "Epoch [1/1], Step [16340/25000], Loss: 0.3597\n",
      "Epoch [1/1], Step [16350/25000], Loss: 0.3363\n",
      "Epoch [1/1], Step [16360/25000], Loss: 0.6115\n",
      "Epoch [1/1], Step [16370/25000], Loss: 0.2237\n",
      "Epoch [1/1], Step [16380/25000], Loss: 0.3785\n",
      "Epoch [1/1], Step [16390/25000], Loss: 0.1036\n",
      "Epoch [1/1], Step [16400/25000], Loss: 0.2227\n",
      "Epoch [1/1], Step [16410/25000], Loss: 0.0583\n",
      "Epoch [1/1], Step [16420/25000], Loss: 0.0376\n",
      "Epoch [1/1], Step [16430/25000], Loss: 0.0490\n",
      "Epoch [1/1], Step [16440/25000], Loss: 0.1456\n",
      "Epoch [1/1], Step [16450/25000], Loss: 0.4295\n",
      "Epoch [1/1], Step [16460/25000], Loss: 0.4224\n",
      "Epoch [1/1], Step [16470/25000], Loss: 0.6891\n",
      "Epoch [1/1], Step [16480/25000], Loss: 0.5404\n",
      "Epoch [1/1], Step [16490/25000], Loss: 0.6900\n",
      "Epoch [1/1], Step [16500/25000], Loss: 0.0496\n",
      "Epoch [1/1], Step [16510/25000], Loss: 0.2318\n",
      "Epoch [1/1], Step [16520/25000], Loss: 0.7558\n",
      "Epoch [1/1], Step [16530/25000], Loss: 0.1157\n",
      "Epoch [1/1], Step [16540/25000], Loss: 0.1173\n",
      "Epoch [1/1], Step [16550/25000], Loss: 0.1204\n",
      "Epoch [1/1], Step [16560/25000], Loss: 0.2409\n",
      "Epoch [1/1], Step [16570/25000], Loss: 0.3410\n",
      "Epoch [1/1], Step [16580/25000], Loss: 0.6224\n",
      "Epoch [1/1], Step [16590/25000], Loss: 0.0529\n",
      "Epoch [1/1], Step [16600/25000], Loss: 0.3940\n",
      "Epoch [1/1], Step [16610/25000], Loss: 0.3687\n",
      "Epoch [1/1], Step [16620/25000], Loss: 0.1557\n",
      "Epoch [1/1], Step [16630/25000], Loss: 0.3005\n",
      "Epoch [1/1], Step [16640/25000], Loss: 0.0600\n",
      "Epoch [1/1], Step [16650/25000], Loss: 0.3329\n",
      "Epoch [1/1], Step [16660/25000], Loss: 0.3363\n",
      "Epoch [1/1], Step [16670/25000], Loss: 0.1313\n",
      "Epoch [1/1], Step [16680/25000], Loss: 0.1084\n",
      "Epoch [1/1], Step [16690/25000], Loss: 0.2681\n",
      "Epoch [1/1], Step [16700/25000], Loss: 0.6327\n",
      "Epoch [1/1], Step [16710/25000], Loss: 0.1686\n",
      "Epoch [1/1], Step [16720/25000], Loss: 0.0763\n",
      "Epoch [1/1], Step [16730/25000], Loss: 0.5804\n",
      "Epoch [1/1], Step [16740/25000], Loss: 0.1034\n",
      "Epoch [1/1], Step [16750/25000], Loss: 0.4517\n",
      "Epoch [1/1], Step [16760/25000], Loss: 0.4151\n",
      "Epoch [1/1], Step [16770/25000], Loss: 0.7526\n",
      "Epoch [1/1], Step [16780/25000], Loss: 1.0464\n",
      "Epoch [1/1], Step [16790/25000], Loss: 0.7764\n",
      "Epoch [1/1], Step [16800/25000], Loss: 0.3942\n",
      "Epoch [1/1], Step [16810/25000], Loss: 0.1746\n",
      "Epoch [1/1], Step [16820/25000], Loss: 0.2494\n",
      "Epoch [1/1], Step [16830/25000], Loss: 0.0294\n",
      "Epoch [1/1], Step [16840/25000], Loss: 0.4261\n",
      "Epoch [1/1], Step [16850/25000], Loss: 0.3099\n",
      "Epoch [1/1], Step [16860/25000], Loss: 0.6973\n",
      "Epoch [1/1], Step [16870/25000], Loss: 0.3968\n",
      "Epoch [1/1], Step [16880/25000], Loss: 0.4152\n",
      "Epoch [1/1], Step [16890/25000], Loss: 1.0510\n",
      "Epoch [1/1], Step [16900/25000], Loss: 0.1352\n",
      "Epoch [1/1], Step [16910/25000], Loss: 0.4928\n",
      "Epoch [1/1], Step [16920/25000], Loss: 0.7758\n",
      "Epoch [1/1], Step [16930/25000], Loss: 0.0700\n",
      "Epoch [1/1], Step [16940/25000], Loss: 0.1995\n",
      "Epoch [1/1], Step [16950/25000], Loss: 0.1804\n",
      "Epoch [1/1], Step [16960/25000], Loss: 0.1561\n",
      "Epoch [1/1], Step [16970/25000], Loss: 0.1360\n",
      "Epoch [1/1], Step [16980/25000], Loss: 0.8565\n",
      "Epoch [1/1], Step [16990/25000], Loss: 0.1302\n",
      "Epoch [1/1], Step [17000/25000], Loss: 0.1242\n",
      "Epoch [1/1], Step [17010/25000], Loss: 0.1899\n",
      "Epoch [1/1], Step [17020/25000], Loss: 0.4747\n",
      "Epoch [1/1], Step [17030/25000], Loss: 0.1324\n",
      "Epoch [1/1], Step [17040/25000], Loss: 0.1741\n",
      "Epoch [1/1], Step [17050/25000], Loss: 0.3708\n",
      "Epoch [1/1], Step [17060/25000], Loss: 0.7012\n",
      "Epoch [1/1], Step [17070/25000], Loss: 0.2587\n",
      "Epoch [1/1], Step [17080/25000], Loss: 0.1357\n",
      "Epoch [1/1], Step [17090/25000], Loss: 0.9460\n",
      "Epoch [1/1], Step [17100/25000], Loss: 0.5685\n",
      "Epoch [1/1], Step [17110/25000], Loss: 0.3555\n",
      "Epoch [1/1], Step [17120/25000], Loss: 0.1527\n",
      "Epoch [1/1], Step [17130/25000], Loss: 0.1634\n",
      "Epoch [1/1], Step [17140/25000], Loss: 0.4718\n",
      "Epoch [1/1], Step [17150/25000], Loss: 0.1334\n",
      "Epoch [1/1], Step [17160/25000], Loss: 0.1189\n",
      "Epoch [1/1], Step [17170/25000], Loss: 0.1254\n",
      "Epoch [1/1], Step [17180/25000], Loss: 0.0664\n",
      "Epoch [1/1], Step [17190/25000], Loss: 0.0307\n",
      "Epoch [1/1], Step [17200/25000], Loss: 0.1441\n",
      "Epoch [1/1], Step [17210/25000], Loss: 0.3897\n",
      "Epoch [1/1], Step [17220/25000], Loss: 0.4122\n",
      "Epoch [1/1], Step [17230/25000], Loss: 0.3125\n",
      "Epoch [1/1], Step [17240/25000], Loss: 1.2217\n",
      "Epoch [1/1], Step [17250/25000], Loss: 0.1265\n",
      "Epoch [1/1], Step [17260/25000], Loss: 0.3485\n",
      "Epoch [1/1], Step [17270/25000], Loss: 0.2859\n",
      "Epoch [1/1], Step [17280/25000], Loss: 0.5913\n",
      "Epoch [1/1], Step [17290/25000], Loss: 0.1231\n",
      "Epoch [1/1], Step [17300/25000], Loss: 0.1308\n",
      "Epoch [1/1], Step [17310/25000], Loss: 0.1361\n",
      "Epoch [1/1], Step [17320/25000], Loss: 0.1291\n",
      "Epoch [1/1], Step [17330/25000], Loss: 0.7396\n",
      "Epoch [1/1], Step [17340/25000], Loss: 0.3612\n",
      "Epoch [1/1], Step [17350/25000], Loss: 0.3741\n",
      "Epoch [1/1], Step [17360/25000], Loss: 0.0361\n",
      "Epoch [1/1], Step [17370/25000], Loss: 0.4396\n",
      "Epoch [1/1], Step [17380/25000], Loss: 0.2694\n",
      "Epoch [1/1], Step [17390/25000], Loss: 0.0473\n",
      "Epoch [1/1], Step [17400/25000], Loss: 0.4425\n",
      "Epoch [1/1], Step [17410/25000], Loss: 0.0968\n",
      "Epoch [1/1], Step [17420/25000], Loss: 0.2620\n",
      "Epoch [1/1], Step [17430/25000], Loss: 0.5013\n",
      "Epoch [1/1], Step [17440/25000], Loss: 0.1163\n",
      "Epoch [1/1], Step [17450/25000], Loss: 0.2772\n",
      "Epoch [1/1], Step [17460/25000], Loss: 0.6231\n",
      "Epoch [1/1], Step [17470/25000], Loss: 0.1356\n",
      "Epoch [1/1], Step [17480/25000], Loss: 0.2696\n",
      "Epoch [1/1], Step [17490/25000], Loss: 0.1833\n",
      "Epoch [1/1], Step [17500/25000], Loss: 0.3091\n",
      "Epoch [1/1], Step [17510/25000], Loss: 0.0300\n",
      "Epoch [1/1], Step [17520/25000], Loss: 0.8437\n",
      "Epoch [1/1], Step [17530/25000], Loss: 0.8008\n",
      "Epoch [1/1], Step [17540/25000], Loss: 0.3438\n",
      "Epoch [1/1], Step [17550/25000], Loss: 0.0831\n",
      "Epoch [1/1], Step [17560/25000], Loss: 0.3977\n",
      "Epoch [1/1], Step [17570/25000], Loss: 0.2187\n",
      "Epoch [1/1], Step [17580/25000], Loss: 0.3624\n",
      "Epoch [1/1], Step [17590/25000], Loss: 0.4132\n",
      "Epoch [1/1], Step [17600/25000], Loss: 0.1302\n",
      "Epoch [1/1], Step [17610/25000], Loss: 0.9286\n",
      "Epoch [1/1], Step [17620/25000], Loss: 0.0783\n",
      "Epoch [1/1], Step [17630/25000], Loss: 0.0557\n",
      "Epoch [1/1], Step [17640/25000], Loss: 0.6649\n",
      "Epoch [1/1], Step [17650/25000], Loss: 0.1076\n",
      "Epoch [1/1], Step [17660/25000], Loss: 0.0640\n",
      "Epoch [1/1], Step [17670/25000], Loss: 0.6692\n",
      "Epoch [1/1], Step [17680/25000], Loss: 0.1002\n",
      "Epoch [1/1], Step [17690/25000], Loss: 0.5169\n",
      "Epoch [1/1], Step [17700/25000], Loss: 0.1156\n",
      "Epoch [1/1], Step [17710/25000], Loss: 0.0758\n",
      "Epoch [1/1], Step [17720/25000], Loss: 1.0376\n",
      "Epoch [1/1], Step [17730/25000], Loss: 0.1592\n",
      "Epoch [1/1], Step [17740/25000], Loss: 0.1225\n",
      "Epoch [1/1], Step [17750/25000], Loss: 0.1223\n",
      "Epoch [1/1], Step [17760/25000], Loss: 0.5280\n",
      "Epoch [1/1], Step [17770/25000], Loss: 0.4942\n",
      "Epoch [1/1], Step [17780/25000], Loss: 1.4237\n",
      "Epoch [1/1], Step [17790/25000], Loss: 0.0738\n",
      "Epoch [1/1], Step [17800/25000], Loss: 0.0265\n",
      "Epoch [1/1], Step [17810/25000], Loss: 0.1051\n",
      "Epoch [1/1], Step [17820/25000], Loss: 1.5026\n",
      "Epoch [1/1], Step [17830/25000], Loss: 0.2671\n",
      "Epoch [1/1], Step [17840/25000], Loss: 0.3424\n",
      "Epoch [1/1], Step [17850/25000], Loss: 0.7949\n",
      "Epoch [1/1], Step [17860/25000], Loss: 0.0820\n",
      "Epoch [1/1], Step [17870/25000], Loss: 0.3929\n",
      "Epoch [1/1], Step [17880/25000], Loss: 0.2301\n",
      "Epoch [1/1], Step [17890/25000], Loss: 0.3985\n",
      "Epoch [1/1], Step [17900/25000], Loss: 0.7817\n",
      "Epoch [1/1], Step [17910/25000], Loss: 1.0229\n",
      "Epoch [1/1], Step [17920/25000], Loss: 0.4503\n",
      "Epoch [1/1], Step [17930/25000], Loss: 1.3963\n",
      "Epoch [1/1], Step [17940/25000], Loss: 0.0566\n",
      "Epoch [1/1], Step [17950/25000], Loss: 0.2618\n",
      "Epoch [1/1], Step [17960/25000], Loss: 0.2234\n",
      "Epoch [1/1], Step [17970/25000], Loss: 0.4136\n",
      "Epoch [1/1], Step [17980/25000], Loss: 0.1407\n",
      "Epoch [1/1], Step [17990/25000], Loss: 0.4524\n",
      "Epoch [1/1], Step [18000/25000], Loss: 0.0272\n",
      "Epoch [1/1], Step [18010/25000], Loss: 0.2924\n",
      "Epoch [1/1], Step [18020/25000], Loss: 0.3313\n",
      "Epoch [1/1], Step [18030/25000], Loss: 0.7207\n",
      "Epoch [1/1], Step [18040/25000], Loss: 0.0842\n",
      "Epoch [1/1], Step [18050/25000], Loss: 0.9600\n",
      "Epoch [1/1], Step [18060/25000], Loss: 0.1077\n",
      "Epoch [1/1], Step [18070/25000], Loss: 0.4012\n",
      "Epoch [1/1], Step [18080/25000], Loss: 0.2746\n",
      "Epoch [1/1], Step [18090/25000], Loss: 0.5024\n",
      "Epoch [1/1], Step [18100/25000], Loss: 0.1616\n",
      "Epoch [1/1], Step [18110/25000], Loss: 0.2847\n",
      "Epoch [1/1], Step [18120/25000], Loss: 0.1845\n",
      "Epoch [1/1], Step [18130/25000], Loss: 0.0643\n",
      "Epoch [1/1], Step [18140/25000], Loss: 0.4360\n",
      "Epoch [1/1], Step [18150/25000], Loss: 0.0372\n",
      "Epoch [1/1], Step [18160/25000], Loss: 0.0254\n",
      "Epoch [1/1], Step [18170/25000], Loss: 0.0546\n",
      "Epoch [1/1], Step [18180/25000], Loss: 0.5143\n",
      "Epoch [1/1], Step [18190/25000], Loss: 0.1542\n",
      "Epoch [1/1], Step [18200/25000], Loss: 0.1733\n",
      "Epoch [1/1], Step [18210/25000], Loss: 0.9452\n",
      "Epoch [1/1], Step [18220/25000], Loss: 0.3556\n",
      "Epoch [1/1], Step [18230/25000], Loss: 0.1755\n",
      "Epoch [1/1], Step [18240/25000], Loss: 0.1217\n",
      "Epoch [1/1], Step [18250/25000], Loss: 0.0788\n",
      "Epoch [1/1], Step [18260/25000], Loss: 0.4486\n",
      "Epoch [1/1], Step [18270/25000], Loss: 0.2864\n",
      "Epoch [1/1], Step [18280/25000], Loss: 0.0363\n",
      "Epoch [1/1], Step [18290/25000], Loss: 0.0444\n",
      "Epoch [1/1], Step [18300/25000], Loss: 0.5609\n",
      "Epoch [1/1], Step [18310/25000], Loss: 0.4931\n",
      "Epoch [1/1], Step [18320/25000], Loss: 0.5630\n",
      "Epoch [1/1], Step [18330/25000], Loss: 0.3868\n",
      "Epoch [1/1], Step [18340/25000], Loss: 0.4980\n",
      "Epoch [1/1], Step [18350/25000], Loss: 0.0966\n",
      "Epoch [1/1], Step [18360/25000], Loss: 0.1369\n",
      "Epoch [1/1], Step [18370/25000], Loss: 0.3402\n",
      "Epoch [1/1], Step [18380/25000], Loss: 0.1481\n",
      "Epoch [1/1], Step [18390/25000], Loss: 0.2370\n",
      "Epoch [1/1], Step [18400/25000], Loss: 0.1806\n",
      "Epoch [1/1], Step [18410/25000], Loss: 0.1034\n",
      "Epoch [1/1], Step [18420/25000], Loss: 0.3462\n",
      "Epoch [1/1], Step [18430/25000], Loss: 0.1719\n",
      "Epoch [1/1], Step [18440/25000], Loss: 0.1257\n",
      "Epoch [1/1], Step [18450/25000], Loss: 0.2648\n",
      "Epoch [1/1], Step [18460/25000], Loss: 0.1289\n",
      "Epoch [1/1], Step [18470/25000], Loss: 0.1500\n",
      "Epoch [1/1], Step [18480/25000], Loss: 0.6299\n",
      "Epoch [1/1], Step [18490/25000], Loss: 0.3976\n",
      "Epoch [1/1], Step [18500/25000], Loss: 0.0398\n",
      "Epoch [1/1], Step [18510/25000], Loss: 0.3774\n",
      "Epoch [1/1], Step [18520/25000], Loss: 0.1079\n",
      "Epoch [1/1], Step [18530/25000], Loss: 0.0517\n",
      "Epoch [1/1], Step [18540/25000], Loss: 0.4063\n",
      "Epoch [1/1], Step [18550/25000], Loss: 0.4920\n",
      "Epoch [1/1], Step [18560/25000], Loss: 0.1521\n",
      "Epoch [1/1], Step [18570/25000], Loss: 0.5716\n",
      "Epoch [1/1], Step [18580/25000], Loss: 0.1573\n",
      "Epoch [1/1], Step [18590/25000], Loss: 0.0317\n",
      "Epoch [1/1], Step [18600/25000], Loss: 0.1147\n",
      "Epoch [1/1], Step [18610/25000], Loss: 0.5718\n",
      "Epoch [1/1], Step [18620/25000], Loss: 0.0829\n",
      "Epoch [1/1], Step [18630/25000], Loss: 0.3095\n",
      "Epoch [1/1], Step [18640/25000], Loss: 1.1494\n",
      "Epoch [1/1], Step [18650/25000], Loss: 0.2397\n",
      "Epoch [1/1], Step [18660/25000], Loss: 0.1606\n",
      "Epoch [1/1], Step [18670/25000], Loss: 0.5077\n",
      "Epoch [1/1], Step [18680/25000], Loss: 0.1969\n",
      "Epoch [1/1], Step [18690/25000], Loss: 0.1458\n",
      "Epoch [1/1], Step [18700/25000], Loss: 0.5359\n",
      "Epoch [1/1], Step [18710/25000], Loss: 1.0606\n",
      "Epoch [1/1], Step [18720/25000], Loss: 0.3681\n",
      "Epoch [1/1], Step [18730/25000], Loss: 0.9957\n",
      "Epoch [1/1], Step [18740/25000], Loss: 0.2713\n",
      "Epoch [1/1], Step [18750/25000], Loss: 0.6353\n",
      "Epoch [1/1], Step [18760/25000], Loss: 0.7221\n",
      "Epoch [1/1], Step [18770/25000], Loss: 0.3080\n",
      "Epoch [1/1], Step [18780/25000], Loss: 0.1068\n",
      "Epoch [1/1], Step [18790/25000], Loss: 0.1892\n",
      "Epoch [1/1], Step [18800/25000], Loss: 0.6676\n",
      "Epoch [1/1], Step [18810/25000], Loss: 0.1236\n",
      "Epoch [1/1], Step [18820/25000], Loss: 0.7168\n",
      "Epoch [1/1], Step [18830/25000], Loss: 0.5106\n",
      "Epoch [1/1], Step [18840/25000], Loss: 0.1877\n",
      "Epoch [1/1], Step [18850/25000], Loss: 0.5663\n",
      "Epoch [1/1], Step [18860/25000], Loss: 0.1814\n",
      "Epoch [1/1], Step [18870/25000], Loss: 0.0251\n",
      "Epoch [1/1], Step [18880/25000], Loss: 0.2017\n",
      "Epoch [1/1], Step [18890/25000], Loss: 0.0651\n",
      "Epoch [1/1], Step [18900/25000], Loss: 0.1793\n",
      "Epoch [1/1], Step [18910/25000], Loss: 0.1338\n",
      "Epoch [1/1], Step [18920/25000], Loss: 0.3739\n",
      "Epoch [1/1], Step [18930/25000], Loss: 0.2156\n",
      "Epoch [1/1], Step [18940/25000], Loss: 0.2871\n",
      "Epoch [1/1], Step [18950/25000], Loss: 0.0385\n",
      "Epoch [1/1], Step [18960/25000], Loss: 0.1793\n",
      "Epoch [1/1], Step [18970/25000], Loss: 0.0811\n",
      "Epoch [1/1], Step [18980/25000], Loss: 0.4136\n",
      "Epoch [1/1], Step [18990/25000], Loss: 0.0423\n",
      "Epoch [1/1], Step [19000/25000], Loss: 0.6862\n",
      "Epoch [1/1], Step [19010/25000], Loss: 0.6734\n",
      "Epoch [1/1], Step [19020/25000], Loss: 0.1337\n",
      "Epoch [1/1], Step [19030/25000], Loss: 0.1390\n",
      "Epoch [1/1], Step [19040/25000], Loss: 0.0724\n",
      "Epoch [1/1], Step [19050/25000], Loss: 0.6321\n",
      "Epoch [1/1], Step [19060/25000], Loss: 0.1874\n",
      "Epoch [1/1], Step [19070/25000], Loss: 0.1793\n",
      "Epoch [1/1], Step [19080/25000], Loss: 0.5167\n",
      "Epoch [1/1], Step [19090/25000], Loss: 0.3368\n",
      "Epoch [1/1], Step [19100/25000], Loss: 0.1838\n",
      "Epoch [1/1], Step [19110/25000], Loss: 0.0400\n",
      "Epoch [1/1], Step [19120/25000], Loss: 0.3361\n",
      "Epoch [1/1], Step [19130/25000], Loss: 0.2326\n",
      "Epoch [1/1], Step [19140/25000], Loss: 0.5732\n",
      "Epoch [1/1], Step [19150/25000], Loss: 0.8574\n",
      "Epoch [1/1], Step [19160/25000], Loss: 0.4086\n",
      "Epoch [1/1], Step [19170/25000], Loss: 0.1381\n",
      "Epoch [1/1], Step [19180/25000], Loss: 0.5810\n",
      "Epoch [1/1], Step [19190/25000], Loss: 0.0535\n",
      "Epoch [1/1], Step [19200/25000], Loss: 0.2665\n",
      "Epoch [1/1], Step [19210/25000], Loss: 0.7168\n",
      "Epoch [1/1], Step [19220/25000], Loss: 0.1713\n",
      "Epoch [1/1], Step [19230/25000], Loss: 0.0858\n",
      "Epoch [1/1], Step [19240/25000], Loss: 0.1648\n",
      "Epoch [1/1], Step [19250/25000], Loss: 0.1471\n",
      "Epoch [1/1], Step [19260/25000], Loss: 0.1012\n",
      "Epoch [1/1], Step [19270/25000], Loss: 0.4154\n",
      "Epoch [1/1], Step [19280/25000], Loss: 0.2799\n",
      "Epoch [1/1], Step [19290/25000], Loss: 0.0389\n",
      "Epoch [1/1], Step [19300/25000], Loss: 0.1133\n",
      "Epoch [1/1], Step [19310/25000], Loss: 0.2820\n",
      "Epoch [1/1], Step [19320/25000], Loss: 0.4206\n",
      "Epoch [1/1], Step [19330/25000], Loss: 0.1637\n",
      "Epoch [1/1], Step [19340/25000], Loss: 0.0622\n",
      "Epoch [1/1], Step [19350/25000], Loss: 0.2543\n",
      "Epoch [1/1], Step [19360/25000], Loss: 0.2319\n",
      "Epoch [1/1], Step [19370/25000], Loss: 0.5962\n",
      "Epoch [1/1], Step [19380/25000], Loss: 0.2113\n",
      "Epoch [1/1], Step [19390/25000], Loss: 0.5645\n",
      "Epoch [1/1], Step [19400/25000], Loss: 0.2738\n",
      "Epoch [1/1], Step [19410/25000], Loss: 0.3482\n",
      "Epoch [1/1], Step [19420/25000], Loss: 0.4001\n",
      "Epoch [1/1], Step [19430/25000], Loss: 0.5434\n",
      "Epoch [1/1], Step [19440/25000], Loss: 1.0482\n",
      "Epoch [1/1], Step [19450/25000], Loss: 0.1809\n",
      "Epoch [1/1], Step [19460/25000], Loss: 0.1511\n",
      "Epoch [1/1], Step [19470/25000], Loss: 0.1746\n",
      "Epoch [1/1], Step [19480/25000], Loss: 0.1680\n",
      "Epoch [1/1], Step [19490/25000], Loss: 0.1201\n",
      "Epoch [1/1], Step [19500/25000], Loss: 0.4636\n",
      "Epoch [1/1], Step [19510/25000], Loss: 0.0446\n",
      "Epoch [1/1], Step [19520/25000], Loss: 0.7427\n",
      "Epoch [1/1], Step [19530/25000], Loss: 0.4046\n",
      "Epoch [1/1], Step [19540/25000], Loss: 1.0607\n",
      "Epoch [1/1], Step [19550/25000], Loss: 0.4519\n",
      "Epoch [1/1], Step [19560/25000], Loss: 0.1234\n",
      "Epoch [1/1], Step [19570/25000], Loss: 0.8079\n",
      "Epoch [1/1], Step [19580/25000], Loss: 0.5977\n",
      "Epoch [1/1], Step [19590/25000], Loss: 0.6666\n",
      "Epoch [1/1], Step [19600/25000], Loss: 0.3471\n",
      "Epoch [1/1], Step [19610/25000], Loss: 0.0944\n",
      "Epoch [1/1], Step [19620/25000], Loss: 0.1375\n",
      "Epoch [1/1], Step [19630/25000], Loss: 0.2921\n",
      "Epoch [1/1], Step [19640/25000], Loss: 0.0774\n",
      "Epoch [1/1], Step [19650/25000], Loss: 0.1687\n",
      "Epoch [1/1], Step [19660/25000], Loss: 0.1552\n",
      "Epoch [1/1], Step [19670/25000], Loss: 0.0352\n",
      "Epoch [1/1], Step [19680/25000], Loss: 0.2420\n",
      "Epoch [1/1], Step [19690/25000], Loss: 0.5058\n",
      "Epoch [1/1], Step [19700/25000], Loss: 0.2771\n",
      "Epoch [1/1], Step [19710/25000], Loss: 0.1069\n",
      "Epoch [1/1], Step [19720/25000], Loss: 0.4226\n",
      "Epoch [1/1], Step [19730/25000], Loss: 0.2830\n",
      "Epoch [1/1], Step [19740/25000], Loss: 0.4002\n",
      "Epoch [1/1], Step [19750/25000], Loss: 0.5918\n",
      "Epoch [1/1], Step [19760/25000], Loss: 0.3286\n",
      "Epoch [1/1], Step [19770/25000], Loss: 0.0629\n",
      "Epoch [1/1], Step [19780/25000], Loss: 0.1246\n",
      "Epoch [1/1], Step [19790/25000], Loss: 0.0551\n",
      "Epoch [1/1], Step [19800/25000], Loss: 0.2491\n",
      "Epoch [1/1], Step [19810/25000], Loss: 0.0849\n",
      "Epoch [1/1], Step [19820/25000], Loss: 0.2320\n",
      "Epoch [1/1], Step [19830/25000], Loss: 0.3069\n",
      "Epoch [1/1], Step [19840/25000], Loss: 0.3301\n",
      "Epoch [1/1], Step [19850/25000], Loss: 0.3472\n",
      "Epoch [1/1], Step [19860/25000], Loss: 0.6391\n",
      "Epoch [1/1], Step [19870/25000], Loss: 0.1815\n",
      "Epoch [1/1], Step [19880/25000], Loss: 0.3138\n",
      "Epoch [1/1], Step [19890/25000], Loss: 0.1944\n",
      "Epoch [1/1], Step [19900/25000], Loss: 0.3238\n",
      "Epoch [1/1], Step [19910/25000], Loss: 0.0227\n",
      "Epoch [1/1], Step [19920/25000], Loss: 0.1368\n",
      "Epoch [1/1], Step [19930/25000], Loss: 0.3103\n",
      "Epoch [1/1], Step [19940/25000], Loss: 0.0362\n",
      "Epoch [1/1], Step [19950/25000], Loss: 0.7401\n",
      "Epoch [1/1], Step [19960/25000], Loss: 0.0524\n",
      "Epoch [1/1], Step [19970/25000], Loss: 0.6011\n",
      "Epoch [1/1], Step [19980/25000], Loss: 0.4647\n",
      "Epoch [1/1], Step [19990/25000], Loss: 0.0631\n",
      "Epoch [1/1], Step [20000/25000], Loss: 0.0771\n",
      "Epoch [1/1], Step [20010/25000], Loss: 0.6062\n",
      "Epoch [1/1], Step [20020/25000], Loss: 0.5377\n",
      "Epoch [1/1], Step [20030/25000], Loss: 0.3151\n",
      "Epoch [1/1], Step [20040/25000], Loss: 0.4173\n",
      "Epoch [1/1], Step [20050/25000], Loss: 0.6210\n",
      "Epoch [1/1], Step [20060/25000], Loss: 0.1710\n",
      "Epoch [1/1], Step [20070/25000], Loss: 0.0751\n",
      "Epoch [1/1], Step [20080/25000], Loss: 0.0367\n",
      "Epoch [1/1], Step [20090/25000], Loss: 0.0700\n",
      "Epoch [1/1], Step [20100/25000], Loss: 0.0323\n",
      "Epoch [1/1], Step [20110/25000], Loss: 0.0293\n",
      "Epoch [1/1], Step [20120/25000], Loss: 0.1277\n",
      "Epoch [1/1], Step [20130/25000], Loss: 0.2632\n",
      "Epoch [1/1], Step [20140/25000], Loss: 0.8667\n",
      "Epoch [1/1], Step [20150/25000], Loss: 0.3039\n",
      "Epoch [1/1], Step [20160/25000], Loss: 0.1861\n",
      "Epoch [1/1], Step [20170/25000], Loss: 0.1620\n",
      "Epoch [1/1], Step [20180/25000], Loss: 0.7776\n",
      "Epoch [1/1], Step [20190/25000], Loss: 0.0210\n",
      "Epoch [1/1], Step [20200/25000], Loss: 0.4267\n",
      "Epoch [1/1], Step [20210/25000], Loss: 0.5909\n",
      "Epoch [1/1], Step [20220/25000], Loss: 0.0760\n",
      "Epoch [1/1], Step [20230/25000], Loss: 0.7704\n",
      "Epoch [1/1], Step [20240/25000], Loss: 0.2306\n",
      "Epoch [1/1], Step [20250/25000], Loss: 0.6925\n",
      "Epoch [1/1], Step [20260/25000], Loss: 0.1782\n",
      "Epoch [1/1], Step [20270/25000], Loss: 0.0339\n",
      "Epoch [1/1], Step [20280/25000], Loss: 1.0486\n",
      "Epoch [1/1], Step [20290/25000], Loss: 0.1818\n",
      "Epoch [1/1], Step [20300/25000], Loss: 0.0930\n",
      "Epoch [1/1], Step [20310/25000], Loss: 0.3819\n",
      "Epoch [1/1], Step [20320/25000], Loss: 0.3969\n",
      "Epoch [1/1], Step [20330/25000], Loss: 0.1419\n",
      "Epoch [1/1], Step [20340/25000], Loss: 0.1444\n",
      "Epoch [1/1], Step [20350/25000], Loss: 0.7740\n",
      "Epoch [1/1], Step [20360/25000], Loss: 0.6186\n",
      "Epoch [1/1], Step [20370/25000], Loss: 0.1519\n",
      "Epoch [1/1], Step [20380/25000], Loss: 0.0884\n",
      "Epoch [1/1], Step [20390/25000], Loss: 1.0189\n",
      "Epoch [1/1], Step [20400/25000], Loss: 0.3237\n",
      "Epoch [1/1], Step [20410/25000], Loss: 0.2124\n",
      "Epoch [1/1], Step [20420/25000], Loss: 0.3569\n",
      "Epoch [1/1], Step [20430/25000], Loss: 0.0467\n",
      "Epoch [1/1], Step [20440/25000], Loss: 0.2592\n",
      "Epoch [1/1], Step [20450/25000], Loss: 0.5912\n",
      "Epoch [1/1], Step [20460/25000], Loss: 0.1065\n",
      "Epoch [1/1], Step [20470/25000], Loss: 0.8448\n",
      "Epoch [1/1], Step [20480/25000], Loss: 0.1087\n",
      "Epoch [1/1], Step [20490/25000], Loss: 0.0680\n",
      "Epoch [1/1], Step [20500/25000], Loss: 1.0078\n",
      "Epoch [1/1], Step [20510/25000], Loss: 1.2508\n",
      "Epoch [1/1], Step [20520/25000], Loss: 0.4154\n",
      "Epoch [1/1], Step [20530/25000], Loss: 0.4886\n",
      "Epoch [1/1], Step [20540/25000], Loss: 0.1999\n",
      "Epoch [1/1], Step [20550/25000], Loss: 0.7933\n",
      "Epoch [1/1], Step [20560/25000], Loss: 0.2830\n",
      "Epoch [1/1], Step [20570/25000], Loss: 0.3400\n",
      "Epoch [1/1], Step [20580/25000], Loss: 0.0898\n",
      "Epoch [1/1], Step [20590/25000], Loss: 0.0401\n",
      "Epoch [1/1], Step [20600/25000], Loss: 0.4471\n",
      "Epoch [1/1], Step [20610/25000], Loss: 0.0258\n",
      "Epoch [1/1], Step [20620/25000], Loss: 0.0238\n",
      "Epoch [1/1], Step [20630/25000], Loss: 0.0676\n",
      "Epoch [1/1], Step [20640/25000], Loss: 0.1992\n",
      "Epoch [1/1], Step [20650/25000], Loss: 0.2431\n",
      "Epoch [1/1], Step [20660/25000], Loss: 0.1931\n",
      "Epoch [1/1], Step [20670/25000], Loss: 0.3013\n",
      "Epoch [1/1], Step [20680/25000], Loss: 0.1047\n",
      "Epoch [1/1], Step [20690/25000], Loss: 0.6730\n",
      "Epoch [1/1], Step [20700/25000], Loss: 0.0286\n",
      "Epoch [1/1], Step [20710/25000], Loss: 0.6597\n",
      "Epoch [1/1], Step [20720/25000], Loss: 0.3338\n",
      "Epoch [1/1], Step [20730/25000], Loss: 0.1784\n",
      "Epoch [1/1], Step [20740/25000], Loss: 0.1940\n",
      "Epoch [1/1], Step [20750/25000], Loss: 0.9714\n",
      "Epoch [1/1], Step [20760/25000], Loss: 0.3475\n",
      "Epoch [1/1], Step [20770/25000], Loss: 0.9935\n",
      "Epoch [1/1], Step [20780/25000], Loss: 0.2320\n",
      "Epoch [1/1], Step [20790/25000], Loss: 0.7231\n",
      "Epoch [1/1], Step [20800/25000], Loss: 0.2099\n",
      "Epoch [1/1], Step [20810/25000], Loss: 0.7749\n",
      "Epoch [1/1], Step [20820/25000], Loss: 0.2426\n",
      "Epoch [1/1], Step [20830/25000], Loss: 0.3647\n",
      "Epoch [1/1], Step [20840/25000], Loss: 0.1041\n",
      "Epoch [1/1], Step [20850/25000], Loss: 0.1465\n",
      "Epoch [1/1], Step [20860/25000], Loss: 0.7765\n",
      "Epoch [1/1], Step [20870/25000], Loss: 0.3382\n",
      "Epoch [1/1], Step [20880/25000], Loss: 0.1516\n",
      "Epoch [1/1], Step [20890/25000], Loss: 0.7817\n",
      "Epoch [1/1], Step [20900/25000], Loss: 0.1380\n",
      "Epoch [1/1], Step [20910/25000], Loss: 0.1924\n",
      "Epoch [1/1], Step [20920/25000], Loss: 0.0318\n",
      "Epoch [1/1], Step [20930/25000], Loss: 0.1790\n",
      "Epoch [1/1], Step [20940/25000], Loss: 0.9633\n",
      "Epoch [1/1], Step [20950/25000], Loss: 0.1140\n",
      "Epoch [1/1], Step [20960/25000], Loss: 0.2081\n",
      "Epoch [1/1], Step [20970/25000], Loss: 0.0859\n",
      "Epoch [1/1], Step [20980/25000], Loss: 0.0506\n",
      "Epoch [1/1], Step [20990/25000], Loss: 0.1201\n",
      "Epoch [1/1], Step [21000/25000], Loss: 1.0958\n",
      "Epoch [1/1], Step [21010/25000], Loss: 0.1461\n",
      "Epoch [1/1], Step [21020/25000], Loss: 0.0450\n",
      "Epoch [1/1], Step [21030/25000], Loss: 0.0770\n",
      "Epoch [1/1], Step [21040/25000], Loss: 0.0250\n",
      "Epoch [1/1], Step [21050/25000], Loss: 0.4104\n",
      "Epoch [1/1], Step [21060/25000], Loss: 0.7632\n",
      "Epoch [1/1], Step [21070/25000], Loss: 0.0711\n",
      "Epoch [1/1], Step [21080/25000], Loss: 0.1262\n",
      "Epoch [1/1], Step [21090/25000], Loss: 0.6590\n",
      "Epoch [1/1], Step [21100/25000], Loss: 0.1502\n",
      "Epoch [1/1], Step [21110/25000], Loss: 0.3257\n",
      "Epoch [1/1], Step [21120/25000], Loss: 0.9960\n",
      "Epoch [1/1], Step [21130/25000], Loss: 0.8265\n",
      "Epoch [1/1], Step [21140/25000], Loss: 0.5695\n",
      "Epoch [1/1], Step [21150/25000], Loss: 0.1357\n",
      "Epoch [1/1], Step [21160/25000], Loss: 0.0786\n",
      "Epoch [1/1], Step [21170/25000], Loss: 0.7443\n",
      "Epoch [1/1], Step [21180/25000], Loss: 0.3505\n",
      "Epoch [1/1], Step [21190/25000], Loss: 0.0803\n",
      "Epoch [1/1], Step [21200/25000], Loss: 0.4735\n",
      "Epoch [1/1], Step [21210/25000], Loss: 0.2856\n",
      "Epoch [1/1], Step [21220/25000], Loss: 0.1342\n",
      "Epoch [1/1], Step [21230/25000], Loss: 0.0560\n",
      "Epoch [1/1], Step [21240/25000], Loss: 0.1075\n",
      "Epoch [1/1], Step [21250/25000], Loss: 0.1461\n",
      "Epoch [1/1], Step [21260/25000], Loss: 0.1422\n",
      "Epoch [1/1], Step [21270/25000], Loss: 0.4356\n",
      "Epoch [1/1], Step [21280/25000], Loss: 0.0681\n",
      "Epoch [1/1], Step [21290/25000], Loss: 0.5106\n",
      "Epoch [1/1], Step [21300/25000], Loss: 0.0773\n",
      "Epoch [1/1], Step [21310/25000], Loss: 0.1067\n",
      "Epoch [1/1], Step [21320/25000], Loss: 0.1313\n",
      "Epoch [1/1], Step [21330/25000], Loss: 0.2744\n",
      "Epoch [1/1], Step [21340/25000], Loss: 0.0685\n",
      "Epoch [1/1], Step [21350/25000], Loss: 0.0695\n",
      "Epoch [1/1], Step [21360/25000], Loss: 0.1316\n",
      "Epoch [1/1], Step [21370/25000], Loss: 0.0849\n",
      "Epoch [1/1], Step [21380/25000], Loss: 0.2524\n",
      "Epoch [1/1], Step [21390/25000], Loss: 0.0847\n",
      "Epoch [1/1], Step [21400/25000], Loss: 0.0334\n",
      "Epoch [1/1], Step [21410/25000], Loss: 0.0253\n",
      "Epoch [1/1], Step [21420/25000], Loss: 0.0606\n",
      "Epoch [1/1], Step [21430/25000], Loss: 1.1296\n",
      "Epoch [1/1], Step [21440/25000], Loss: 0.1441\n",
      "Epoch [1/1], Step [21450/25000], Loss: 0.2040\n",
      "Epoch [1/1], Step [21460/25000], Loss: 0.0638\n",
      "Epoch [1/1], Step [21470/25000], Loss: 0.1636\n",
      "Epoch [1/1], Step [21480/25000], Loss: 0.0603\n",
      "Epoch [1/1], Step [21490/25000], Loss: 0.1382\n",
      "Epoch [1/1], Step [21500/25000], Loss: 0.0510\n",
      "Epoch [1/1], Step [21510/25000], Loss: 0.5393\n",
      "Epoch [1/1], Step [21520/25000], Loss: 0.1185\n",
      "Epoch [1/1], Step [21530/25000], Loss: 0.2390\n",
      "Epoch [1/1], Step [21540/25000], Loss: 0.0850\n",
      "Epoch [1/1], Step [21550/25000], Loss: 0.0491\n",
      "Epoch [1/1], Step [21560/25000], Loss: 0.2445\n",
      "Epoch [1/1], Step [21570/25000], Loss: 1.0202\n",
      "Epoch [1/1], Step [21580/25000], Loss: 0.1627\n",
      "Epoch [1/1], Step [21590/25000], Loss: 0.1059\n",
      "Epoch [1/1], Step [21600/25000], Loss: 0.0668\n",
      "Epoch [1/1], Step [21610/25000], Loss: 0.0857\n",
      "Epoch [1/1], Step [21620/25000], Loss: 0.0610\n",
      "Epoch [1/1], Step [21630/25000], Loss: 0.5273\n",
      "Epoch [1/1], Step [21640/25000], Loss: 0.0788\n",
      "Epoch [1/1], Step [21650/25000], Loss: 0.1766\n",
      "Epoch [1/1], Step [21660/25000], Loss: 0.1188\n",
      "Epoch [1/1], Step [21670/25000], Loss: 0.0197\n",
      "Epoch [1/1], Step [21680/25000], Loss: 0.0451\n",
      "Epoch [1/1], Step [21690/25000], Loss: 0.4926\n",
      "Epoch [1/1], Step [21700/25000], Loss: 0.2866\n",
      "Epoch [1/1], Step [21710/25000], Loss: 0.3665\n",
      "Epoch [1/1], Step [21720/25000], Loss: 0.2179\n",
      "Epoch [1/1], Step [21730/25000], Loss: 0.4364\n",
      "Epoch [1/1], Step [21740/25000], Loss: 0.2726\n",
      "Epoch [1/1], Step [21750/25000], Loss: 0.0839\n",
      "Epoch [1/1], Step [21760/25000], Loss: 0.2432\n",
      "Epoch [1/1], Step [21770/25000], Loss: 0.3721\n",
      "Epoch [1/1], Step [21780/25000], Loss: 0.1012\n",
      "Epoch [1/1], Step [21790/25000], Loss: 0.7842\n",
      "Epoch [1/1], Step [21800/25000], Loss: 0.3864\n",
      "Epoch [1/1], Step [21810/25000], Loss: 0.3861\n",
      "Epoch [1/1], Step [21820/25000], Loss: 0.3809\n",
      "Epoch [1/1], Step [21830/25000], Loss: 0.1211\n",
      "Epoch [1/1], Step [21840/25000], Loss: 0.6876\n",
      "Epoch [1/1], Step [21850/25000], Loss: 0.0331\n",
      "Epoch [1/1], Step [21860/25000], Loss: 0.2517\n",
      "Epoch [1/1], Step [21870/25000], Loss: 0.1181\n",
      "Epoch [1/1], Step [21880/25000], Loss: 0.0944\n",
      "Epoch [1/1], Step [21890/25000], Loss: 0.1191\n",
      "Epoch [1/1], Step [21900/25000], Loss: 0.0855\n",
      "Epoch [1/1], Step [21910/25000], Loss: 0.6415\n",
      "Epoch [1/1], Step [21920/25000], Loss: 0.0870\n",
      "Epoch [1/1], Step [21930/25000], Loss: 0.1215\n",
      "Epoch [1/1], Step [21940/25000], Loss: 0.0434\n",
      "Epoch [1/1], Step [21950/25000], Loss: 0.0415\n",
      "Epoch [1/1], Step [21960/25000], Loss: 0.2820\n",
      "Epoch [1/1], Step [21970/25000], Loss: 0.1554\n",
      "Epoch [1/1], Step [21980/25000], Loss: 0.3188\n",
      "Epoch [1/1], Step [21990/25000], Loss: 0.0915\n",
      "Epoch [1/1], Step [22000/25000], Loss: 0.6837\n",
      "Epoch [1/1], Step [22010/25000], Loss: 0.0612\n",
      "Epoch [1/1], Step [22020/25000], Loss: 0.1164\n",
      "Epoch [1/1], Step [22030/25000], Loss: 0.2297\n",
      "Epoch [1/1], Step [22040/25000], Loss: 0.4692\n",
      "Epoch [1/1], Step [22050/25000], Loss: 0.5234\n",
      "Epoch [1/1], Step [22060/25000], Loss: 1.3696\n",
      "Epoch [1/1], Step [22070/25000], Loss: 0.0675\n",
      "Epoch [1/1], Step [22080/25000], Loss: 0.2386\n",
      "Epoch [1/1], Step [22090/25000], Loss: 0.1103\n",
      "Epoch [1/1], Step [22100/25000], Loss: 0.3959\n",
      "Epoch [1/1], Step [22110/25000], Loss: 0.0601\n",
      "Epoch [1/1], Step [22120/25000], Loss: 0.4596\n",
      "Epoch [1/1], Step [22130/25000], Loss: 0.5037\n",
      "Epoch [1/1], Step [22140/25000], Loss: 0.1650\n",
      "Epoch [1/1], Step [22150/25000], Loss: 0.0767\n",
      "Epoch [1/1], Step [22160/25000], Loss: 0.2398\n",
      "Epoch [1/1], Step [22170/25000], Loss: 0.0732\n",
      "Epoch [1/1], Step [22180/25000], Loss: 0.0981\n",
      "Epoch [1/1], Step [22190/25000], Loss: 0.6918\n",
      "Epoch [1/1], Step [22200/25000], Loss: 0.9406\n",
      "Epoch [1/1], Step [22210/25000], Loss: 0.3472\n",
      "Epoch [1/1], Step [22220/25000], Loss: 0.2418\n",
      "Epoch [1/1], Step [22230/25000], Loss: 0.1523\n",
      "Epoch [1/1], Step [22240/25000], Loss: 0.1106\n",
      "Epoch [1/1], Step [22250/25000], Loss: 0.1095\n",
      "Epoch [1/1], Step [22260/25000], Loss: 0.7751\n",
      "Epoch [1/1], Step [22270/25000], Loss: 0.6451\n",
      "Epoch [1/1], Step [22280/25000], Loss: 0.1877\n",
      "Epoch [1/1], Step [22290/25000], Loss: 0.2292\n",
      "Epoch [1/1], Step [22300/25000], Loss: 0.2832\n",
      "Epoch [1/1], Step [22310/25000], Loss: 0.3369\n",
      "Epoch [1/1], Step [22320/25000], Loss: 0.4503\n",
      "Epoch [1/1], Step [22330/25000], Loss: 0.2497\n",
      "Epoch [1/1], Step [22340/25000], Loss: 0.3618\n",
      "Epoch [1/1], Step [22350/25000], Loss: 0.4070\n",
      "Epoch [1/1], Step [22360/25000], Loss: 0.2689\n",
      "Epoch [1/1], Step [22370/25000], Loss: 0.5777\n",
      "Epoch [1/1], Step [22380/25000], Loss: 0.9741\n",
      "Epoch [1/1], Step [22390/25000], Loss: 0.2540\n",
      "Epoch [1/1], Step [22400/25000], Loss: 0.2102\n",
      "Epoch [1/1], Step [22410/25000], Loss: 0.2189\n",
      "Epoch [1/1], Step [22420/25000], Loss: 0.0885\n",
      "Epoch [1/1], Step [22430/25000], Loss: 0.5774\n",
      "Epoch [1/1], Step [22440/25000], Loss: 0.0274\n",
      "Epoch [1/1], Step [22450/25000], Loss: 0.1679\n",
      "Epoch [1/1], Step [22460/25000], Loss: 0.1232\n",
      "Epoch [1/1], Step [22470/25000], Loss: 0.0890\n",
      "Epoch [1/1], Step [22480/25000], Loss: 0.3237\n",
      "Epoch [1/1], Step [22490/25000], Loss: 0.7420\n",
      "Epoch [1/1], Step [22500/25000], Loss: 0.1661\n",
      "Epoch [1/1], Step [22510/25000], Loss: 0.2855\n",
      "Epoch [1/1], Step [22520/25000], Loss: 0.1386\n",
      "Epoch [1/1], Step [22530/25000], Loss: 0.3761\n",
      "Epoch [1/1], Step [22540/25000], Loss: 0.5094\n",
      "Epoch [1/1], Step [22550/25000], Loss: 0.1726\n",
      "Epoch [1/1], Step [22560/25000], Loss: 0.1867\n",
      "Epoch [1/1], Step [22570/25000], Loss: 0.2705\n",
      "Epoch [1/1], Step [22580/25000], Loss: 0.3209\n",
      "Epoch [1/1], Step [22590/25000], Loss: 0.2412\n",
      "Epoch [1/1], Step [22600/25000], Loss: 0.6328\n",
      "Epoch [1/1], Step [22610/25000], Loss: 0.0829\n",
      "Epoch [1/1], Step [22620/25000], Loss: 0.5142\n",
      "Epoch [1/1], Step [22630/25000], Loss: 0.1360\n",
      "Epoch [1/1], Step [22640/25000], Loss: 0.2403\n",
      "Epoch [1/1], Step [22650/25000], Loss: 0.3053\n",
      "Epoch [1/1], Step [22660/25000], Loss: 0.3046\n",
      "Epoch [1/1], Step [22670/25000], Loss: 0.0934\n",
      "Epoch [1/1], Step [22680/25000], Loss: 0.5412\n",
      "Epoch [1/1], Step [22690/25000], Loss: 0.2745\n",
      "Epoch [1/1], Step [22700/25000], Loss: 0.4233\n",
      "Epoch [1/1], Step [22710/25000], Loss: 0.4172\n",
      "Epoch [1/1], Step [22720/25000], Loss: 0.0857\n",
      "Epoch [1/1], Step [22730/25000], Loss: 0.1700\n",
      "Epoch [1/1], Step [22740/25000], Loss: 0.0463\n",
      "Epoch [1/1], Step [22750/25000], Loss: 0.0847\n",
      "Epoch [1/1], Step [22760/25000], Loss: 0.4236\n",
      "Epoch [1/1], Step [22770/25000], Loss: 0.0144\n",
      "Epoch [1/1], Step [22780/25000], Loss: 0.3738\n",
      "Epoch [1/1], Step [22790/25000], Loss: 0.3210\n",
      "Epoch [1/1], Step [22800/25000], Loss: 0.0312\n",
      "Epoch [1/1], Step [22810/25000], Loss: 0.7939\n",
      "Epoch [1/1], Step [22820/25000], Loss: 0.4921\n",
      "Epoch [1/1], Step [22830/25000], Loss: 0.1024\n",
      "Epoch [1/1], Step [22840/25000], Loss: 0.1145\n",
      "Epoch [1/1], Step [22850/25000], Loss: 0.9615\n",
      "Epoch [1/1], Step [22860/25000], Loss: 0.2527\n",
      "Epoch [1/1], Step [22870/25000], Loss: 0.4533\n",
      "Epoch [1/1], Step [22880/25000], Loss: 0.1178\n",
      "Epoch [1/1], Step [22890/25000], Loss: 0.3019\n",
      "Epoch [1/1], Step [22900/25000], Loss: 0.1069\n",
      "Epoch [1/1], Step [22910/25000], Loss: 0.3089\n",
      "Epoch [1/1], Step [22920/25000], Loss: 0.1321\n",
      "Epoch [1/1], Step [22930/25000], Loss: 0.0920\n",
      "Epoch [1/1], Step [22940/25000], Loss: 0.3303\n",
      "Epoch [1/1], Step [22950/25000], Loss: 0.1335\n",
      "Epoch [1/1], Step [22960/25000], Loss: 0.0720\n",
      "Epoch [1/1], Step [22970/25000], Loss: 0.0653\n",
      "Epoch [1/1], Step [22980/25000], Loss: 0.2753\n",
      "Epoch [1/1], Step [22990/25000], Loss: 0.0117\n",
      "Epoch [1/1], Step [23000/25000], Loss: 0.0104\n",
      "Epoch [1/1], Step [23010/25000], Loss: 0.2771\n",
      "Epoch [1/1], Step [23020/25000], Loss: 0.1974\n",
      "Epoch [1/1], Step [23030/25000], Loss: 0.8210\n",
      "Epoch [1/1], Step [23040/25000], Loss: 0.1016\n",
      "Epoch [1/1], Step [23050/25000], Loss: 0.7352\n",
      "Epoch [1/1], Step [23060/25000], Loss: 0.1326\n",
      "Epoch [1/1], Step [23070/25000], Loss: 0.1331\n",
      "Epoch [1/1], Step [23080/25000], Loss: 0.1902\n",
      "Epoch [1/1], Step [23090/25000], Loss: 0.0554\n",
      "Epoch [1/1], Step [23100/25000], Loss: 0.0217\n",
      "Epoch [1/1], Step [23110/25000], Loss: 0.0852\n",
      "Epoch [1/1], Step [23120/25000], Loss: 0.2700\n",
      "Epoch [1/1], Step [23130/25000], Loss: 0.5608\n",
      "Epoch [1/1], Step [23140/25000], Loss: 0.0334\n",
      "Epoch [1/1], Step [23150/25000], Loss: 0.7472\n",
      "Epoch [1/1], Step [23160/25000], Loss: 0.2321\n",
      "Epoch [1/1], Step [23170/25000], Loss: 0.2801\n",
      "Epoch [1/1], Step [23180/25000], Loss: 0.0571\n",
      "Epoch [1/1], Step [23190/25000], Loss: 0.1191\n",
      "Epoch [1/1], Step [23200/25000], Loss: 0.8738\n",
      "Epoch [1/1], Step [23210/25000], Loss: 0.1770\n",
      "Epoch [1/1], Step [23220/25000], Loss: 0.0533\n",
      "Epoch [1/1], Step [23230/25000], Loss: 0.1052\n",
      "Epoch [1/1], Step [23240/25000], Loss: 0.1206\n",
      "Epoch [1/1], Step [23250/25000], Loss: 0.3758\n",
      "Epoch [1/1], Step [23260/25000], Loss: 0.1125\n",
      "Epoch [1/1], Step [23270/25000], Loss: 0.0417\n",
      "Epoch [1/1], Step [23280/25000], Loss: 0.2042\n",
      "Epoch [1/1], Step [23290/25000], Loss: 0.5511\n",
      "Epoch [1/1], Step [23300/25000], Loss: 0.0823\n",
      "Epoch [1/1], Step [23310/25000], Loss: 0.9196\n",
      "Epoch [1/1], Step [23320/25000], Loss: 0.0173\n",
      "Epoch [1/1], Step [23330/25000], Loss: 0.5683\n",
      "Epoch [1/1], Step [23340/25000], Loss: 0.1217\n",
      "Epoch [1/1], Step [23350/25000], Loss: 0.1400\n",
      "Epoch [1/1], Step [23360/25000], Loss: 0.3211\n",
      "Epoch [1/1], Step [23370/25000], Loss: 0.1755\n",
      "Epoch [1/1], Step [23380/25000], Loss: 0.0851\n",
      "Epoch [1/1], Step [23390/25000], Loss: 0.0630\n",
      "Epoch [1/1], Step [23400/25000], Loss: 0.1368\n",
      "Epoch [1/1], Step [23410/25000], Loss: 0.1937\n",
      "Epoch [1/1], Step [23420/25000], Loss: 0.3141\n",
      "Epoch [1/1], Step [23430/25000], Loss: 0.4632\n",
      "Epoch [1/1], Step [23440/25000], Loss: 0.1184\n",
      "Epoch [1/1], Step [23450/25000], Loss: 0.5396\n",
      "Epoch [1/1], Step [23460/25000], Loss: 0.4937\n",
      "Epoch [1/1], Step [23470/25000], Loss: 0.0941\n",
      "Epoch [1/1], Step [23480/25000], Loss: 0.2072\n",
      "Epoch [1/1], Step [23490/25000], Loss: 0.7555\n",
      "Epoch [1/1], Step [23500/25000], Loss: 0.2729\n",
      "Epoch [1/1], Step [23510/25000], Loss: 1.0421\n",
      "Epoch [1/1], Step [23520/25000], Loss: 0.3232\n",
      "Epoch [1/1], Step [23530/25000], Loss: 0.9213\n",
      "Epoch [1/1], Step [23540/25000], Loss: 0.0174\n",
      "Epoch [1/1], Step [23550/25000], Loss: 0.7324\n",
      "Epoch [1/1], Step [23560/25000], Loss: 0.4808\n",
      "Epoch [1/1], Step [23570/25000], Loss: 0.6260\n",
      "Epoch [1/1], Step [23580/25000], Loss: 0.0951\n",
      "Epoch [1/1], Step [23590/25000], Loss: 0.2336\n",
      "Epoch [1/1], Step [23600/25000], Loss: 0.7059\n",
      "Epoch [1/1], Step [23610/25000], Loss: 0.4403\n",
      "Epoch [1/1], Step [23620/25000], Loss: 0.1389\n",
      "Epoch [1/1], Step [23630/25000], Loss: 0.2850\n",
      "Epoch [1/1], Step [23640/25000], Loss: 0.4158\n",
      "Epoch [1/1], Step [23650/25000], Loss: 0.1706\n",
      "Epoch [1/1], Step [23660/25000], Loss: 0.7788\n",
      "Epoch [1/1], Step [23670/25000], Loss: 0.0589\n",
      "Epoch [1/1], Step [23680/25000], Loss: 0.5552\n",
      "Epoch [1/1], Step [23690/25000], Loss: 0.0677\n",
      "Epoch [1/1], Step [23700/25000], Loss: 0.4378\n",
      "Epoch [1/1], Step [23710/25000], Loss: 0.1301\n",
      "Epoch [1/1], Step [23720/25000], Loss: 0.0467\n",
      "Epoch [1/1], Step [23730/25000], Loss: 0.0734\n",
      "Epoch [1/1], Step [23740/25000], Loss: 0.1715\n",
      "Epoch [1/1], Step [23750/25000], Loss: 0.4179\n",
      "Epoch [1/1], Step [23760/25000], Loss: 0.0704\n",
      "Epoch [1/1], Step [23770/25000], Loss: 0.1734\n",
      "Epoch [1/1], Step [23780/25000], Loss: 0.4011\n",
      "Epoch [1/1], Step [23790/25000], Loss: 0.0584\n",
      "Epoch [1/1], Step [23800/25000], Loss: 0.2848\n",
      "Epoch [1/1], Step [23810/25000], Loss: 0.4691\n",
      "Epoch [1/1], Step [23820/25000], Loss: 1.2722\n",
      "Epoch [1/1], Step [23830/25000], Loss: 0.1017\n",
      "Epoch [1/1], Step [23840/25000], Loss: 0.0949\n",
      "Epoch [1/1], Step [23850/25000], Loss: 0.5278\n",
      "Epoch [1/1], Step [23860/25000], Loss: 0.1243\n",
      "Epoch [1/1], Step [23870/25000], Loss: 0.1837\n",
      "Epoch [1/1], Step [23880/25000], Loss: 0.1627\n",
      "Epoch [1/1], Step [23890/25000], Loss: 0.1317\n",
      "Epoch [1/1], Step [23900/25000], Loss: 0.0853\n",
      "Epoch [1/1], Step [23910/25000], Loss: 1.0996\n",
      "Epoch [1/1], Step [23920/25000], Loss: 0.0384\n",
      "Epoch [1/1], Step [23930/25000], Loss: 0.1130\n",
      "Epoch [1/1], Step [23940/25000], Loss: 0.0788\n",
      "Epoch [1/1], Step [23950/25000], Loss: 0.3790\n",
      "Epoch [1/1], Step [23960/25000], Loss: 0.1833\n",
      "Epoch [1/1], Step [23970/25000], Loss: 0.0852\n",
      "Epoch [1/1], Step [23980/25000], Loss: 0.0601\n",
      "Epoch [1/1], Step [23990/25000], Loss: 0.7470\n",
      "Epoch [1/1], Step [24000/25000], Loss: 0.6672\n",
      "Epoch [1/1], Step [24010/25000], Loss: 0.8636\n",
      "Epoch [1/1], Step [24020/25000], Loss: 0.1564\n",
      "Epoch [1/1], Step [24030/25000], Loss: 0.2019\n",
      "Epoch [1/1], Step [24040/25000], Loss: 1.1263\n",
      "Epoch [1/1], Step [24050/25000], Loss: 0.2077\n",
      "Epoch [1/1], Step [24060/25000], Loss: 0.5065\n",
      "Epoch [1/1], Step [24070/25000], Loss: 0.0540\n",
      "Epoch [1/1], Step [24080/25000], Loss: 0.0884\n",
      "Epoch [1/1], Step [24090/25000], Loss: 0.5271\n",
      "Epoch [1/1], Step [24100/25000], Loss: 0.0953\n",
      "Epoch [1/1], Step [24110/25000], Loss: 0.5511\n",
      "Epoch [1/1], Step [24120/25000], Loss: 0.0302\n",
      "Epoch [1/1], Step [24130/25000], Loss: 0.1382\n",
      "Epoch [1/1], Step [24140/25000], Loss: 0.1989\n",
      "Epoch [1/1], Step [24150/25000], Loss: 0.1145\n",
      "Epoch [1/1], Step [24160/25000], Loss: 0.0559\n",
      "Epoch [1/1], Step [24170/25000], Loss: 0.0907\n",
      "Epoch [1/1], Step [24180/25000], Loss: 0.0746\n",
      "Epoch [1/1], Step [24190/25000], Loss: 0.8690\n",
      "Epoch [1/1], Step [24200/25000], Loss: 0.0399\n",
      "Epoch [1/1], Step [24210/25000], Loss: 0.2964\n",
      "Epoch [1/1], Step [24220/25000], Loss: 0.5797\n",
      "Epoch [1/1], Step [24230/25000], Loss: 0.1234\n",
      "Epoch [1/1], Step [24240/25000], Loss: 0.4736\n",
      "Epoch [1/1], Step [24250/25000], Loss: 0.2765\n",
      "Epoch [1/1], Step [24260/25000], Loss: 0.0287\n",
      "Epoch [1/1], Step [24270/25000], Loss: 0.0586\n",
      "Epoch [1/1], Step [24280/25000], Loss: 0.3397\n",
      "Epoch [1/1], Step [24290/25000], Loss: 0.0409\n",
      "Epoch [1/1], Step [24300/25000], Loss: 0.1831\n",
      "Epoch [1/1], Step [24310/25000], Loss: 1.8840\n",
      "Epoch [1/1], Step [24320/25000], Loss: 0.0383\n",
      "Epoch [1/1], Step [24330/25000], Loss: 0.2488\n",
      "Epoch [1/1], Step [24340/25000], Loss: 1.5921\n",
      "Epoch [1/1], Step [24350/25000], Loss: 0.4229\n",
      "Epoch [1/1], Step [24360/25000], Loss: 0.3998\n",
      "Epoch [1/1], Step [24370/25000], Loss: 0.0963\n",
      "Epoch [1/1], Step [24380/25000], Loss: 0.1112\n",
      "Epoch [1/1], Step [24390/25000], Loss: 0.1912\n",
      "Epoch [1/1], Step [24400/25000], Loss: 0.0541\n",
      "Epoch [1/1], Step [24410/25000], Loss: 0.0989\n",
      "Epoch [1/1], Step [24420/25000], Loss: 0.3450\n",
      "Epoch [1/1], Step [24430/25000], Loss: 0.4777\n",
      "Epoch [1/1], Step [24440/25000], Loss: 0.1102\n",
      "Epoch [1/1], Step [24450/25000], Loss: 0.2307\n",
      "Epoch [1/1], Step [24460/25000], Loss: 0.0322\n",
      "Epoch [1/1], Step [24470/25000], Loss: 0.2410\n",
      "Epoch [1/1], Step [24480/25000], Loss: 0.1778\n",
      "Epoch [1/1], Step [24490/25000], Loss: 0.1014\n",
      "Epoch [1/1], Step [24500/25000], Loss: 0.0316\n",
      "Epoch [1/1], Step [24510/25000], Loss: 0.9372\n",
      "Epoch [1/1], Step [24520/25000], Loss: 0.0880\n",
      "Epoch [1/1], Step [24530/25000], Loss: 0.1996\n",
      "Epoch [1/1], Step [24540/25000], Loss: 0.1894\n",
      "Epoch [1/1], Step [24550/25000], Loss: 0.0571\n",
      "Epoch [1/1], Step [24560/25000], Loss: 0.2125\n",
      "Epoch [1/1], Step [24570/25000], Loss: 0.0585\n",
      "Epoch [1/1], Step [24580/25000], Loss: 0.7691\n",
      "Epoch [1/1], Step [24590/25000], Loss: 0.6391\n",
      "Epoch [1/1], Step [24600/25000], Loss: 0.4805\n",
      "Epoch [1/1], Step [24610/25000], Loss: 0.6294\n",
      "Epoch [1/1], Step [24620/25000], Loss: 0.0716\n",
      "Epoch [1/1], Step [24630/25000], Loss: 0.6147\n",
      "Epoch [1/1], Step [24640/25000], Loss: 0.3633\n",
      "Epoch [1/1], Step [24650/25000], Loss: 0.0893\n",
      "Epoch [1/1], Step [24660/25000], Loss: 0.3433\n",
      "Epoch [1/1], Step [24670/25000], Loss: 0.3644\n",
      "Epoch [1/1], Step [24680/25000], Loss: 0.0804\n",
      "Epoch [1/1], Step [24690/25000], Loss: 0.6763\n",
      "Epoch [1/1], Step [24700/25000], Loss: 0.2821\n",
      "Epoch [1/1], Step [24710/25000], Loss: 0.7398\n",
      "Epoch [1/1], Step [24720/25000], Loss: 0.2476\n",
      "Epoch [1/1], Step [24730/25000], Loss: 0.1192\n",
      "Epoch [1/1], Step [24740/25000], Loss: 0.0838\n",
      "Epoch [1/1], Step [24750/25000], Loss: 1.0783\n",
      "Epoch [1/1], Step [24760/25000], Loss: 0.0998\n",
      "Epoch [1/1], Step [24770/25000], Loss: 0.6199\n",
      "Epoch [1/1], Step [24780/25000], Loss: 0.1328\n",
      "Epoch [1/1], Step [24790/25000], Loss: 0.7814\n",
      "Epoch [1/1], Step [24800/25000], Loss: 0.3678\n",
      "Epoch [1/1], Step [24810/25000], Loss: 0.1334\n",
      "Epoch [1/1], Step [24820/25000], Loss: 0.0426\n",
      "Epoch [1/1], Step [24830/25000], Loss: 0.0750\n",
      "Epoch [1/1], Step [24840/25000], Loss: 0.5602\n",
      "Epoch [1/1], Step [24850/25000], Loss: 0.0258\n",
      "Epoch [1/1], Step [24860/25000], Loss: 0.2055\n",
      "Epoch [1/1], Step [24870/25000], Loss: 0.5342\n",
      "Epoch [1/1], Step [24880/25000], Loss: 0.5776\n",
      "Epoch [1/1], Step [24890/25000], Loss: 0.1817\n",
      "Epoch [1/1], Step [24900/25000], Loss: 0.9074\n",
      "Epoch [1/1], Step [24910/25000], Loss: 0.5180\n",
      "Epoch [1/1], Step [24920/25000], Loss: 0.3082\n",
      "Epoch [1/1], Step [24930/25000], Loss: 0.1117\n",
      "Epoch [1/1], Step [24940/25000], Loss: 0.0877\n",
      "Epoch [1/1], Step [24950/25000], Loss: 0.2601\n",
      "Epoch [1/1], Step [24960/25000], Loss: 0.1022\n",
      "Epoch [1/1], Step [24970/25000], Loss: 0.7380\n",
      "Epoch [1/1], Step [24980/25000], Loss: 0.2315\n",
      "Epoch [1/1], Step [24990/25000], Loss: 0.1968\n",
      "Epoch [1/1], Step [25000/25000], Loss: 0.1990\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Comments_Classifier(len(vocab), embedding_dim, hidden_size, num_classes)\n",
    "model.to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (cmt, lbl) in enumerate(dataloader):\n",
    "        cmt = cmt.to(device)\n",
    "        lbl = lbl.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model(cmt)\n",
    "        loss = criterion(outputs, lbl)\n",
    "\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), 'comments_classifier.pth')\n",
    "# 模型词典\n",
    "torch.save(vocab, 'comments_vocab.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "187fd433",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Nameless\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.453 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "评论1预测结果: 0\n",
      "评论2预测结果: 1\n"
     ]
    }
   ],
   "source": [
    "# 测试模型\n",
    "embedding_dim = 100\n",
    "hidden_size = 128\n",
    "num_classes = 2\n",
    "\n",
    "# 加载词典\n",
    "vocab = torch.load('comments_vocab.pth')\n",
    "# 测试模型\n",
    "comment1 = '这部电影真好看！全程无尿点'\n",
    "comment2 = '看到一半就不想看了，太无聊了，演员演技也很差'\n",
    "\n",
    "# 将评论转换为索引\n",
    "comment1_idx = torch.tensor([vocab.get(word, vocab['__UNK__']) for word in jieba.lcut(comment1)])\n",
    "comment2_idx = torch.tensor([vocab.get(word, vocab['__UNK__']) for word in jieba.lcut(comment2)])\n",
    "# 将评论转换为tensor\n",
    "comment1_idx = comment1_idx.unsqueeze(0).to(device)  # 添加batch维度    \n",
    "comment2_idx = comment2_idx.unsqueeze(0).to(device)  # 添加batch维度\n",
    "\n",
    "# 加载模型\n",
    "model = Comments_Classifier(len(vocab), embedding_dim, hidden_size, num_classes)\n",
    "model.load_state_dict(torch.load('comments_classifier.pth'))\n",
    "model.to(device)\n",
    "\n",
    "# 模型推理\n",
    "pred1 = model(comment1_idx)\n",
    "pred2 = model(comment2_idx)\n",
    "\n",
    "# 取最大值的索引作为预测结果\n",
    "pred1 = torch.argmax(pred1, dim=1).item()\n",
    "pred2 = torch.argmax(pred2, dim=1).item()\n",
    "print(f'评论1预测结果: {pred1}')\n",
    "print(f'评论2预测结果: {pred2}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3127",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
