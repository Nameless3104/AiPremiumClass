{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42713d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from EncoderDecoderAttentionModel import Seq2Seq\n",
    "\n",
    "state_dict = torch.load('seq2seq_state.bin')\n",
    "\n",
    "with open('./train_data/vocabs', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    tokens = ['PAD', 'UNK', 'BOS', 'EOS'] + [line.strip() for line in lines if line.strip()]\n",
    "    vocab = { tk:i for i, tk in enumerate(tokens)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3fe5bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\miniconda3\\envs\\py3127\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Seq2Seq(\n",
    "    enc_emb_size=len(vocab),\n",
    "    dec_emb_size=len(vocab),\n",
    "    emb_dim=100,\n",
    "    hidden_size=120,\n",
    "    dropout=0.5,\n",
    ")\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f3907cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建解码器反向字典\n",
    "dvoc_inv = {v:k for k,v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f445bbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "evoc, dvoc = vocab, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f86cc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一溪水一溪水一溪水一溪水一溪水一溪水一溪水一溪水一溪水一溪水一溪水一溪水一溪水一溪水一溪水一溪水一溪\n"
     ]
    }
   ],
   "source": [
    "# 用户输入\n",
    "enc_input = \"几 树 梅 花 数 竿 竹\"\n",
    "# enc_input = \"What I'm about to say is strictly between you and me\"\n",
    "# enc_input = \"I used to go swimming in the sea when I was a child\"\n",
    "enc_idx = torch.tensor([[evoc[tk] for tk in enc_input.split()]])\n",
    "\n",
    "\n",
    "# 推理\n",
    "# 最大解码长度\n",
    "max_dec_len = 50\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # 编码器\n",
    "    encoder_state, enc_outputs = model.encoder(enc_idx)\n",
    "    # hidden_state, enc_outputs = model.encoder(enc_idx)  # attention\n",
    "\n",
    "    # 解码器输入 shape [1,1]\n",
    "    dec_input = torch.tensor([[dvoc['BOS']]])\n",
    "\n",
    "    # 循环decoder\n",
    "    dec_tokens = []\n",
    "    while True:\n",
    "        if len(dec_tokens) >= max_dec_len:\n",
    "            break\n",
    "        # 解码器 \n",
    "        # logits: [1,1,dec_voc_size]\n",
    "        logits,hidden_state = model.decoder(dec_input, encoder_state, enc_outputs)\n",
    "        \n",
    "        # 下个token index\n",
    "        next_token = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        if dvoc_inv[next_token.squeeze().item()] == 'EOS':\n",
    "            break\n",
    "        # 收集每次token_index 【解码集合】\n",
    "        dec_tokens.append(next_token.squeeze().item())\n",
    "        # decoder的下一个输入 = token_index\n",
    "        dec_input = next_token\n",
    "        hidden_state = hidden_state.view(1, -1)\n",
    "\n",
    "# 输出解码结果\n",
    "print(''.join([dvoc_inv[tk] for tk in dec_tokens]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3127",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
